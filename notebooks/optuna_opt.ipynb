{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ea491d60-89fe-471f-a66b-b76a65e58f34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import optuna\n",
    "import os\n",
    "os.environ['NEPTUNE_PROJECT']=\"mlop3n/SDP\"\n",
    "os.environ['NEPTUNE_NOTEBOOK_PATH']=\"PycharmProjects/sdpiit/notebooks/Pipeline_components_builder.ipynb\"\n",
    "import warnings\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "patch_sklearn()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import (\n",
    "    BackwardDifferenceEncoder,\n",
    "    BaseNEncoder,\n",
    "    BinaryEncoder,\n",
    "    CatBoostEncoder,\n",
    "    CountEncoder,\n",
    "    GLMMEncoder,\n",
    "    HelmertEncoder,\n",
    "    JamesSteinEncoder,\n",
    "    LeaveOneOutEncoder,\n",
    "    MEstimateEncoder,\n",
    "    QuantileEncoder,\n",
    "    SummaryEncoder,\n",
    "    TargetEncoder,\n",
    "    WOEEncoder,\n",
    ")\n",
    "from sklearn import set_config\n",
    "from sklearn.base import clone as model_clone\n",
    "from sklearn.cluster import *\n",
    "from sklearn.compose import *\n",
    "from sklearn.cross_decomposition import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.gaussian_process import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.multioutput import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.utils import *\n",
    "from sklearn.dummy import *\n",
    "from sklearn.semi_supervised import *\n",
    "from sklearn.discriminant_analysis import *\n",
    "import sklearnex, daal4py\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from BorutaShap import BorutaShap\n",
    "\n",
    "from sklearn.calibration import *\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pd.options.display.max_columns = 50\n",
    "set_config(display=\"diagram\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import parallel_backend\n",
    "from joblib.memory import Memory\n",
    "\n",
    "sns.set()\n",
    "from pprint import pprint\n",
    "from helpers import PolynomialWrapper as PWrapper\n",
    "from helpers import NestedCVWrapper as NCVWrapper\n",
    "from helpers import ColumnSelectors\n",
    "import sklearn\n",
    "from helpers import DFCollection\n",
    "from helpers import plot_mean_std_max\n",
    "from helpers import CustomMetrics\n",
    "import gc\n",
    "\n",
    "CACHE_DIR = Memory(location='../data/joblib_memory/')\n",
    "OPTUNA_DB = \"postgresql+psycopg2://postgres:302492@localhost:5433/optuna\"\n",
    "def allow_stopping(func):\n",
    "    def wrapper():\n",
    "        try:\n",
    "            value = func()\n",
    "            return value\n",
    "            # gc.collect()\n",
    "        except KeyboardInterrupt as e:\n",
    "            print(\"Program Stopped\")\n",
    "        gc.collect()\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0005778b-264b-41b5-8f88-678fbea0ea0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = DFCollection()\n",
    "column_selector = ColumnSelectors()\n",
    "# classifiers = [f() for f in cls_names]\n",
    "dtype_info = column_selector.dtype_info\n",
    "ordinal = column_selector.ordinal_cols\n",
    "nominal = column_selector.nominal_cols\n",
    "binary = column_selector.binary_cols\n",
    "ratio = column_selector.ratio_cols\n",
    "\n",
    "\n",
    "final_data = db.final_data\n",
    "final_pred_data = db.final_pred_data\n",
    "baseline_prediction_data = db.baseline_prediction_data\n",
    "data_logit = db.data_logits\n",
    "prediction_data = db.prediction_data\n",
    "master_data = db.master\n",
    "given_data = db.data\n",
    "\n",
    "ordinal_data, nominal_data, binary_data, ratio_data = db.categorise_data()\n",
    "nominal_categories = db.nominal_categories\n",
    "ordinal_categories = db.ordinal_categories\n",
    "class_labels, n_classes, class_priors = class_distribution(final_data.target.to_numpy().reshape(-1,1))\n",
    "\n",
    "def gen_balanced_trained_test(data, p):\n",
    "    Y = data.target\n",
    "    X_2 = Y_2 = Y[Y == 2].index\n",
    "    X_0 = Y_0 = Y[Y == 0].index\n",
    "    X_1 = Y_1 = Y[Y == 1].index\n",
    "    train_size = int(p * Y_2.shape[0])\n",
    "    test_size = int((1 - p) * Y_2.shape[0])\n",
    "\n",
    "    train_idx_2 = np.random.choice(Y_2, (train_size,))\n",
    "    train_idx_1 = np.random.choice(Y_1, (train_size,))\n",
    "    train_idx_0 = np.random.choice(Y_0, (train_size,))\n",
    "    train_idx = np.r_[train_idx_0, train_idx_1, train_idx_2]\n",
    "    # train_idx.shape\n",
    "\n",
    "    test_idx_2 = np.random.choice(np.setdiff1d(Y_2, train_idx_2), (test_size,))\n",
    "    test_idx_1 = np.random.choice(np.setdiff1d(Y_1, train_idx_1), (test_size,))\n",
    "    test_idx_0 = np.random.choice(np.setdiff1d(Y_0, train_idx_0), (test_size,))\n",
    "    test_idx = np.r_[test_idx_0, test_idx_1, test_idx_2]\n",
    "    # test_idx.shape\n",
    "    return train_idx, test_idx\n",
    "\n",
    "\n",
    "def gen_nominal_maps(bs: pd.DataFrame = master_data) -> tuple[defaultdict, defaultdict]:\n",
    "    nominal_master_db = bs.loc[:, nominal]\n",
    "    nominal_cont_map = defaultdict(dict)\n",
    "    nominal_indvl_map = defaultdict(dict)\n",
    "    for c in nominal:\n",
    "        un = sorted(nominal_master_db[c].unique().tolist())\n",
    "        n = len(un)\n",
    "        new_id = list(range(n))\n",
    "        nominal_indvl_map[c] = dict(zip(un, new_id))\n",
    "    start = 0\n",
    "    for c in nominal:\n",
    "        un = sorted(nominal_master_db[c].unique().tolist())\n",
    "        n = len(un)\n",
    "        new_id = list(range(start, start + n))\n",
    "        nominal_cont_map[c] = dict(zip(un, new_id))\n",
    "        start += n\n",
    "    return nominal_indvl_map, nominal_cont_map\n",
    "\n",
    "\n",
    "# nominal_indvl_map, nominal_cont_map = gen_nominal_maps()\n",
    "# nominal_master_db = bs.loc[:, nominal]\n",
    "\n",
    "# nominal_master_db_indvl = nominal_master_db.copy()\n",
    "# nominal_master_db_cont = nominal_master_db.copy()\n",
    "\n",
    "\n",
    "# nominal_indvl_map\n",
    "def nm_indvl_data_trnsform(row):\n",
    "    for c in nominal:\n",
    "        curr = row[c]\n",
    "        row[c] = nominal_indvl_map[c][curr]\n",
    "    return row\n",
    "\n",
    "\n",
    "# test1_nominal = nominal_master_db_indvl.apply(nm_indvl_data_trnsform, axis=1)\n",
    "\n",
    "\n",
    "def nm_cont_data_trnsform(row):\n",
    "    for c in nominal:\n",
    "        curr = row[c]\n",
    "        row[c] = nominal_cont_map[c][curr]\n",
    "    return row\n",
    "\n",
    "\n",
    "# test2_nominal = nominal_master_db_cont.apply(nm_cont_data_trnsform, axis=1)\n",
    "# prediction_data = pd.read_pickle(\"../data/pred_data.pkl\")\n",
    "# est_ = [(\"cnb\",CategoricalNB()),]\n",
    "\n",
    "\n",
    "def wf_create(cat_encoder=TargetEncoder, model=None):\n",
    "    \"\"\"\n",
    "    :param cat_encoder: category_encoders\n",
    "    :param model: scikit-learn Model\n",
    "    :return pipe: sklearn.pipeline.Pipline\n",
    "    Examples of model param:\n",
    "\n",
    "    model = ComplementNB(norm=True,fit_prior=True,)\n",
    "    model = MultinomialNB()\n",
    "    model = LogisticRegression(n_jobs=-1, max_iter=10000,random_state=19)\n",
    "    \"\"\"\n",
    "    _steps = []\n",
    "    encoder__name = cat_encoder.__class__.__name__\n",
    "    _steps.append(\n",
    "        (\"PW\" + encoder__name, PolynomialWrapper(feature_encoder=cat_encoder))\n",
    "    )\n",
    "    if model is None:\n",
    "        passordinal_columns\n",
    "    else:\n",
    "        model__name = model.__class__.__name__\n",
    "        _steps.append((model__name, model))\n",
    "    pipe = Pipeline(steps=_steps)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a117d1d3-2321-4ffa-a7f5-1d3403950e2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define Base Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0607a9c7-8d6f-4b00-aa9f-8e958b833b67",
   "metadata": {},
   "source": [
    "```python\n",
    "{'n_layers': 9,\n",
    " 'l_1': 290,\n",
    " 'l_3': 230,\n",
    " 'l_5': 70,\n",
    " 'l_7': 28,\n",
    " 'activation': 'tanh',\n",
    " 'beta_1': 0.33005594239944347,\n",
    " 'batch_size': 394,\n",
    " 'n_estimators': 512,\n",
    " 'solver': 'adam',\n",
    " 'l_0': 535,\n",
    " 'l_2': 233,\n",
    " 'l_4': 126,\n",
    " 'l_6': 79,\n",
    " 'l_8': 27,\n",
    " 'alpha': 0.0005037897305741738,\n",
    " 'beta_2': 0.5311681482120456,\n",
    " 'learning_rate_init': 2.485794716152278e-05,\n",
    " 'power_t': 0.38,\n",
    " 'm_class_method': 'ovr'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4151f2f4-3f46-451e-bb55-c88c9bd3b306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-34 {color: black;background-color: white;}#sk-container-id-34 pre{padding: 0;}#sk-container-id-34 div.sk-toggleable {background-color: white;}#sk-container-id-34 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-34 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-34 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-34 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-34 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-34 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-34 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-34 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-34 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-34 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-34 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-34 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-34 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-34 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-34 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-34 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-34 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-34 div.sk-item {position: relative;z-index: 1;}#sk-container-id-34 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-34 div.sk-item::before, #sk-container-id-34 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-34 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-34 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-34 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-34 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-34 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-34 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-34 div.sk-label-container {text-align: center;}#sk-container-id-34 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-34 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-34\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneVsOneClassifier(estimator=Pipeline(steps=[(&#x27;rbm&#x27;,\n",
       "                                              BernoulliRBM(n_components=15,\n",
       "                                                           random_state=42)),\n",
       "                                             (&#x27;logistic&#x27;,\n",
       "                                              LogisticRegression(max_iter=109000,\n",
       "                                                                 random_state=43))]),\n",
       "                   n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-130\" type=\"checkbox\" ><label for=\"sk-estimator-id-130\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneVsOneClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsOneClassifier(estimator=Pipeline(steps=[(&#x27;rbm&#x27;,\n",
       "                                              BernoulliRBM(n_components=15,\n",
       "                                                           random_state=42)),\n",
       "                                             (&#x27;logistic&#x27;,\n",
       "                                              LogisticRegression(max_iter=109000,\n",
       "                                                                 random_state=43))]),\n",
       "                   n_jobs=-1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-131\" type=\"checkbox\" ><label for=\"sk-estimator-id-131\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;rbm&#x27;, BernoulliRBM(n_components=15, random_state=42)),\n",
       "                (&#x27;logistic&#x27;,\n",
       "                 LogisticRegression(max_iter=109000, random_state=43))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-132\" type=\"checkbox\" ><label for=\"sk-estimator-id-132\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliRBM</label><div class=\"sk-toggleable__content\"><pre>BernoulliRBM(n_components=15, random_state=42)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-133\" type=\"checkbox\" ><label for=\"sk-estimator-id-133\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=109000, random_state=43)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneVsOneClassifier(estimator=Pipeline(steps=[('rbm',\n",
       "                                              BernoulliRBM(n_components=15,\n",
       "                                                           random_state=42)),\n",
       "                                             ('logistic',\n",
       "                                              LogisticRegression(max_iter=109000,\n",
       "                                                                 random_state=43))]),\n",
       "                   n_jobs=-1)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params = {\n",
    "    \"activation\": \"tanh\",\n",
    "    \"hidden_layer_sizes\" : (256,128,64,32,16,8,4),\n",
    "    \"early_stopping\": True,\n",
    "    \"random_state\": 42,\n",
    "    \"max_iter\":512,\n",
    "    'batch_size': 1300,\n",
    "    \"solver\": 'adam',\n",
    "    \"validation_fraction\": 0.1,\n",
    "    \"tol\": 0.00001,\n",
    "    'beta_2': 0.5311681482120456,\n",
    "    'beta_1': 0.33005594239944347,\n",
    "    \"verbose\": False,\n",
    "    'alpha': 0.0005037897305741738,\n",
    "     'learning_rate_init': 2.485794716152278e-05,\n",
    "     'power_t': 0.38,\n",
    "    \"warm_start\": False,\n",
    "}\n",
    "rbm = BernoulliRBM(n_components=15,\n",
    "                   # learning_rate=0.01,\n",
    "                   # batch_size=19,\n",
    "                   # n_iter=100,\n",
    "                   verbose=0,\n",
    "                   random_state=42,)\n",
    "clf = MLPClassifier(**model_params)\n",
    "\n",
    "rbm_features_classifier = Pipeline(steps=[(\"rbm\", rbm), (\"logistic\",LogisticRegressionCV(max_iter=109000,random_state=43,scoring='f1_macro',n_jobs=-1))])\n",
    "rbm_features_classifier = Pipeline(steps=[(\"rbm\", rbm), (\"logistic\",LogisticRegression(max_iter=109000,random_state=43))])\n",
    "rbm_features_classifier = Pipeline(steps=[(\"rbm\", rbm), (\"logistic\",LogisticRegression(max_iter=109000,random_state=43))])\n",
    "# rbm_features_classifier = Pipeline(steps=[(\"logistic\",LogisticRegression(max_iter=109000,random_state=43))])\n",
    "# rbm_features_classifier = Pipeline(steps=[(\"rbm\", rbm), (\"logistic\",clf)])\n",
    "# clf = LogisticRegression(random_state=42)\n",
    "# mclass_clf = OneVsRestClassifier(clf,n_jobs=-1)\n",
    "# mclass_clf\n",
    "rbm_wf = OneVsOneClassifier(rbm_features_classifier,n_jobs=-1)\n",
    "rbm_wf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98fa0dc-7392-4bae-bc6d-83cd9b148e0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "008dfc4e-101b-49ee-b117-75b1099323b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "ohe_nominal_data = encoder.fit_transform(\n",
    "    nominal_data.drop([\"nominal__v_12\", \"nominal__v_21\"], axis=1)\n",
    ")\n",
    "target = final_data.target\n",
    "def convert_to_dfs(X_train, X_test, y_train, y_test, encoder):\n",
    "    columns = encoder.get_feature_names_out()\n",
    "    X_train = pd.DataFrame(X_train, columns=columns)\n",
    "    X_test = pd.DataFrame(X_test, columns=columns)\n",
    "    y_train = pd.DataFrame(y_train, columns=[\"target\"])\n",
    "    y_test = pd.DataFrame(y_test, columns=[\"target\"])\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def load_data(ohe_nominal_data, target, encoder = encoder):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ohe_nominal_data, target, stratify=target, test_size=0.2)    \n",
    "    X_train, X_test, y_train, y_test = convert_to_dfs(X_train, X_test, y_train, y_test, encoder=encoder)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test  = load_data(ohe_nominal_data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7378b31f-9ae0-4318-9fe8-faba526f32f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d0cf73-5b0c-42ef-9067-ef02e0b7882c",
   "metadata": {},
   "source": [
    "#### RBM Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "03e8f261-aa90-41f4-a400-bdcceabc0cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext cython\n"
     ]
    }
   ],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "a9f7ce87-0f34-433d-a5da-9abe379867c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%cython -+ \n",
    "from pymongo import MongoClient\n",
    "import optuna\n",
    "import os\n",
    "os.environ['NEPTUNE_PROJECT']=\"mlop3n/SDP\"\n",
    "os.environ['NEPTUNE_NOTEBOOK_PATH']=\"PycharmProjects/sdpiit/notebooks/Pipeline_components_builder.ipynb\"\n",
    "import warnings\n",
    "# from sklearnex import patch_sklearn\n",
    "\n",
    "# patch_sklearn()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import (\n",
    "    BackwardDifferenceEncoder,\n",
    "    BaseNEncoder,\n",
    "    BinaryEncoder,\n",
    "    CatBoostEncoder,\n",
    "    CountEncoder,\n",
    "    GLMMEncoder,\n",
    "    HelmertEncoder,\n",
    "    JamesSteinEncoder,\n",
    "    LeaveOneOutEncoder,\n",
    "    MEstimateEncoder,\n",
    "    QuantileEncoder,\n",
    "    SummaryEncoder,\n",
    "    TargetEncoder,\n",
    "    WOEEncoder,\n",
    ")\n",
    "from sklearn import set_config\n",
    "from sklearn.base import clone as model_clone\n",
    "from sklearn.cluster import *\n",
    "from sklearn.compose import *\n",
    "from sklearn.cross_decomposition import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.gaussian_process import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.multioutput import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.utils import *\n",
    "from sklearn.dummy import *\n",
    "from sklearn.semi_supervised import *\n",
    "from sklearn.discriminant_analysis import *\n",
    "import sklearnex, daal4py\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from BorutaShap import BorutaShap\n",
    "\n",
    "from sklearn.calibration import *\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pd.options.display.max_columns = 50\n",
    "set_config(display=\"diagram\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import parallel_backend\n",
    "from joblib.memory import Memory\n",
    "\n",
    "sns.set()\n",
    "from pprint import pprint\n",
    "from helpers import PolynomialWrapper as PWrapper\n",
    "from helpers import NestedCVWrapper as NCVWrapper\n",
    "from helpers import ColumnSelectors\n",
    "import sklearn\n",
    "from helpers import DFCollection\n",
    "from helpers import plot_mean_std_max\n",
    "from helpers import CustomMetrics\n",
    "import gc\n",
    "\n",
    "CACHE_DIR = Memory(location='../data/joblib_memory/')\n",
    "OPTUNA_DB = \"postgresql+psycopg2://postgres:302492@localhost:5433/optuna\"\n",
    "db = DFCollection()\n",
    "column_selector = ColumnSelectors()\n",
    "# classifiers = [f() for f in cls_names]\n",
    "dtype_info = column_selector.dtype_info\n",
    "ordinal = column_selector.ordinal_cols\n",
    "nominal = column_selector.nominal_cols\n",
    "binary = column_selector.binary_cols\n",
    "ratio = column_selector.ratio_cols\n",
    "\n",
    "\n",
    "final_data = db.final_data\n",
    "final_pred_data = db.final_pred_data\n",
    "baseline_prediction_data = db.baseline_prediction_data\n",
    "data_logit = db.data_logits\n",
    "prediction_data = db.prediction_data\n",
    "master_data = db.master\n",
    "given_data = db.data\n",
    "\n",
    "ordinal_data, nominal_data, binary_data, ratio_data = db.categorise_data()\n",
    "nominal_categories = db.nominal_categories\n",
    "ordinal_categories = db.ordinal_categories\n",
    "class_labels, n_classes, class_priors = class_distribution(final_data.target.to_numpy().reshape(-1,1))\n",
    "def allow_stopping(func):\n",
    "    def wrapper():\n",
    "        try:\n",
    "            value = func()\n",
    "            return value\n",
    "            # gc.collect()\n",
    "        except KeyboardInterrupt as e:\n",
    "            print(\"Program Stopped\")\n",
    "        gc.collect()\n",
    "    return wrapper\n",
    "model_params = {\n",
    "    \"activation\": \"tanh\",\n",
    "    \"hidden_layer_sizes\" : (256,128,64,32,16,8,4),\n",
    "    \"early_stopping\": True,\n",
    "    \"random_state\": 42,\n",
    "    \"max_iter\":512,\n",
    "    'batch_size': 1300,\n",
    "    \"solver\": 'adam',\n",
    "    \"validation_fraction\": 0.1,\n",
    "    \"tol\": 0.00001,\n",
    "    'beta_2': 0.5311681482120456,\n",
    "    'beta_1': 0.33005594239944347,\n",
    "    \"verbose\": False,\n",
    "    'alpha': 0.0005037897305741738,\n",
    "     'learning_rate_init': 2.485794716152278e-05,\n",
    "     'power_t': 0.38,\n",
    "    \"warm_start\": False,\n",
    "}\n",
    "# cpdef execute():\n",
    "#     for i in range(10,50):\n",
    "# rbm = BernoulliRBM(n_components=i,\n",
    "#                    # learning_rate=0.01,\n",
    "#                    # batch_size=19,\n",
    "#                    # n_iter=100,\n",
    "#                    verbose=0,\n",
    "#                    random_state=42,)\n",
    "# # clf = MLPClassifier(**model_params)\n",
    "\n",
    "# rbm_features_classifier = Pipeline(steps=[(\"rbm\", rbm), (\"logistic\",LogisticRegressionCV(max_iter=109000,random_state=43,scoring='f1_macro',n_jobs=-1))])\n",
    "# rbm_features_classifier = Pipeline(steps=[(\"rbm\", rbm), (\"logistic\",LogisticRegression(max_iter=109000,random_state=43))])\n",
    "# rbm_features_classifier = Pipeline(steps=[(\"logistic\",LogisticRegression(max_iter=109000,random_state=43))])\n",
    "# rbm_features_classifier = Pipeline(steps=[(\"rbm\", rbm), (\"logistic\",clf)])\n",
    "# clf = LogisticRegression(random_state=42)\n",
    "# mclass_clf = OneVsRestClassifier(clf,n_jobs=-1)\n",
    "# mclass_clf\n",
    "# rbm_wf = OneVsOneClassifier(rbm_features_classifier,n_jobs=-1)\n",
    "# Load Data\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "ohe_nominal_data = encoder.fit_transform(\n",
    "    nominal_data.drop([\"nominal__v_12\", \"nominal__v_21\"], axis=1))\n",
    "target = final_data.target\n",
    "\n",
    "# cpdef convert_to_dfs(X_train, X_test, y_train, y_test, encoder):\n",
    "cpdef convert_to_dfs(X_train, X_test, y_train, y_test, encoder):\n",
    "    columns = encoder.get_feature_names_out()\n",
    "    X_train = pd.DataFrame(X_train, columns=columns)\n",
    "    X_test = pd.DataFrame(X_test, columns=columns)\n",
    "    y_train = pd.DataFrame(y_train, columns=[\"target\"])\n",
    "    y_test = pd.DataFrame(y_test, columns=[\"target\"])\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# cpdef load_data(ohe_nominal_data, target, encoder = encoder):\n",
    "cpdef load_data(ohe_nominal_data, target, encoder = encoder):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ohe_nominal_data, target, stratify=target, test_size=0.3)    \n",
    "    X_train, X_test, y_train, y_test = convert_to_dfs(X_train, X_test, y_train, y_test, encoder=encoder)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test  = load_data(ohe_nominal_data, target)\n",
    "\n",
    "cpdef run(start: int, stop: int, step: int):\n",
    "# def run():\n",
    "    for i in range(start,stop,step):\n",
    "        rbm = BernoulliRBM(n_components=i,\n",
    "                       # learning_rate=0.01,\n",
    "                       # batch_size=19,\n",
    "                       # n_iter=100,\n",
    "                       verbose=0,\n",
    "                       random_state=42,)\n",
    "        rbm_clf = OneVsOneClassifier(LogisticRegressionCV(max_iter=109000,n_jobs=-1,fit_intercept=True,random_state=43),n_jobs=-1)\n",
    "        rbm_clf = OneVsRestClassifier(RandomForestClassifier(n_jobs=-1,random_state=42),n_jobs=-1)\n",
    "        # rbm_features_classifier = Pipeline(steps=[(\"rbm\", rbm), (\"logistic\",rbm_clf)])\n",
    "        rbm_wf = Pipeline(steps=[(\"rbm\", rbm), (\"logistic\",rbm_clf)])\n",
    "\n",
    "        with parallel_backend('loky'):\n",
    "            rbm_wf.fit(X_train, y_train)\n",
    "            y_pred_test = rbm_wf.predict(X_test)\n",
    "            y_pred_train = rbm_wf.predict(X_train)\n",
    "            metric1 = f1_score(y_test, y_pred_test, average=\"macro\", labels=[0, 1, 2])\n",
    "            metric2 = f1_score(y_train, y_pred_train,\n",
    "                               average=\"macro\", labels=[0, 1, 2])\n",
    "\n",
    "            print(metric1, metric2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6eda49-1c78-4bf8-ac9e-177d16247fca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████████████████████████████████████▏                                                                                                                                   | 116/393 [01:40<08:48,  1.91s/it]"
     ]
    }
   ],
   "source": [
    "# %%cython -+ \n",
    "from pymongo import MongoClient\n",
    "import optuna\n",
    "import os\n",
    "os.environ['NEPTUNE_PROJECT']=\"mlop3n/SDP\"\n",
    "os.environ['NEPTUNE_NOTEBOOK_PATH']=\"PycharmProjects/sdpiit/notebooks/Pipeline_components_builder.ipynb\"\n",
    "import warnings\n",
    "# from sklearnex import patch_sklearn\n",
    "\n",
    "# patch_sklearn()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import (\n",
    "    BackwardDifferenceEncoder,\n",
    "    BaseNEncoder,\n",
    "    BinaryEncoder,\n",
    "    CatBoostEncoder,\n",
    "    CountEncoder,\n",
    "    GLMMEncoder,\n",
    "    HelmertEncoder,\n",
    "    JamesSteinEncoder,\n",
    "    LeaveOneOutEncoder,\n",
    "    MEstimateEncoder,\n",
    "    QuantileEncoder,\n",
    "    SummaryEncoder,\n",
    "    TargetEncoder,\n",
    "    WOEEncoder,\n",
    ")\n",
    "from sklearn import set_config\n",
    "from sklearn.base import clone as model_clone\n",
    "from sklearn.cluster import *\n",
    "from sklearn.compose import *\n",
    "from sklearn.cross_decomposition import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.gaussian_process import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.multioutput import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.utils import *\n",
    "from sklearn.dummy import *\n",
    "from sklearn.semi_supervised import *\n",
    "from sklearn.discriminant_analysis import *\n",
    "import sklearnex, daal4py\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from BorutaShap import BorutaShap\n",
    "\n",
    "from sklearn.calibration import *\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pd.options.display.max_columns = 50\n",
    "set_config(display=\"diagram\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import parallel_backend\n",
    "from joblib.memory import Memory\n",
    "\n",
    "sns.set()\n",
    "from pprint import pprint\n",
    "from helpers import PolynomialWrapper as PWrapper\n",
    "from helpers import NestedCVWrapper as NCVWrapper\n",
    "from helpers import ColumnSelectors\n",
    "import sklearn\n",
    "from helpers import DFCollection\n",
    "from helpers import plot_mean_std_max\n",
    "from helpers import CustomMetrics\n",
    "import gc\n",
    "\n",
    "CACHE_DIR = Memory(location='../data/joblib_memory/')\n",
    "OPTUNA_DB = \"postgresql+psycopg2://postgres:302492@localhost:5433/optuna\"\n",
    "db = DFCollection()\n",
    "column_selector = ColumnSelectors()\n",
    "# classifiers = [f() for f in cls_names]\n",
    "dtype_info = column_selector.dtype_info\n",
    "ordinal = column_selector.ordinal_cols\n",
    "nominal = column_selector.nominal_cols\n",
    "binary = column_selector.binary_cols\n",
    "ratio = column_selector.ratio_cols\n",
    "\n",
    "\n",
    "final_data = db.final_data\n",
    "final_pred_data = db.final_pred_data\n",
    "baseline_prediction_data = db.baseline_prediction_data\n",
    "data_logit = db.data_logits\n",
    "prediction_data = db.prediction_data\n",
    "master_data = db.master\n",
    "given_data = db.data\n",
    "\n",
    "ordinal_data, nominal_data, binary_data, ratio_data = db.categorise_data()\n",
    "nominal_categories = db.nominal_categories\n",
    "ordinal_categories = db.ordinal_categories\n",
    "class_labels, n_classes, class_priors = class_distribution(final_data.target.to_numpy().reshape(-1,1))\n",
    "def allow_stopping(func):\n",
    "    def wrapper():\n",
    "        try:\n",
    "            value = func()\n",
    "            return value\n",
    "            # gc.collect()\n",
    "        except KeyboardInterrupt as e:\n",
    "            print(\"Program Stopped\")\n",
    "        gc.collect()\n",
    "    return wrapper\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "ohe_nominal_data = encoder.fit_transform(\n",
    "    nominal_data.drop([\"nominal__v_12\", \"nominal__v_21\"], axis=1))\n",
    "target = final_data.target\n",
    "\n",
    "# cpdef convert_to_dfs(X_train, X_test, y_train, y_test, encoder):\n",
    "def convert_to_dfs(X_train, X_test, y_train, y_test, encoder):\n",
    "    columns = encoder.get_feature_names_out()\n",
    "    X_train = pd.DataFrame(X_train, columns=columns)\n",
    "    X_test = pd.DataFrame(X_test, columns=columns)\n",
    "    y_train = pd.DataFrame(y_train, columns=[\"target\"])\n",
    "    y_test = pd.DataFrame(y_test, columns=[\"target\"])\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# cpdef load_data(ohe_nominal_data, target, encoder = encoder):\n",
    "def load_data(ohe_nominal_data, target, encoder = encoder):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ohe_nominal_data, target, stratify=target, test_size=0.3)    \n",
    "    X_train, X_test, y_train, y_test = convert_to_dfs(X_train, X_test, y_train, y_test, encoder=encoder)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test  = load_data(ohe_nominal_data, target)\n",
    "    \n",
    "df_t = {}\n",
    "\n",
    "def transformed_collecs(i: int):\n",
    "    # for i in tqdm(range(start,stop,step)):\n",
    "    global df_t\n",
    "    rbm = BernoulliRBM(n_components=i,\n",
    "                   verbose=0,\n",
    "                   random_state=42,)\n",
    "    X_train_t =  rbm.fit_transform(X_train,y_train,)\n",
    "    X_test_t = rbm.transform(X_test)\n",
    "    X_train_t_df = pd.DataFrame(X_train_t, columns=rbm.get_feature_names_out(),index=X_train.index)\n",
    "    X_test_t_df = pd.DataFrame(X_test_t, columns=rbm.get_feature_names_out(),index=X_test.index)\n",
    "    df_t[i] = (X_train_t_df,X_test_t_df)\n",
    "    # return df_t\n",
    "\n",
    "# payload = transformed_collecs(7,400,1)\n",
    "import joblib\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "from tqdm import trange\n",
    "with parallel_backend(\"threading\"):\n",
    "    results = Parallel(n_jobs=-1)(delayed(transformed_collecs)(j)  for  j in trange(7,400,1))\n",
    "payload = df_t\n",
    "joblib.dump(payload, '../data/rbm_transforms.pkl', compress='lzma',protocol=pickle.HIGHEST_PROTOCOL)\n",
    "def run(payload=payload):\n",
    "    # def run():\n",
    "    corrs_test = {}\n",
    "    corrs_train = {}\n",
    "    for i in payload:\n",
    "        X_train_t_df,X_test_t_df  = payload[i]\n",
    "     \n",
    "\n",
    "        corrs_test[i] = [\n",
    "            X_test_t_df.corrwith(y_test, method='kendall'),\n",
    "            X_test_t_df.corrwith(y_test, method='pearson'),\n",
    "            X_test_t_df.corrwith(y_test, method='spearman'),\n",
    "        ]\n",
    "        corrs_train[i] = [\n",
    "            X_train_t_df.corrwith(y_train, method='kendall'),\n",
    "            X_train_t_df.corrwith(y_train, method='pearson'),\n",
    "            X_train_t_df.corrwith(y_train, method='spearman'),\n",
    "        ]\n",
    "\n",
    "        return corrs_test, corrs_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "7fa7ade4-ac6d-4a94-8098-018062c070f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bernoullirbm0' 'bernoullirbm1' 'bernoullirbm2' 'bernoullirbm3'\n",
      " 'bernoullirbm4' 'bernoullirbm5' 'bernoullirbm6' 'bernoullirbm7'\n",
      " 'bernoullirbm8' 'bernoullirbm9']\n",
      "CPU times: user 357 ms, sys: 8.65 ms, total: 365 ms\n",
      "Wall time: 344 ms\n"
     ]
    }
   ],
   "source": [
    "%time run(10,50,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35830ed8-e2b4-48e9-9ca0-006ffe862b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2657, 256)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_t = rbm.fit_transform(X_train,y_train)\n",
    "\n",
    "X_test_t = rbm.transform(X_test)\n",
    "\n",
    "X_train_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c0548f0-3a97-416c-bd9d-7d8347c46b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "z = optuna.load_study(study_name=\"MLP_Layers=multi.test.1\", storage=OPTUNA_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39aa913b-9067-4ff4-bc95-2f80f2231e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2657, 433)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "b8117d94-8941-46f9-accd-1835cfe2717d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/rbm_transforms.pkl']"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb3e9d-08b7-44a1-9b1f-7c1c2d749354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autoskl)",
   "language": "python",
   "name": "autoskl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
