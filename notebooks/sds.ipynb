{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7f0c3e5-af72-4256-bd1e-7466f82c2dca",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "634ed7f6-a02a-4f33-8a33-182bf25fc427",
   "metadata": {
    "code_folding": [
     77,
     97,
     112,
     119,
     125,
     128,
     134,
     149,
     165,
     186,
     204,
     309,
     312,
     320,
     329,
     337,
     348,
     368,
     382,
     385,
     392,
     428,
     438,
     451,
     459,
     468,
     475,
     477,
     505,
     512,
     517,
     525,
     538,
     587,
     602
    ],
    "execution": {
     "iopub.execute_input": "2022-09-12T12:33:52.873906Z",
     "iopub.status.busy": "2022-09-12T12:33:52.873453Z",
     "iopub.status.idle": "2022-09-12T12:34:47.994800Z",
     "shell.execute_reply": "2022-09-12T12:34:47.994246Z",
     "shell.execute_reply.started": "2022-09-12T12:33:52.873799Z"
    },
    "hidden": true,
    "init_cell": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import warnings\n",
    "from sklearnex import patch_sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import (\n",
    "    BackwardDifferenceEncoder,\n",
    "    BaseNEncoder,\n",
    "    BinaryEncoder,\n",
    "    CatBoostEncoder,\n",
    "    CountEncoder,\n",
    "    GLMMEncoder,\n",
    "    HelmertEncoder,\n",
    "    JamesSteinEncoder,\n",
    "    LeaveOneOutEncoder,\n",
    "    MEstimateEncoder,\n",
    "    SummaryEncoder,\n",
    "    TargetEncoder,\n",
    "    WOEEncoder,\n",
    ")\n",
    "from sklearn import set_config\n",
    "from sklearn.base import clone as model_clone\n",
    "from sklearn.cluster import *\n",
    "from sklearn.compose import *\n",
    "from sklearn.cross_decomposition import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.gaussian_process import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.multioutput import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.kernel_approximation import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.utils import *\n",
    "from sklearn.dummy import *\n",
    "from sklearn.semi_supervised import *\n",
    "from sklearn.discriminant_analysis import *\n",
    "from sklearn.covariance import *\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.calibration import *\n",
    "import joblib\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "set_config(display=\"diagram\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline as make_imb_pipeline\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "from imblearn.over_sampling import (\n",
    "    ADASYN,\n",
    "    SMOTE,\n",
    "    RandomOverSampler,\n",
    "    SVMSMOTE,\n",
    "    SMOTENC,\n",
    "    SMOTEN,\n",
    "    BorderlineSMOTE,\n",
    "    KMeansSMOTE,\n",
    ")\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "from joblib import parallel_backend\n",
    "from joblib.memory import Memory\n",
    "\n",
    "# patch_sklearn()\n",
    "\n",
    "\n",
    "def allow_stopping(func):\n",
    "    def wrapper():\n",
    "        try:\n",
    "            value = func()\n",
    "            return value\n",
    "            # gc.collect()\n",
    "        except KeyboardInterrupt as e:\n",
    "            print(\"Program Stopped\")\n",
    "        gc.collect()\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "__refresh__ = 1\n",
    "\n",
    "\n",
    "def run_if_refresh(func):\n",
    "    def wrapper():\n",
    "        global __refresh__\n",
    "        if __refresh__ == 1:\n",
    "            value = func()\n",
    "            return value\n",
    "        else:\n",
    "            print(\n",
    "                \"Using Cache, Set Refresh to '__refresh__=1' to regenerate \"\n",
    "                \"the output of this  function\"\n",
    "            )\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def gen_train_test(X, y, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=10\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def categorise_input(X: pd.DataFrame):\n",
    "    X.loc[:, nominal + ordinal] = X.loc[:, nominal + ordinal].astype(\"category\")\n",
    "    return X\n",
    "\n",
    "\n",
    "KAGGLE_ENV = 1\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "if \"mlop3n/Pycharm\" in cwd:\n",
    "    KAGGLE_ENV = 0\n",
    "\n",
    "if KAGGLE_ENV == 1:\n",
    "\n",
    "    categoriser = FunctionTransformer(\n",
    "        categorise_input,\n",
    "    )\n",
    "categoriser = FunctionTransformer(categorise_input)\n",
    "\n",
    "\n",
    "def quick_test(X):\n",
    "    clfs = [\n",
    "        RandomForestClassifier(class_weight=\"balanced_subsample\", random_state=42),\n",
    "        DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "        HistGradientBoostingClassifier(random_state=42),\n",
    "        LogisticRegressionCV(max_iter=1000, class_weight=\"balanced\", random_state=42),\n",
    "    ]\n",
    "    y = raw_data.target\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(X, y, test_size=0.5)\n",
    "    for clf in clfs:\n",
    "        y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "        score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        print(f\"{clf.__class__.__name__} :: {score}\")\n",
    "\n",
    "\n",
    "def find_correlated_features(df, threshold=0.8):\n",
    "    correlated_features = set()\n",
    "    correlation_matrix = df.corr()\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                colname = correlation_matrix.columns[i]\n",
    "                correlated_features.add(colname)\n",
    "    return list(correlated_features)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "PCA MCA Equivalent\n",
    "\"\"\"\n",
    "\n",
    "# df = raw_data\n",
    "def MCA(df):\n",
    "    tmp_nom = np.zeros(df.shape[0])\n",
    "    for c in df.columns:\n",
    "        nom_f = df[c]\n",
    "        nom_f_vc = nom_f.value_counts()\n",
    "        nom_f_ohe = pd.get_dummies(nom_f)\n",
    "        nom_f_vc_pk = nom_f_vc / df.shape[0]\n",
    "        for ci in nom_f_ohe.columns:\n",
    "            nom_f_ohe[ci] = (nom_f_ohe[ci] / nom_f_vc_pk[ci]) - 1\n",
    "        tmp_nom = np.c_[tmp_nom, nom_f_ohe.to_numpy()]\n",
    "        # break\n",
    "    final_tmp_nom = tmp_nom[:, 1:]\n",
    "    dime = PCA(\n",
    "        svd_solver=\"full\",\n",
    "    )\n",
    "    final_nome_t = dime.fit_transform(final_tmp_nom)\n",
    "    return final_nome_t\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Binary Target Distributions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def binary_target_dist(df):\n",
    "    bin_target_0 = pd.DataFrame(columns=binary, index=[0, 1, 2])\n",
    "    bin_target_1 = pd.DataFrame(columns=binary, index=[0, 1, 2])\n",
    "    for c in binary:\n",
    "        target_census = df.target.value_counts().to_dict()\n",
    "        f = df.groupby([c])[\"target\"].value_counts().sort_index().unstack().transpose()\n",
    "        # print(f[0])\n",
    "        bin_target_0[c] = f[0]\n",
    "        bin_target_1[c] = f[1]\n",
    "    for i in range(3):\n",
    "        bin_target_0.loc[i, :] = bin_target_0.loc[i, :] / target_census[i]\n",
    "        bin_target_1.loc[i, :] = bin_target_1.loc[i, :] / target_census[i]\n",
    "    return bin_target_0, bin_target_1\n",
    "\n",
    "\n",
    "def m_VI(df):\n",
    "    VI = np.linalg.inv(np.cov(df, rowvar=False))\n",
    "    return VI\n",
    "\n",
    "\n",
    "class ColumnSelectors:\n",
    "    def __init__(self, default=None):\n",
    "        self.dtype_info = {\n",
    "            \"binary__v_1\": \"Binary\",\n",
    "            \"binary__v_11\": \"Binary\",\n",
    "            \"binary__v_14\": \"Binary\",\n",
    "            \"binary__v_26\": \"Binary\",\n",
    "            \"binary__v_27\": \"Binary\",\n",
    "            \"binary__v_28\": \"Binary\",\n",
    "            \"binary__v_30\": \"Binary\",\n",
    "            \"binary__v_9\": \"Binary\",\n",
    "            \"nominal__v_12\": \"Nominal\",\n",
    "            \"nominal__v_18\": \"Nominal\",\n",
    "            \"nominal__v_20\": \"Nominal\",\n",
    "            \"nominal__v_21\": \"Nominal\",\n",
    "            \"nominal__v_25\": \"Nominal\",\n",
    "            \"nominal__v_3\": \"Nominal\",\n",
    "            \"nominal__v_32\": \"Nominal\",\n",
    "            \"nominal__v_4\": \"Nominal\",\n",
    "            \"ordinal__v_0\": \"Ordinal\",\n",
    "            \"ordinal__v_10\": \"Ordinal\",\n",
    "            \"ordinal__v_13\": \"Ordinal\",\n",
    "            \"ordinal__v_15\": \"Ordinal\",\n",
    "            \"ordinal__v_17\": \"Ordinal\",\n",
    "            \"ordinal__v_19\": \"Ordinal\",\n",
    "            \"ordinal__v_22\": \"Ordinal\",\n",
    "            \"ordinal__v_23\": \"Ordinal\",\n",
    "            \"ordinal__v_24\": \"Ordinal\",\n",
    "            \"ordinal__v_29\": \"Ordinal\",\n",
    "            \"ordinal__v_31\": \"Ordinal\",\n",
    "            \"ordinal__v_33\": \"Ordinal\",\n",
    "            \"ordinal__v_5\": \"Ordinal\",\n",
    "            \"ordinal__v_6\": \"Ordinal\",\n",
    "            \"ratio__v_16\": \"Ratio\",\n",
    "            \"ratio__v_2\": \"Ratio\",\n",
    "            \"ratio__v_34\": \"Ratio\",\n",
    "            \"ratio__v_35\": \"Ratio\",\n",
    "            \"ratio__v_36\": \"Ratio\",\n",
    "            \"ratio__v_37\": \"Ratio\",\n",
    "            \"ratio__v_38\": \"Ratio\",\n",
    "            \"ratio__v_39\": \"Ratio\",\n",
    "            \"ratio__v_40\": \"Ratio\",\n",
    "            \"ratio__v_7\": \"Ratio\",\n",
    "            \"ratio__v_8\": \"Ratio\",\n",
    "        }\n",
    "\n",
    "        self.ordinal_cols = [\n",
    "            i for i in self.dtype_info if self.dtype_info[i] == \"Ordinal\"\n",
    "        ]\n",
    "        self.nominal_cols = [\n",
    "            i for i in self.dtype_info if self.dtype_info[i] == \"Nominal\"\n",
    "        ]\n",
    "        self.binary_cols = [\n",
    "            i for i in self.dtype_info if self.dtype_info[i] == \"Binary\"\n",
    "        ]\n",
    "        self.ratio_cols = [i for i in self.dtype_info if self.dtype_info[i] == \"Ratio\"]\n",
    "        self.ordinal = make_column_selector(\n",
    "            pattern=\"|\".join(self.ordinal_cols),\n",
    "        )\n",
    "        self.nominal = make_column_selector(\n",
    "            pattern=\"|\".join(self.nominal_cols),\n",
    "        )\n",
    "        self.binary = make_column_selector(\n",
    "            pattern=\"|\".join(self.binary_cols),\n",
    "        )\n",
    "        self.ratio = make_column_selector(\n",
    "            pattern=\"|\".join(self.ratio_cols),\n",
    "        )\n",
    "        self.nominal_ohe = None\n",
    "        self.ordinal_ohe = None\n",
    "\n",
    "    def add_feature(self, dtype, feature_name):\n",
    "        dtype_map = {\n",
    "            \"o\": self.ordinal_cols,\n",
    "            \"r\": self.ratio_cols,\n",
    "            \"b\": self.binary_cols,\n",
    "            \"n\": self.nominal_cols,\n",
    "        }\n",
    "        dtype_map[dtype].append(feature_name)\n",
    "        return dtype_map[dtype]\n",
    "\n",
    "    def ordinal_selector(self):\n",
    "        return self.ordinal\n",
    "\n",
    "    def nominal_selector(self):\n",
    "        return self.nominal\n",
    "\n",
    "    def binary_selector(self):\n",
    "        return self.binary\n",
    "\n",
    "    def ratio_selector(self):\n",
    "        return self.ratio\n",
    "\n",
    "    def ordinal_features(self):\n",
    "        return self.ordinal_cols\n",
    "\n",
    "    def nominal_features(self):\n",
    "        return self.nominal_cols\n",
    "\n",
    "    def binary_features(self):\n",
    "        return self.binary_cols\n",
    "\n",
    "    def ratio_features(self):\n",
    "        return self.ratio_cols\n",
    "\n",
    "    def update_ohe_features(self, n, o):\n",
    "        self.nominal_ohe = n\n",
    "        self.ordinal_ohe = o\n",
    "\n",
    "    def nominal_ohe_features(self):\n",
    "        return self.nominal_ohe\n",
    "\n",
    "    def ordinal_ohe_features(self):\n",
    "        return self.ordinal_ohe\n",
    "\n",
    "    def categorise_data(self, df: pd.DataFrame = None):\n",
    "        \"\"\"\n",
    "        Categorise Data based on given data\n",
    "        :params df : pandas.Dataframe\n",
    "        \"\"\"\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            ordinal_data = df.loc[:, self.ordinal_cols]\n",
    "            nominal_data = df.loc[:, self.nominal_cols]\n",
    "            binary_data = df.loc[:, self.binary_cols]\n",
    "            ratio_data = df.loc[:, self.ratio_cols]\n",
    "        else:\n",
    "            print(\"Please provide valid Data\")\n",
    "        return ordinal_data, nominal_data, binary_data, ratio_data\n",
    "\n",
    "\n",
    "column_directory = ColumnSelectors()\n",
    "\n",
    "DATA_PATH = \"/kaggle/input/students-drop-out-prediction/\"\n",
    "DATA_SAVE_PATH = \"/kaggle/working/\"\n",
    "TRAIN_DATA = \"train.csv\"\n",
    "TEST_DATA = \"test.csv\"\n",
    "KAGGLE_ENV = 1\n",
    "BENCHMARK_INPUT = \"/kaggle/input/sub-sample/\"\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "if \"mlop3n/Pycharm\" in cwd or \"u164131\" in cwd:\n",
    "    KAGGLE_ENV = 0\n",
    "\n",
    "if KAGGLE_ENV == 0:\n",
    "    LOCAL_PATH = \"../data\"\n",
    "    DATA_PATH = LOCAL_PATH + DATA_PATH\n",
    "    DATA_SAVE_PATH = LOCAL_PATH + DATA_SAVE_PATH\n",
    "    BENCHMARK_INPUT = LOCAL_PATH + BENCHMARK_INPUT\n",
    "\n",
    "CACHE_ = Memory(DATA_SAVE_PATH, verbose=0)\n",
    "\n",
    "\n",
    "def write_raw_data(raw_data: pd.DataFrame, raw_data_eval: pd.DataFrame):\n",
    "    global DATA_SAVE_PATH\n",
    "    raw_data.to_parquet(DATA_SAVE_PATH + \"train.parquet\")\n",
    "    raw_data_eval.to_parquet(DATA_SAVE_PATH + \"test.parquet\")\n",
    "\n",
    "\n",
    "def write_raw_data_ohe(raw_data_ohe: pd.DataFrame, raw_data_ohe_eval: pd.DataFrame):\n",
    "    global DATA_SAVE_PATH\n",
    "    raw_data_ohe.to_parquet(DATA_SAVE_PATH + \"train_ohe.parquet\")\n",
    "    raw_data_ohe_eval.to_parquet(DATA_SAVE_PATH + \"test_ohe.parquet\")\n",
    "\n",
    "\n",
    "# TODO rerun this script to rename the target column\n",
    "\n",
    "\n",
    "def reload_raw_data():\n",
    "    global DATA_SAVE_PATH\n",
    "    raw_data = pd.read_parquet(DATA_SAVE_PATH + \"train.parquet\")\n",
    "    raw_data_eval = pd.read_parquet(DATA_SAVE_PATH + \"test.parquet\")\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "\n",
    "def reload_raw_data_ohe():\n",
    "    global DATA_SAVE_PATH\n",
    "    raw_data_ohe = pd.read_parquet(DATA_SAVE_PATH + \"train_ohe.parquet\")\n",
    "    raw_data_ohe_eval = pd.read_parquet(DATA_SAVE_PATH + \"test_ohe.parquet\")\n",
    "    return raw_data_ohe, raw_data_ohe_eval\n",
    "\n",
    "\n",
    "benchmark = pd.read_csv(BENCHMARK_INPUT + \"sample.csv\")\n",
    "\n",
    "\n",
    "def make_submission(y):\n",
    "    trial = 0\n",
    "    y_df = pd.DataFrame(y, columns=[\"label\"])\n",
    "    y_df.index.rename(\"id\", inplace=True)\n",
    "    y_df.to_csv(\n",
    "        DATA_SAVE_PATH + \"submission.csv\",\n",
    "    )\n",
    "    if trial == 0:\n",
    "        benchmark.to_csv(DATA_SAVE_PATH + \"submission.csv\")\n",
    "\n",
    "\n",
    "def rename_columns_with_dtype(\n",
    "    DATA_PATH=DATA_PATH, DATA_SAVE_PATH=DATA_SAVE_PATH, clear_history=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Set clear_history to False\n",
    "    \"\"\"\n",
    "    initial_column_directory = ColumnSelectors()\n",
    "    train_data = pd.read_csv(DATA_PATH + TRAIN_DATA, index_col=0)\n",
    "    test_data = pd.read_csv(DATA_PATH + TEST_DATA, index_col=0)\n",
    "\n",
    "    raw_dtypes_info = {}\n",
    "    saved_dtypes_info = initial_column_directory.dtype_info\n",
    "    for k, v in saved_dtypes_info.items():\n",
    "        tmp = k.split(\"__\")\n",
    "        data_type = tmp[0]\n",
    "        column_name = tmp[1]\n",
    "        raw_dtypes_info[column_name] = k\n",
    "\n",
    "    train_data.rename(columns=raw_dtypes_info, inplace=True)\n",
    "    test_data.rename(columns=raw_dtypes_info, inplace=True)\n",
    "    train_data.rename(columns={\"label\": \"target\"}, inplace=True)\n",
    "    if clear_history:\n",
    "        write_raw_data(train_data, test_data)\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "# rename_columns_with_dtype()\n",
    "def reset_data(remove_cache=True):\n",
    "    return rename_columns_with_dtype(clear_history=remove_cache)\n",
    "\n",
    "\n",
    "raw_data, raw_data_eval = reset_data()\n",
    "\n",
    "X = raw_data.drop([\"target\"], axis=1)\n",
    "y = raw_data.target\n",
    "import copy\n",
    "\n",
    "ordinal = column_directory.ordinal_features()\n",
    "nominal = column_directory.nominal_features()\n",
    "binary = column_directory.binary_features()\n",
    "ratios = column_directory.ratio_features()\n",
    "discrete_binary = copy.deepcopy(binary)\n",
    "discrete_ordinal = copy.deepcopy(ordinal)\n",
    "ordinal_data, nominal_data, binary_data, ratios_data = column_directory.categorise_data(\n",
    "    raw_data\n",
    ")\n",
    "(\n",
    "    ordinal_data_eval,\n",
    "    nominal_data_eval,\n",
    "    binary_data_eval,\n",
    "    ratios_data_eval,\n",
    ") = column_directory.categorise_data(raw_data_eval)\n",
    "\n",
    "raw_data, raw_data_eval = reset_data()\n",
    "\n",
    "\n",
    "def gen_categorical_mask(df):\n",
    "    categorical_f_mask = []\n",
    "    for f in df.columns:\n",
    "        if f in ordinal + nominal + binary:\n",
    "            categorical_f_mask.append(True)\n",
    "        else:\n",
    "            categorical_f_mask.append(False)\n",
    "    return categorical_f_mask\n",
    "\n",
    "\n",
    "def update_ordinal_data(raw_data=raw_data, raw_data_eval=raw_data_eval):\n",
    "    categorical_columns = [ordinal, nominal]\n",
    "    ordinal_data = raw_data.loc[:, ordinal].copy()\n",
    "    ordinal_data_eval = raw_data_eval.loc[:, ordinal].copy()\n",
    "    raw_data.loc[:, ordinal] = ordinal_data - ordinal_data.min()\n",
    "    raw_data_eval.loc[:, ordinal] = ordinal_data_eval - ordinal_data.min()\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "\n",
    "def sort_columns_by_name(raw_data=raw_data, raw_data_eval=raw_data_eval):\n",
    "    raw_data = raw_data.sort_index(axis=1)\n",
    "    raw_data_eval = raw_data_eval.sort_index(axis=1)\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "\n",
    "# @run_if_refresh\n",
    "def ordinally_encode_nominal_data(raw_data=raw_data, raw_data_eval=raw_data_eval):\n",
    "    X_master = pd.concat(\n",
    "        [raw_data.loc[:, raw_data_eval.columns], raw_data_eval],\n",
    "        ignore_index=True,\n",
    "        axis=0,\n",
    "    )\n",
    "    ordinal_enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=125)\n",
    "    ordinal_enc.fit(X_master.loc[:, nominal])\n",
    "    nominal_enc_data = ordinal_enc.transform(raw_data.loc[:, nominal])\n",
    "    nominal_enc_data_eval = ordinal_enc.transform(raw_data_eval.loc[:, nominal])\n",
    "    raw_data.loc[:, nominal] = nominal_enc_data\n",
    "    raw_data_eval.loc[:, nominal] = nominal_enc_data_eval\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "\n",
    "# @run_if_refresh\n",
    "def binary_feature_sum(raw_data=raw_data, raw_data_eval=raw_data_eval):\n",
    "    global column_directory\n",
    "    raw_data[\"binary__sum\"] = raw_data.loc[:, binary].sum(axis=1) / 8\n",
    "    raw_data_eval[\"binary__sum\"] = raw_data_eval.loc[:, binary].sum(axis=1) / 8\n",
    "    column_directory.add_feature(\"b\", \"binary__sum\")\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "\n",
    "# @run_if_refresh\n",
    "def standardise_ratio_features(\n",
    "    raw_data=raw_data.copy(), raw_data_eval=raw_data_eval.copy()\n",
    "):\n",
    "    # Load Data\n",
    "    payload = raw_data.loc[:, ratios]\n",
    "    payload_eval = raw_data_eval.loc[:, ratios]\n",
    "    mscaler = Normalizer()\n",
    "    # Apply Scaling\n",
    "    payload_t = mscaler.fit_transform(payload)\n",
    "    payload_eval_t = mscaler.transform(payload_eval)\n",
    "    # Create Dataframe with new features to concatenate\n",
    "    tmp_raw_data = pd.DataFrame(payload_t, columns=ratios, index=raw_data.index)\n",
    "    tmp_raw_data_eval = pd.DataFrame(\n",
    "        payload_eval_t, columns=ratios, index=raw_data_eval.index\n",
    "    )\n",
    "    # Update in place\n",
    "    raw_data.loc[:, ratios] = tmp_raw_data\n",
    "    raw_data_eval.loc[:, ratios] = tmp_raw_data_eval\n",
    "    # Write modified data to disk\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "\n",
    "def ordinal_norm(df):\n",
    "    scaler = PowerTransformer()\n",
    "    tmp_ord = df.loc[:, ordinal].to_numpy()\n",
    "    tmp_ord_sq = tmp_ord @ tmp_ord.T\n",
    "    tmp_ord_sum = np.sqrt(tmp_ord_sq.sum(axis=1))\n",
    "    tmp_ord_nom = scaler.fit_transform(tmp_ord_sum.reshape(-1, 1))\n",
    "    return tmp_ord_nom\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Making Categories Similar within Training Data and Test Data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def cleanup_feature_names(ct):\n",
    "    f_names = []\n",
    "    fd = ct.get_feature_names_out()\n",
    "    for c in fd:\n",
    "        st = c.split(\"__\")\n",
    "        e = c.replace(st[0] + \"__\", \"\")\n",
    "        f_names.append(e)\n",
    "    return f_names\n",
    "\n",
    "\n",
    "enc = OneHotEncoder(\n",
    "    sparse=False,\n",
    ")\n",
    "num_enc = Normalizer()\n",
    "num_f = np.setdiff1d(\n",
    "    raw_data_eval.columns, nominal + discrete_ordinal + discrete_binary\n",
    ")\n",
    "ct = make_column_transformer(\n",
    "    (enc, nominal + discrete_ordinal),\n",
    "    (\"passthrough\", discrete_binary),\n",
    "    sparse_threshold=0,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "comb_ct = make_column_transformer(\n",
    "    (enc, nominal + discrete_ordinal),\n",
    "    (\"passthrough\", discrete_binary),\n",
    "    (num_enc, num_f),\n",
    "    sparse_threshold=0,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "X_master = pd.concat(\n",
    "    [raw_data.loc[:, raw_data_eval.columns], raw_data_eval], ignore_index=True, axis=0\n",
    ")\n",
    "\n",
    "ct.fit(X_master.loc[:, nominal + discrete_ordinal + discrete_binary])\n",
    "comb_ct.fit(X_master)\n",
    "Tx = comb_ct.transform(raw_data)\n",
    "Tx_ = comb_ct.transform(raw_data_eval)\n",
    "f_names = cleanup_feature_names(ct)\n",
    "X = pd.DataFrame(ct.transform(raw_data), columns=f_names)\n",
    "X_ = pd.DataFrame(ct.transform(raw_data_eval), columns=f_names)\n",
    "d_metrics = [\n",
    "    \"matching\",\n",
    "    \"jaccard\",\n",
    "    \"dice\",\n",
    "    \"kulsinski\",\n",
    "    \"rogerstanimoto\",\n",
    "    \"russellrao\",\n",
    "    \"sokalmichener\",\n",
    "    \"sokalsneath\",\n",
    "    \"hamming\",\n",
    "]\n",
    "d_num_metrics = [\n",
    "    \"braycurtis\",\n",
    "    \"canberra\",\n",
    "    \"chebyshev\",\n",
    "    \"cityblock\",\n",
    "    \"euclidean\",\n",
    "    \"l1\",\n",
    "    \"l2\",\n",
    "    \"manhattan\",\n",
    "    \"minkowski\",\n",
    "    \"p\",\n",
    "]\n",
    "\n",
    "\n",
    "def counterpart_eval(li_idx, c):\n",
    "    return raw_data_eval.loc[li_idx, c]\n",
    "\n",
    "\n",
    "def counterpart_train(li_idx, c):\n",
    "    return raw_data.loc[li_idx, c]\n",
    "\n",
    "\n",
    "def link_outlier_categories_train(raw_data, raw_data_eval):\n",
    "    ufo_val = {}\n",
    "    ufo_val_idx = {x: {} for x in nominal + discrete_ordinal}\n",
    "    u_i = []\n",
    "    ufo_idx = None\n",
    "    for c in nominal + discrete_ordinal:\n",
    "        i_ = raw_data[c].unique()\n",
    "        i__ = raw_data_eval[c].unique()\n",
    "        ufo = np.setdiff1d(i_, i__)\n",
    "        ufo_val[c] = list(ufo)\n",
    "        if len(ufo) > 0:\n",
    "            #         tmp = raw_data[raw_data[c]==list(ufo)[0]].index\n",
    "            for val in list(ufo):\n",
    "                ufo_val_idx[c][val] = raw_data[raw_data[c] == val].index\n",
    "\n",
    "    ufo_val_idx_ = copy.deepcopy(ufo_val_idx)\n",
    "    for i in ufo_val_idx:\n",
    "        if len(ufo_val_idx[i].keys()) == 0:\n",
    "            del ufo_val_idx_[i]\n",
    "    # import pprint\n",
    "    # pprint.pprint(ufo_val_idx_)\n",
    "    ufo_idx = []\n",
    "    for c in ufo_val_idx_:\n",
    "        for k in ufo_val_idx_[c]:\n",
    "            ufo_idx.extend(list(ufo_val_idx_[c][k]))\n",
    "    ufo_idx = list(set(ufo_idx))\n",
    "    len(ufo_idx)\n",
    "\n",
    "    trees_categ = [BallTree(X_, leaf_size=1, metric=d_m) for d_m in d_metrics]\n",
    "    tree_comb = [BallTree(Tx_, leaf_size=1, metric=d_m) for d_m in d_num_metrics]\n",
    "    # trees_num =\n",
    "\n",
    "    cat_n_idx = []\n",
    "    comb_n_idx = []\n",
    "    for tree in trees_categ:\n",
    "        d, idx = tree.query(X.loc[ufo_idx, :], k=1)\n",
    "        cat_n_idx.append(idx.ravel())\n",
    "\n",
    "    for tree in tree_comb:\n",
    "        d, idx = tree.query(Tx[ufo_idx, :], k=1)\n",
    "        comb_n_idx.append(idx.ravel())\n",
    "\n",
    "    cat_N = pd.DataFrame(index=ufo_idx)\n",
    "    for i in range(len(d_metrics)):\n",
    "        cat_N[i] = cat_n_idx[i]\n",
    "\n",
    "    for i in range(len(d_num_metrics)):\n",
    "        cat_N[\"comb__\" + str(i)] = comb_n_idx[i]\n",
    "\n",
    "    cat_N[\"mde\"] = cat_N.mode(axis=1)[0]\n",
    "\n",
    "    cat_N[\"mde\"] = cat_N[\"mde\"].astype(\"int\")\n",
    "\n",
    "    ufo_val_cpart = {x: {} for x in nominal + discrete_ordinal}\n",
    "    for c in ufo_val_idx_:\n",
    "        uf_subs = {}\n",
    "\n",
    "        for val in ufo_val_idx_[c]:\n",
    "            uf_ids = list(ufo_val_idx_[c][val])\n",
    "            for u_id in uf_ids:\n",
    "                # uf_subs.append()\n",
    "                uf_subs[u_id] = counterpart_eval(cat_N[\"mde\"].loc[u_id], c)\n",
    "        ufo_val_cpart[c] = uf_subs\n",
    "    # ufo_val_cpart\n",
    "    raw_data_ufo = raw_data.loc[:, nominal + discrete_ordinal].copy()\n",
    "    for c in ufo_val_cpart:\n",
    "        for k, v in ufo_val_cpart[c].items():\n",
    "            raw_data_ufo.loc[k, c] = v\n",
    "    raw_data_ufo_train = raw_data_ufo\n",
    "    return raw_data_ufo_train\n",
    "\n",
    "\n",
    "def link_outlier_categories_eval(raw_data, raw_data_eval):\n",
    "    ufo_val = {}\n",
    "    ufo_val_idx = {x: {} for x in nominal + discrete_ordinal}\n",
    "    u_i = []\n",
    "    ufo_idx = None\n",
    "    for c in nominal + discrete_ordinal:\n",
    "        i_ = raw_data[c].unique()\n",
    "        i__ = raw_data_eval[c].unique()\n",
    "        ufo = np.setdiff1d(i__, i_)\n",
    "        ufo_val[c] = list(ufo)\n",
    "        if len(ufo) > 0:\n",
    "            #         tmp = raw_data[raw_data[c]==list(ufo)[0]].index\n",
    "            for val in list(ufo):\n",
    "                ufo_val_idx[c][val] = raw_data_eval[raw_data_eval[c] == val].index\n",
    "\n",
    "    ufo_val_idx_ = copy.deepcopy(ufo_val_idx)\n",
    "    for i in ufo_val_idx:\n",
    "        if len(ufo_val_idx[i].keys()) == 0:\n",
    "            del ufo_val_idx_[i]\n",
    "    # import pprint\n",
    "    # pprint.pprint(ufo_val_idx_)\n",
    "    ufo_idx = []\n",
    "    for c in ufo_val_idx_:\n",
    "        for k in ufo_val_idx_[c]:\n",
    "            ufo_idx.extend(list(ufo_val_idx_[c][k]))\n",
    "    ufo_idx = list(set(ufo_idx))\n",
    "    trees_categ = [BallTree(X, leaf_size=1, metric=d_m) for d_m in d_metrics]\n",
    "    tree_comb = [BallTree(Tx, leaf_size=1, metric=d_m) for d_m in d_num_metrics]\n",
    "    # trees_num =\n",
    "\n",
    "    cat_n_idx = []\n",
    "    comb_n_idx = []\n",
    "    for tree in trees_categ:\n",
    "        d, idx = tree.query(X_.loc[ufo_idx, :], k=1)\n",
    "        cat_n_idx.append(idx.ravel())\n",
    "\n",
    "    for tree in tree_comb:\n",
    "        d, idx = tree.query(Tx_[ufo_idx, :], k=1)\n",
    "        comb_n_idx.append(idx.ravel())\n",
    "\n",
    "    cat_N = pd.DataFrame(index=ufo_idx)\n",
    "    for i in range(len(d_metrics)):\n",
    "        cat_N[i] = cat_n_idx[i]\n",
    "\n",
    "    for i in range(len(d_num_metrics)):\n",
    "        cat_N[\"comb__\" + str(i)] = comb_n_idx[i]\n",
    "\n",
    "    cat_N[\"mde\"] = cat_N.mode(axis=1)[0]\n",
    "\n",
    "    cat_N[\"mde\"] = cat_N[\"mde\"].astype(\"int\")\n",
    "    ufo_val_cpart = {x: {} for x in nominal + discrete_ordinal}\n",
    "    for c in ufo_val_idx_:\n",
    "        uf_subs = {}\n",
    "\n",
    "        for val in ufo_val_idx_[c]:\n",
    "            uf_ids = list(ufo_val_idx_[c][val])\n",
    "            for u_id in uf_ids:\n",
    "                # uf_subs.append()\n",
    "                uf_subs[u_id] = counterpart_train(cat_N[\"mde\"].loc[u_id], c)\n",
    "        ufo_val_cpart[c] = uf_subs\n",
    "\n",
    "    # ufo_val_cpart\n",
    "    raw_data_ufo = raw_data_eval.loc[:, nominal + discrete_ordinal].copy()\n",
    "    for c in ufo_val_cpart:\n",
    "        for k, v in ufo_val_cpart[c].items():\n",
    "            raw_data_ufo.loc[k, c] = v\n",
    "    raw_data_ufo_eval = raw_data_ufo\n",
    "    return raw_data_ufo_eval\n",
    "\n",
    "\n",
    "compactor = defaultdict(list)\n",
    "\n",
    "\n",
    "def compact_categories(raw_data, raw_data_eval):\n",
    "    def get_protected_by_feature():\n",
    "        discrete = nominal + discrete_ordinal\n",
    "        X = raw_data.copy()\n",
    "        # spec1 = X[(X[\"target\"] != 2)][discrete]\n",
    "        unq_f = {c: {} for c in discrete}\n",
    "        partitions = [X[(X[\"target\"] == i)][discrete] for i in range(3)]\n",
    "\n",
    "        cmn_f = {c: [] for c in discrete}\n",
    "        for c in discrete:\n",
    "            p_uniq = [p[c].unique() for p in partitions]\n",
    "            cmmn = np.intersect1d(p_uniq[0], p_uniq[1])\n",
    "            cmmn = np.intersect1d(cmmn, p_uniq[2])\n",
    "            cmn_f[c] = cmmn\n",
    "\n",
    "        unq_f = {c: {} for c in discrete}\n",
    "\n",
    "        for c in discrete:\n",
    "            t_uniq = [p[c].unique() for p in partitions]\n",
    "            # A - A ∩ (B U C)\n",
    "            targets = [0, 1, 2]\n",
    "            other_union = [np.array([]) for i in range(3)]\n",
    "\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    else:\n",
    "                        other_union[i] = np.union1d(other_union[i], t_uniq[j])\n",
    "            intersects = [np.intersect1d(t_uniq[i], other_union[i]) for i in range(3)]\n",
    "            spec_uniq = [np.setdiff1d(t_uniq[i], intersects[i]) for i in range(3)]\n",
    "            unq_f[c] = {i: spec_uniq[i] for i in range(3)}\n",
    "        #     unq_f = {k:v for k,v in unq_f.items() if len(v.keys())>0}\n",
    "        protected = {c: [] for c in nominal + discrete_ordinal}\n",
    "        for c in unq_f:\n",
    "            protected[c] = np.array([])\n",
    "            for v in unq_f[c].values():\n",
    "                protected[c] = np.union1d(protected[c], v).astype(\"int\")\n",
    "        return protected\n",
    "\n",
    "    protected = get_protected_by_feature()\n",
    "\n",
    "    def compact_nominal(threshold):\n",
    "        global compactor\n",
    "        X = raw_data.copy()\n",
    "        tmp_ = X[X[\"target\"] != 2]\n",
    "        #     minority = X[X[\"target\"] == 2]\n",
    "        for c in nominal:\n",
    "            vc = tmp_[c].value_counts().sort_values(ascending=False)\n",
    "            cum_vc = vc.cumsum(axis=0)\n",
    "            consolid = cum_vc[cum_vc > threshold * tmp_.shape[0]].index\n",
    "            consolid = [val for val in consolid if val not in protected[c]]\n",
    "            compactor[c] = list(consolid)\n",
    "\n",
    "    def compact_ordinal(threshold):\n",
    "        global compactor\n",
    "        X = raw_data.copy()\n",
    "        tmp_ = X[X[\"target\"] != 2]\n",
    "        #     minority = X[X[\"target\"] == 2]\n",
    "        for c in discrete_ordinal:\n",
    "            vc = tmp_[c].value_counts().sort_values(ascending=False)\n",
    "            cum_vc = vc.cumsum(axis=0)\n",
    "            consolid = cum_vc[cum_vc > threshold * tmp_.shape[0]].index\n",
    "            consolid = [val for val in consolid if val not in protected[c]]\n",
    "            compactor[c] = list(consolid)\n",
    "\n",
    "    def compact_discrete(row):\n",
    "        global compactor\n",
    "        for c in compactor:\n",
    "            if row[c] in compactor[c]:\n",
    "                row[c] = int(np.mean(compactor[c]))\n",
    "        return row\n",
    "\n",
    "    X = raw_data.copy()\n",
    "    compact_nominal(0.95)\n",
    "    X_eval = raw_data_eval.copy()\n",
    "    X = X.apply(compact_discrete, axis=1)\n",
    "    X_eval = X_eval.apply(compact_discrete, axis=1)\n",
    "    return X, X_eval\n",
    "\n",
    "\n",
    "def gen_ordinal__norm(raw_data=raw_data, raw_data_eval=raw_data_eval):\n",
    "    global column_directory\n",
    "    raw_data[\"ordinal__norm\"] = ordinal_norm(raw_data)\n",
    "    raw_data_eval[\"ordinal__norm\"] = ordinal_norm(raw_data_eval)\n",
    "    column_directory.add_feature(\"o\", \"ordinal__norm\")\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "\n",
    "def gen_ohe_data(raw_data=raw_data, raw_data_eval=raw_data_eval):\n",
    "    global column_directory\n",
    "    least_frequent_categories = {\n",
    "        \"nominal__v_12\": 33,\n",
    "        \"nominal__v_18\": 79,\n",
    "        \"nominal__v_20\": 61,\n",
    "        \"nominal__v_21\": 18,\n",
    "        \"nominal__v_25\": 25,\n",
    "        \"nominal__v_3\": 117,\n",
    "        \"nominal__v_32\": 77,\n",
    "        \"nominal__v_4\": 1,\n",
    "        \"ordinal__v_0\": 25,\n",
    "        \"ordinal__v_10\": 40,\n",
    "        \"ordinal__v_13\": 38,\n",
    "        \"ordinal__v_15\": 9,\n",
    "        \"ordinal__v_17\": 72,\n",
    "        \"ordinal__v_19\": 45,\n",
    "        \"ordinal__v_22\": 38,\n",
    "        \"ordinal__v_23\": 17,\n",
    "        \"ordinal__v_24\": 37,\n",
    "        \"ordinal__v_29\": 31,\n",
    "        \"ordinal__v_31\": 70,\n",
    "        \"ordinal__v_33\": 45,\n",
    "        \"ordinal__v_5\": 63,\n",
    "        \"ordinal__v_6\": 30,\n",
    "    }\n",
    "    drop_mask = np.array([least_frequent_categories[x] for x in nominal + ordinal])\n",
    "    X = pd.concat(\n",
    "        [raw_data.loc[:, raw_data_eval.columns], raw_data_eval],\n",
    "        ignore_index=True,\n",
    "        axis=0,\n",
    "    )\n",
    "    ohe = OneHotEncoder(sparse=False, dtype=np.int8)\n",
    "    ohe.fit(X.loc[:, nominal + ordinal])\n",
    "    nominal_ohe = [x for x in ohe.get_feature_names_out() if \"nominal__\" in x]\n",
    "    ordinal_ohe = [x for x in ohe.get_feature_names_out() if \"ordinal__\" in x]\n",
    "    ohe_data = pd.DataFrame(\n",
    "        ohe.transform(raw_data.loc[:, nominal + ordinal]),\n",
    "        columns=ohe.get_feature_names_out(),\n",
    "        index=raw_data.index,\n",
    "    )\n",
    "    ohe_data_eval = pd.DataFrame(\n",
    "        ohe.transform(raw_data_eval.loc[:, nominal + ordinal]),\n",
    "        columns=ohe.get_feature_names_out(),\n",
    "        index=raw_data_eval.index,\n",
    "    )\n",
    "    other_than_nominal = [x for x in raw_data_eval.columns if \"nominal__\" not in x]\n",
    "    raw_data_ohe = pd.concat([ohe_data, raw_data.loc[:, other_than_nominal]], axis=1)\n",
    "    raw_data_ohe_eval = pd.concat(\n",
    "        [ohe_data_eval, raw_data_eval.loc[:, other_than_nominal]], axis=1\n",
    "    )\n",
    "    column_directory.update_ohe_features(nominal_ohe, ordinal_ohe)\n",
    "    return raw_data_ohe, raw_data_ohe_eval\n",
    "\n",
    "\n",
    "def gen_binary_literal(row):\n",
    "    bin_chr = [str(row[b]) for b in binary if \"sum\" not in b]\n",
    "    b_l = \"\"\n",
    "    for c in bin_chr:\n",
    "        b_l += c\n",
    "    row.binary__literal = int(b_l, 2) / 255\n",
    "    return row\n",
    "\n",
    "\n",
    "def add_literal_to_data(raw_data, raw_data_eval):\n",
    "    global column_directory\n",
    "    b_lit_train = pd.DataFrame(columns=[\"binary__literal\"], index=raw_data.index)\n",
    "    b_lit_eval = pd.DataFrame(columns=[\"binary__literal\"], index=raw_data_eval.index)\n",
    "    raw_data[\"binary__literal\"] = \"0\"\n",
    "    raw_data_eval[\"binary__literal\"] = \"0\"\n",
    "    raw_data = raw_data.apply(gen_binary_literal, axis=1)\n",
    "    raw_data_eval = raw_data_eval.apply(gen_binary_literal, axis=1)\n",
    "    column_directory.add_feature(\"b\", \"binary__literal\")\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "\n",
    "def standardise_ordinal_data(raw_data, raw_data_eval, raw_data_ohe, raw_data_ohe_eval):\n",
    "    scaler = sklearn.preprocessing.Normalizer()\n",
    "    scaler.fit(X_master.loc[:, ordinal])\n",
    "    ordinal_data = pd.DataFrame(\n",
    "        scaler.transform(raw_data.loc[:, ordinal]),\n",
    "        columns=ordinal,\n",
    "        index=raw_data.index,\n",
    "    )\n",
    "    ordinal_data_eval = pd.DataFrame(\n",
    "        scaler.transform(raw_data_eval.loc[:, ordinal]),\n",
    "        columns=ordinal,\n",
    "        index=raw_data_eval.index,\n",
    "    )\n",
    "\n",
    "    #     raw_data.loc[:,ordinal] = ordinal_data\n",
    "    raw_data_ohe.loc[:, ordinal] = ordinal_data\n",
    "    #     raw_data_eval.loc[:,ordinal] = ordinal_data_eval\n",
    "    raw_data_ohe_eval.loc[:, ordinal] = ordinal_data_eval\n",
    "    return raw_data, raw_data_eval, raw_data_ohe, raw_data_ohe_eval\n",
    "\n",
    "\n",
    "def make_int_nominal(df):\n",
    "    global column_directory\n",
    "    nominal = column_directory.nominal_features()\n",
    "    df.loc[:, nominal] = df.loc[:, nominal].astype(\"int\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_transformations():\n",
    "    global X_master\n",
    "    raw_data, raw_data_eval = reset_data(remove_cache=True)\n",
    "\n",
    "    raw_data = make_int_nominal(raw_data)\n",
    "    raw_data_eval = make_int_nominal(raw_data_eval)\n",
    "\n",
    "    raw_data, raw_data_eval = binary_feature_sum(raw_data, raw_data_eval)\n",
    "\n",
    "    raw_data, raw_data_eval = standardise_ratio_features(raw_data, raw_data_eval)\n",
    "\n",
    "    raw_data, raw_data_eval = add_literal_to_data(raw_data, raw_data_eval)\n",
    "    raw_data.loc[:, nominal + discrete_ordinal] = link_outlier_categories_train(\n",
    "        raw_data, raw_data_eval\n",
    "    )\n",
    "    raw_data_eval.loc[:, nominal + discrete_ordinal] = link_outlier_categories_eval(\n",
    "        raw_data, raw_data_eval\n",
    "    )\n",
    "    raw_data, raw_data_eval = compact_categories(raw_data, raw_data_eval)\n",
    "    raw_data, raw_data_eval = ordinally_encode_nominal_data(raw_data, raw_data_eval)\n",
    "    raw_data = make_int_nominal(raw_data)\n",
    "    raw_data_eval = make_int_nominal(raw_data_eval)\n",
    "    raw_data_ohe, raw_data_ohe_eval = gen_ohe_data(raw_data, raw_data_eval)\n",
    "\n",
    "    raw_data, raw_data_eval = update_ordinal_data(raw_data, raw_data_eval)\n",
    "\n",
    "    raw_data, raw_data_eval, raw_data_ohe, raw_data_ohe_eval = standardise_ordinal_data(\n",
    "        raw_data, raw_data_eval, raw_data_ohe, raw_data_ohe_eval\n",
    "    )\n",
    "\n",
    "    raw_data, raw_data_eval = gen_ordinal__norm(raw_data, raw_data_eval)\n",
    "\n",
    "    raw_data_ohe[\"ordinal__norm\"] = raw_data[\"ordinal__norm\"]\n",
    "\n",
    "    raw_data_ohe_eval[\"ordinal__norm\"] = raw_data_eval[\"ordinal__norm\"]\n",
    "    write_raw_data(raw_data, raw_data_eval)\n",
    "    write_raw_data_ohe(raw_data_ohe, raw_data_ohe_eval)\n",
    "    X_master = pd.concat(\n",
    "        [raw_data.loc[:, raw_data_eval.columns], raw_data_eval],\n",
    "        ignore_index=True,\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "\n",
    "apply_transformations()\n",
    "raw_data, raw_data_eval = reload_raw_data()\n",
    "\n",
    "raw_data_ohe, raw_data_ohe_eval = reload_raw_data_ohe()\n",
    "\n",
    "\n",
    "def refresh_data():\n",
    "    global __refresh__\n",
    "    __refresh__ = 1\n",
    "    raw_data, raw_data_eval = reset_data()\n",
    "    # rename_columns_with_dtype()\n",
    "    apply_transformations()\n",
    "    raw_data, raw_data_eval = reload_raw_data()\n",
    "    __refresh__ = 0\n",
    "    print(\"Data Refresh Success\")\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "\n",
    "# raw_data, raw_data_eval = refresh_data()\n",
    "\n",
    "features = raw_data.drop([\"target\"], axis=1).columns\n",
    "\n",
    "\n",
    "def create_target_encoding():\n",
    "    new_column_names = []\n",
    "    stubs = [\"_0\", \"_1\", \"_2\"]\n",
    "    for c in ordinal + nominal:\n",
    "        for _suffix in stubs:\n",
    "            new_column_names.append(c + _suffix)\n",
    "    df_train = pd.DataFrame(index=raw_data.index, columns=new_column_names)\n",
    "    df_eval = pd.DataFrame(index=raw_data_eval.index, columns=new_column_names)\n",
    "    target_census = raw_data.target.value_counts().to_dict()\n",
    "    for c in ordinal + nominal:\n",
    "        target_counts = (\n",
    "            raw_data.groupby(c)[\"target\"].value_counts().unstack(fill_value=0)\n",
    "        )\n",
    "        tf: pd.DataFrame = target_counts.loc[:, [0, 1, 2]]\n",
    "        target_counts[\"sum\"] = tf.sum(axis=1)\n",
    "        target_counts[\"std\"] = tf.std(axis=1)\n",
    "        tf2 = tf.divide((target_counts[\"sum\"]), axis=\"rows\")\n",
    "        target_map = {0: 0, 1: 1, 2: 2}\n",
    "        for val in target_map:\n",
    "            mapping = defaultdict(int, tf2.to_dict()[target_map[val]])\n",
    "            df_train[c + stubs[val]] = raw_data[c].map(mapping)\n",
    "            df_eval[c + stubs[val]] = raw_data_eval[c].map(mapping)\n",
    "    return df_train, df_eval\n",
    "\n",
    "\n",
    "target_encoded_train, target_encoded_eval = create_target_encoding()\n",
    "\n",
    "\"\"\"\n",
    "Refreshing Data Bifurication\n",
    "\"\"\"\n",
    "\n",
    "ordinal = column_directory.ordinal_features()\n",
    "nominal = column_directory.nominal_features()\n",
    "binary = column_directory.binary_features()\n",
    "ratios = column_directory.ratio_features()\n",
    "nominal_ohe = column_directory.nominal_ohe_features()\n",
    "ordinal_ohe = column_directory.ordinal_ohe_features()\n",
    "ordinal_data, nominal_data, binary_data, ratios_data = column_directory.categorise_data(\n",
    "    raw_data\n",
    ")\n",
    "(\n",
    "    ordinal_data_eval,\n",
    "    nominal_data_eval,\n",
    "    binary_data_eval,\n",
    "    ratios_data_eval,\n",
    ") = column_directory.categorise_data(raw_data_eval)\n",
    "original_ordinal_data, original_ordinal_data_eval = reset_data(remove_cache=False)\n",
    "o_f = [x for x in original_ordinal_data.columns if \"ordinal\" in x]\n",
    "old_ordinal = original_ordinal_data.loc[:, o_f]\n",
    "old_ordinal_eval = original_ordinal_data_eval.loc[:, o_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dee40f2-8e00-485b-81bd-a1746542e07b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:34:47.996064Z",
     "iopub.status.busy": "2022-09-12T12:34:47.995801Z",
     "iopub.status.idle": "2022-09-12T12:34:48.029584Z",
     "shell.execute_reply": "2022-09-12T12:34:48.029097Z",
     "shell.execute_reply.started": "2022-09-12T12:34:47.996047Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cl_weight = sklearn.utils.compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=[0, 1, 2], y=y\n",
    ")\n",
    "CLASS_WEIGHTS = {i: cl_weight[i] for i in range(3)}\n",
    "def_cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=5, random_state=42)\n",
    "lgr_params = dict(\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    fit_intercept=False,\n",
    "    multi_class=\"ovr\",\n",
    "    max_iter=2000000,\n",
    "    random_state=42,\n",
    "    n_jobs=24,\n",
    "    #     penalty=\"elasticnet\",\n",
    "    cv=def_cv,\n",
    "    scoring=\"f1_macro\",\n",
    "    solver=\"lbfgs\",\n",
    "    Cs=100,\n",
    "    #     l1_ratios=np.linspace(0, 1, endpoint=False, num=100),\n",
    ")\n",
    "# from sklearnex import unpatch_sklearn\n",
    "\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "discrete = nominal + discrete_ordinal + discrete_binary\n",
    "X_master = pd.concat(\n",
    "    [raw_data.loc[:, raw_data_eval.columns], raw_data_eval], ignore_index=True, axis=0\n",
    ")\n",
    "class_priors = (raw_data.target.value_counts() / 3796).to_numpy()\n",
    "raw_data[discrete] = raw_data[discrete].astype(np.uint32)\n",
    "X_master[discrete] = X_master[discrete].astype(np.uint32)\n",
    "numeric = list(\n",
    "    np.setdiff1d(raw_data_eval.columns, discrete_binary + discrete_ordinal + nominal+['target'])\n",
    ")\n",
    "for c in raw_data_eval.columns:\n",
    "    if c not in numeric:\n",
    "        raw_data[c] = raw_data[c].astype(np.uint32)\n",
    "        raw_data_eval[c] = raw_data_eval[c].astype(np.uint32)\n",
    "raw_data['target'] = raw_data['target'].astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8a10f915-e69f-4a1f-86cd-ec93da22d472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:15:21.690453Z",
     "iopub.status.busy": "2022-09-12T13:15:21.690036Z",
     "iopub.status.idle": "2022-09-12T13:15:22.035287Z",
     "shell.execute_reply": "2022-09-12T13:15:22.034676Z",
     "shell.execute_reply.started": "2022-09-12T13:15:21.690410Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import has_fit_parameter\n",
    "\n",
    "# has_fit_parameter(clf, \"warm_start\")\n",
    "clf_list = sklearn.utils.all_estimators(type_filter=\"classifier\")\n",
    "props = {\"warm_start\": [], \"partial_fit\": []}\n",
    "errors = []\n",
    "for name, clf in clf_list:\n",
    "    try:\n",
    "        if \"warm_start\" in clf().get_params():\n",
    "            props[\"warm_start\"].append(clf)\n",
    "        try:\n",
    "            clf().partial_fit(\n",
    "                raw_data[raw_data_eval.columns].abs(),\n",
    "                raw_data.target,\n",
    "                classes=[0, 1, 2],\n",
    "            )\n",
    "            props[\"partial_fit\"].append(clf)\n",
    "        except AttributeError:\n",
    "            errors.append(name)\n",
    "\n",
    "    except TypeError:\n",
    "        errors.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02eb755d-3158-4102-a1c1-988d07a7bb0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:15:22.058391Z",
     "iopub.status.busy": "2022-09-12T13:15:22.058158Z",
     "iopub.status.idle": "2022-09-12T13:15:22.071240Z",
     "shell.execute_reply": "2022-09-12T13:15:22.070730Z",
     "shell.execute_reply.started": "2022-09-12T13:15:22.058371Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_logit_ordinal(X):\n",
    "    probit = {c: {} for c in discrete_ordinal}\n",
    "\n",
    "    for c in discrete_ordinal:\n",
    "        vc = X_master[c].value_counts()\n",
    "        N = X_master.shape[0]\n",
    "        vc = vc / N\n",
    "        probit_value = {val: 0 for val in X_master[c].unique()}\n",
    "        for val in X_master[c].unique():\n",
    "            probit_value[val] = np.log1p(\n",
    "                (vc[vc.index <= val].sum()) / (vc[vc.index > val].sum() + 1e-09)\n",
    "            )\n",
    "        probit[c] = probit_value\n",
    "\n",
    "    def transform_ordinal_to_logit(row):\n",
    "        for c in discrete_ordinal:\n",
    "            row[c] = probit[c][row[c]]\n",
    "        return row\n",
    "\n",
    "    probit_x = pd.DataFrame(X, columns=discrete_ordinal)\n",
    "    probit_x.apply(transform_ordinal_to_logit, axis=1)\n",
    "    return probit_x[discrete_ordinal].to_numpy()\n",
    "\n",
    "\n",
    "base_n = BaseNEncoder(cols=nominal,handle_unknown=0)\n",
    "numeric = list(\n",
    "    np.setdiff1d(raw_data_eval.columns, discrete_binary + discrete_ordinal + nominal)\n",
    ")\n",
    "tree_ready = make_column_transformer(\n",
    "    (base_n, nominal),\n",
    "    (\n",
    "        Normalizer(),\n",
    "        numeric,\n",
    "    ),\n",
    "    (\"passthrough\", discrete_binary + discrete_ordinal),\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "\n",
    "linear_ready = make_column_transformer(\n",
    "    #     (OneHotEncoder(sparse=False),nominal),\n",
    "    (BaseNEncoder(cols=nominal,handle_unknown=0), nominal),\n",
    "    (\n",
    "        FunctionTransformer(get_logit_ordinal, feature_names_out=\"one-to-one\"),\n",
    "        discrete_ordinal,\n",
    "    ),\n",
    "    (StandardScaler(), numeric),\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False,)\n",
    "ohe.fit(X_master[nominal])\n",
    "nom_categories = ohe.categories_\n",
    "bayesian_prep = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (OneHotEncoder(sparse=False,categories=nom_categories), nominal),\n",
    "        (\"passthrough\", discrete_binary + discrete_ordinal),\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0,\n",
    "    ),\n",
    "    FunctionTransformer(np.abs)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8052fdc-8cff-4a95-aa83-34cb58f6b6c0",
   "metadata": {},
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e756e0d3-cc17-43b9-9123-faf43dd3c7b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:15:22.788650Z",
     "iopub.status.busy": "2022-09-12T13:15:22.788300Z",
     "iopub.status.idle": "2022-09-12T13:15:24.528011Z",
     "shell.execute_reply": "2022-09-12T13:15:24.527463Z",
     "shell.execute_reply.started": "2022-09-12T13:15:22.788609Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = raw_data[raw_data_eval.columns]\n",
    "y = raw_data.target\n",
    "X_eval = raw_data_eval\n",
    "X_train, X_test, y_train, y_test = gen_train_test(X, y, test_size=0.3)\n",
    "c_msk = np.array([True] * 44 + [False] * 15 + [True] * 22)\n",
    "min_c = pd.DataFrame(bayesian_prep.fit_transform(X)).max().astype(np.uint32).to_list()\n",
    "min_c = [mc+1 for mc in min_c]\n",
    "c_msk = c_msk.reshape(\n",
    "    81,\n",
    ")\n",
    "b_clfs = []\n",
    "bayesian_model_data = bayesian_prep.fit_transform(X,y)\n",
    "bayesian_model_eval = bayesian_prep.transform(X_eval)\n",
    "\n",
    "t_clfs = []\n",
    "tree_model_data = tree_ready.fit_transform(X,y)\n",
    "tree_model_eval = tree_ready.transform(X_eval)\n",
    "\n",
    "l_clfs = []\n",
    "linear_model_data = linear_ready.fit_transform(X,y)\n",
    "linear_model_eval = linear_ready.transform(X_eval)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "svc_ = SVC(\n",
    "    probability=True,\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    random_state=42,\n",
    "    tol=1e-06,\n",
    "    kernel=\"rbf\",\n",
    "    gamma=\"scale\",\n",
    "    break_ties=True,\n",
    ")\n",
    "\n",
    "svc =BaggingClassifier( base_estimator=svc_,n_estimators=10,\n",
    "        random_state=42,\n",
    "        n_jobs=24,\n",
    "        bootstrap=False,\n",
    "        warm_start=True, )\n",
    "\n",
    "\n",
    "sgd = SGDClassifier(\n",
    "    warm_start=True,\n",
    "    average=True,\n",
    "    random_state=42,\n",
    "    fit_intercept=False,\n",
    "    n_iter_no_change=50,\n",
    "    loss=\"log_loss\",\n",
    "    max_iter=10000,class_weight=CLASS_WEIGHTS\n",
    ")\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    warm_start=True,\n",
    "    categorical_features=c_msk,\n",
    "    random_state=42,\n",
    "    scoring=\"f1_macro\",\n",
    "    max_iter=500,\n",
    "    early_stopping=\"auto\",\n",
    "    n_iter_no_change=50,\n",
    "\n",
    ")\n",
    "rfc = RandomForestClassifier(\n",
    "    warm_start=True,\n",
    "    min_impurity_decrease=0.00055,\n",
    "    n_jobs=24,\n",
    "    random_state=42,\n",
    "    max_features=None,\n",
    "    class_weight=CLASS_WEIGHTS\n",
    ")\n",
    "efc = ExtraTreesClassifier(\n",
    "    warm_start=True,\n",
    "    min_impurity_decrease=0.00055,\n",
    "    n_jobs=24,\n",
    "    random_state=42,\n",
    "    max_features=None,\n",
    "    class_weight=CLASS_WEIGHTS\n",
    ")\n",
    "ALPHA = 1.0e-10\n",
    "cnb = CategoricalNB(alpha=ALPHA, fit_prior=True, min_categories=min_c)\n",
    "conb = ComplementNB(alpha=ALPHA, fit_prior=True, norm=False)\n",
    "benb = BernoulliNB(binarize=False, alpha=ALPHA, fit_prior=True)\n",
    "munb = MultinomialNB(alpha=ALPHA, fit_prior=True)\n",
    "\n",
    "\n",
    "def print_clf_perf(clf,X_test,y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\":\"*80)\n",
    "\n",
    "    \n",
    "    \n",
    "for bayesian_learner in [conb, benb, munb]:\n",
    "    b_clfs.append((bayesian_learner.__class__.__name__, bayesian_learner))\n",
    "\n",
    "for tree_learner in [hgb, rfc, efc]:\n",
    "    t_clfs.append((tree_learner.__class__.__name__, tree_learner))\n",
    "\n",
    "for linear_learner in [svc, sgd]:\n",
    "    l_clfs.append((linear_learner.__class__.__name__, linear_learner))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3189102e-8ab7-41ba-9c56-9421321e45e0",
   "metadata": {},
   "source": [
    "### BASE PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95ee8cc7-5d1f-491e-bef9-d39c121741d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:15:24.529699Z",
     "iopub.status.busy": "2022-09-12T13:15:24.529442Z",
     "iopub.status.idle": "2022-09-12T13:15:39.000724Z",
     "shell.execute_reply": "2022-09-12T13:15:39.000011Z",
     "shell.execute_reply.started": "2022-09-12T13:15:24.529683Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComplementNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.68      0.66       373\n",
      "           1       0.66      0.85      0.74       559\n",
      "           2       0.43      0.05      0.09       207\n",
      "\n",
      "    accuracy                           0.65      1139\n",
      "   macro avg       0.58      0.53      0.50      1139\n",
      "weighted avg       0.61      0.65      0.60      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "BernoulliNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.56      0.63       373\n",
      "           1       0.64      0.78      0.71       559\n",
      "           2       0.27      0.21      0.24       207\n",
      "\n",
      "    accuracy                           0.61      1139\n",
      "   macro avg       0.54      0.52      0.52      1139\n",
      "weighted avg       0.60      0.61      0.59      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "MultinomialNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.62      0.65       373\n",
      "           1       0.71      0.81      0.76       559\n",
      "           2       0.44      0.36      0.39       207\n",
      "\n",
      "    accuracy                           0.66      1139\n",
      "   macro avg       0.61      0.59      0.60      1139\n",
      "weighted avg       0.65      0.66      0.66      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "HistGradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.68      0.71       373\n",
      "           1       0.75      0.90      0.81       559\n",
      "           2       0.48      0.29      0.37       207\n",
      "\n",
      "    accuracy                           0.71      1139\n",
      "   macro avg       0.66      0.62      0.63      1139\n",
      "weighted avg       0.70      0.71      0.70      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.70      0.74       373\n",
      "           1       0.75      0.92      0.83       559\n",
      "           2       0.58      0.35      0.44       207\n",
      "\n",
      "    accuracy                           0.75      1139\n",
      "   macro avg       0.71      0.66      0.67      1139\n",
      "weighted avg       0.73      0.75      0.73      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "ExtraTreesClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       373\n",
      "           1       0.74      0.90      0.81       559\n",
      "           2       0.55      0.40      0.46       207\n",
      "\n",
      "    accuracy                           0.74      1139\n",
      "   macro avg       0.70      0.66      0.67      1139\n",
      "weighted avg       0.73      0.74      0.73      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.68      0.72       373\n",
      "           1       0.74      0.89      0.81       559\n",
      "           2       0.51      0.32      0.39       207\n",
      "\n",
      "    accuracy                           0.72      1139\n",
      "   macro avg       0.67      0.63      0.64      1139\n",
      "weighted avg       0.70      0.72      0.70      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "SGDClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.69      0.73       373\n",
      "           1       0.76      0.85      0.80       559\n",
      "           2       0.47      0.41      0.44       207\n",
      "\n",
      "    accuracy                           0.72      1139\n",
      "   macro avg       0.67      0.65      0.66      1139\n",
      "weighted avg       0.71      0.72      0.71      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n"
     ]
    }
   ],
   "source": [
    "for clf_name,clf in b_clfs:\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(bayesian_model_data, y, test_size=0.3)\n",
    "    print(clf_name)\n",
    "    y_pred_base = clf.fit(X_train, y_train).predict(X_test)\n",
    "    print(classification_report(y_test, y_pred_base))\n",
    "    print(\":\"*80)\n",
    "    \n",
    "for clf_name,clf in t_clfs:\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(tree_model_data, y, test_size=0.3)\n",
    "    print(clf_name)\n",
    "    y_pred_base = clf.fit(X_train, y_train).predict(X_test)\n",
    "    print(classification_report(y_test, y_pred_base))\n",
    "    print(\":\"*80)\n",
    "    \n",
    "for clf_name,clf in l_clfs:\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(linear_model_data, y, test_size=0.3)\n",
    "    print(clf_name)\n",
    "    y_pred_base = clf.fit(X_train, y_train).predict(X_test)\n",
    "    print(classification_report(y_test, y_pred_base))\n",
    "    print(\":\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8aa8984d-7777-4b2c-9e99-a20f27c3e77c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:15:39.003447Z",
     "iopub.status.busy": "2022-09-12T13:15:39.003150Z",
     "iopub.status.idle": "2022-09-12T13:16:32.607988Z",
     "shell.execute_reply": "2022-09-12T13:16:32.606988Z",
     "shell.execute_reply.started": "2022-09-12T13:15:39.003426Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CategoricalNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72       373\n",
      "           1       0.78      0.81      0.79       559\n",
      "           2       0.49      0.55      0.51       207\n",
      "\n",
      "    accuracy                           0.72      1139\n",
      "   macro avg       0.68      0.68      0.68      1139\n",
      "weighted avg       0.72      0.72      0.72      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "ComplementNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.70      0.67       373\n",
      "           1       0.67      0.84      0.74       559\n",
      "           2       0.65      0.07      0.13       207\n",
      "\n",
      "    accuracy                           0.66      1139\n",
      "   macro avg       0.65      0.54      0.51      1139\n",
      "weighted avg       0.65      0.66      0.61      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "BernoulliNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.62      0.68       373\n",
      "           1       0.66      0.80      0.73       559\n",
      "           2       0.37      0.28      0.31       207\n",
      "\n",
      "    accuracy                           0.65      1139\n",
      "   macro avg       0.60      0.57      0.57      1139\n",
      "weighted avg       0.64      0.65      0.64      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "MultinomialNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.64      0.66       373\n",
      "           1       0.74      0.83      0.78       559\n",
      "           2       0.52      0.43      0.47       207\n",
      "\n",
      "    accuracy                           0.69      1139\n",
      "   macro avg       0.65      0.63      0.64      1139\n",
      "weighted avg       0.68      0.69      0.69      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "HistGradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.66      0.70       373\n",
      "           1       0.74      0.90      0.81       559\n",
      "           2       0.49      0.30      0.37       207\n",
      "\n",
      "    accuracy                           0.71      1139\n",
      "   macro avg       0.66      0.62      0.63      1139\n",
      "weighted avg       0.69      0.71      0.69      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.70      0.74       373\n",
      "           1       0.75      0.92      0.83       559\n",
      "           2       0.58      0.35      0.44       207\n",
      "\n",
      "    accuracy                           0.75      1139\n",
      "   macro avg       0.71      0.66      0.67      1139\n",
      "weighted avg       0.73      0.75      0.73      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "ExtraTreesClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       373\n",
      "           1       0.74      0.90      0.81       559\n",
      "           2       0.55      0.40      0.46       207\n",
      "\n",
      "    accuracy                           0.74      1139\n",
      "   macro avg       0.70      0.66      0.67      1139\n",
      "weighted avg       0.73      0.74      0.73      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "SGDClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.62      0.64       373\n",
      "           1       0.72      0.81      0.76       559\n",
      "           2       0.40      0.32      0.35       207\n",
      "\n",
      "    accuracy                           0.66      1139\n",
      "   macro avg       0.60      0.58      0.59      1139\n",
      "weighted avg       0.65      0.66      0.65      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.68      0.72       373\n",
      "           1       0.74      0.89      0.81       559\n",
      "           2       0.51      0.32      0.39       207\n",
      "\n",
      "    accuracy                           0.72      1139\n",
      "   macro avg       0.67      0.63      0.64      1139\n",
      "weighted avg       0.70      0.72      0.70      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading',n_jobs=24):  \n",
    "    X_train, X_test, y_train, y_test = gen_train_test(bayesian_model_data, y, test_size=0.3)\n",
    "#     y_pred_base = clf.fit(X_train, y_train).predict(X_test)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=2)\n",
    "    for train_index, test_index in cv.split(X_train, y_train):\n",
    "        X_train_, X_test_ = bayesian_model_data[train_index, :], bayesian_model_data[test_index, :]\n",
    "        y_train_, y_test_ = y.loc[train_index], y.loc[test_index]\n",
    "        #     plug.fit(X_test_, y_test_)\n",
    "        cnb.partial_fit(X_train_, y_train_, classes=[0,1,2])\n",
    "        cnb.partial_fit(X_test_, y_test_, classes=[0,1,2])\n",
    "        conb.partial_fit(X_train_, y_train_, classes=[0,1,2])\n",
    "        conb.partial_fit(X_test_, y_test_, classes=[0,1,2])\n",
    "        benb.partial_fit(X_train_, y_train_, classes=[0,1,2])\n",
    "        benb.partial_fit(X_test_, y_test_, classes=[0,1,2])\n",
    "        munb.partial_fit(X_train_, y_train_, classes=[0,1,2])\n",
    "        munb.partial_fit(X_test_, y_test_, classes=[0,1,2])\n",
    "    print_clf_perf(cnb,X_test,y_test)\n",
    "    print_clf_perf(conb,X_test,y_test)\n",
    "    print_clf_perf(benb,X_test,y_test)\n",
    "    print_clf_perf(munb,X_test,y_test)\n",
    "    \n",
    "\n",
    "#     for clf_name,clf in t_clfs:\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(tree_model_data, y, test_size=0.3)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=2)\n",
    "    for train_index, test_index in cv.split(X_train, y_train):\n",
    "        X_train_, X_test_ = tree_model_data[train_index, :], tree_model_data[test_index, :]\n",
    "        y_train_, y_test_ = y.loc[train_index], y.loc[test_index]\n",
    "        #     plug.fit(X_test_, y_test_)\n",
    "        hgb.fit(X_train_, y_train_)\n",
    "        hgb.fit(X_test_, y_test_)\n",
    "        rfc.fit(X_train_, y_train_)\n",
    "        rfc.fit(X_test_, y_test_)\n",
    "        efc.fit(X_train_, y_train_)\n",
    "        efc.fit(X_test_, y_test_)\n",
    "    print_clf_perf(hgb,X_test,y_test)\n",
    "    print_clf_perf(rfc,X_test,y_test)\n",
    "    print_clf_perf(efc,X_test,y_test)\n",
    "    \n",
    "\n",
    "#     for clf_name,clf in l_clfs:\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(linear_model_data, y, test_size=0.3)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=2)\n",
    "    for train_index, test_index in cv.split(X_train, y_train):\n",
    "        X_train_, X_test_ = linear_model_data[train_index, :], linear_model_data[test_index, :]\n",
    "        y_train_, y_test_ = y.loc[train_index], y.loc[test_index]\n",
    "        #     plug.fit(X_test_, y_test_)\n",
    "        sgd.fit(X_train_, y_train_)\n",
    "        sgd.fit(X_test_, y_test_)\n",
    "        svc.fit(X_train_, y_train_)\n",
    "        svc.fit(X_test_, y_test_)\n",
    "    print_clf_perf(sgd,X_test,y_test)\n",
    "    print_clf_perf(svc,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b609571-0c49-4d89-8d13-e3e13bd05901",
   "metadata": {},
   "source": [
    "## COMBINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5fccab5e-268e-4c85-91ce-5f66819a7494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:16:32.611802Z",
     "iopub.status.busy": "2022-09-12T13:16:32.611405Z",
     "iopub.status.idle": "2022-09-12T13:16:32.622016Z",
     "shell.execute_reply": "2022-09-12T13:16:32.621206Z",
     "shell.execute_reply.started": "2022-09-12T13:16:32.611762Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vc_tree = VotingClassifier(estimators = [\n",
    "    ('1',hgb),\n",
    "    ('3',rfc),\n",
    "    ('4',efc),\n",
    "],\n",
    "                           voting='soft' ,\n",
    "                           n_jobs=24,\n",
    ")\n",
    "\n",
    "vc_ll = VotingClassifier(estimators=[\n",
    "    ('2',sgd),\n",
    "    ('9',svc),\n",
    "    \n",
    "] ,voting='soft' ,\n",
    "                           n_jobs=24,)\n",
    "\n",
    "vc_nb = VotingClassifier(estimators=[\n",
    "    ('5',cnb),\n",
    "    ('7',munb),\n",
    "    ('8',conb),],voting='soft' ,\n",
    "                           n_jobs=24,)\n",
    "\n",
    "\n",
    "fe = LogisticRegressionCV(**lgr_params)\n",
    "# fe = MLPClassifier(warm_start=True,random_state=42)\n",
    "\n",
    "SC_tree = StackingClassifier(estimators = [\n",
    "    ('1',hgb),\n",
    "    ('3',rfc),\n",
    "    ('4',efc),\n",
    "],\n",
    "                           cv='prefit',\n",
    "                             final_estimator=fe,\n",
    "                           n_jobs=24,\n",
    ")\n",
    "SC_nb = StackingClassifier(estimators=[\n",
    "    \n",
    "#     ('5',cnb),\n",
    "#     ('6',benb),\n",
    "    ('7',munb),\n",
    "    ('8',conb),\n",
    "], \n",
    "                           cv='prefit',\n",
    "                           final_estimator=fe,\n",
    "                           n_jobs=24,\n",
    "                        \n",
    "                        )\n",
    "SC_ll = StackingClassifier(estimators=[\n",
    "    ('2',sgd),\n",
    "    ('9',svc),\n",
    "    \n",
    "] , \n",
    "                           cv='prefit',\n",
    "                           final_estimator=fe,\n",
    "                           n_jobs=24,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "011f10d6-141f-4410-8684-e0f6b16364a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:16:32.623541Z",
     "iopub.status.busy": "2022-09-12T13:16:32.623139Z",
     "iopub.status.idle": "2022-09-12T13:16:48.902819Z",
     "shell.execute_reply": "2022-09-12T13:16:48.902300Z",
     "shell.execute_reply.started": "2022-09-12T13:16:32.623516Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VC_NB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.67       373\n",
      "           1       0.72      0.81      0.76       559\n",
      "           2       0.46      0.32      0.38       207\n",
      "\n",
      "    accuracy                           0.67      1139\n",
      "   macro avg       0.62      0.60      0.60      1139\n",
      "weighted avg       0.66      0.67      0.66      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "LL\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.70      0.73       373\n",
      "           1       0.75      0.86      0.80       559\n",
      "           2       0.47      0.36      0.41       207\n",
      "\n",
      "    accuracy                           0.71      1139\n",
      "   macro avg       0.66      0.64      0.65      1139\n",
      "weighted avg       0.70      0.71      0.71      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "vc_tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.68      0.72       373\n",
      "           1       0.74      0.91      0.82       559\n",
      "           2       0.50      0.31      0.39       207\n",
      "\n",
      "    accuracy                           0.72      1139\n",
      "   macro avg       0.67      0.63      0.64      1139\n",
      "weighted avg       0.71      0.72      0.71      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Voter_Training\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_train_test(bayesian_model_data, y, test_size=0.3)\n",
    "print(\"VC_NB\")\n",
    "y_pred_base = vc_nb.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "print(\":\"*80)\n",
    "X_train, X_test, y_train, y_test = gen_train_test(linear_model_data, y, test_size=0.3)\n",
    "print(\"LL\")\n",
    "y_pred_base = vc_ll.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "print(\":\"*80)\n",
    "X_train, X_test, y_train, y_test = gen_train_test(tree_model_data, y, test_size=0.3)\n",
    "print(\"vc_tree\")\n",
    "y_pred_base = vc_tree.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "print(\":\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e168cb5d-29a9-44c6-96d7-5c679c932260",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:16:48.904457Z",
     "iopub.status.busy": "2022-09-12T13:16:48.904082Z",
     "iopub.status.idle": "2022-09-12T13:17:01.017040Z",
     "shell.execute_reply": "2022-09-12T13:17:01.016462Z",
     "shell.execute_reply.started": "2022-09-12T13:16:48.904432Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC_NB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.64      0.66       373\n",
      "           1       0.77      0.78      0.78       559\n",
      "           2       0.50      0.54      0.52       207\n",
      "\n",
      "    accuracy                           0.69      1139\n",
      "   macro avg       0.65      0.66      0.65      1139\n",
      "weighted avg       0.70      0.69      0.69      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "SC_LL\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.63      0.69       373\n",
      "           1       0.80      0.81      0.80       559\n",
      "           2       0.42      0.54      0.47       207\n",
      "\n",
      "    accuracy                           0.70      1139\n",
      "   macro avg       0.66      0.66      0.66      1139\n",
      "weighted avg       0.72      0.70      0.71      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "vsc_tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.64      0.70       373\n",
      "           1       0.73      0.91      0.81       559\n",
      "           2       0.50      0.33      0.40       207\n",
      "\n",
      "    accuracy                           0.72      1139\n",
      "   macro avg       0.67      0.63      0.64      1139\n",
      "weighted avg       0.71      0.72      0.70      1139\n",
      "\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stack Training\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_train_test(bayesian_model_data, y, test_size=0.3)\n",
    "print(\"SC_NB\")\n",
    "y_pred_base = SC_nb.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "print(\":\"*80)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_train_test(linear_model_data, y, test_size=0.3)\n",
    "print(\"SC_LL\")\n",
    "y_pred_base = SC_ll.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "print(\":\"*80)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_train_test(tree_model_data, y, test_size=0.3)\n",
    "print(\"vsc_tree\")\n",
    "y_pred_base = SC_tree.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "print(\":\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea492865-f641-4f52-8fac-b2bdfbb1c134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:17:01.021487Z",
     "iopub.status.busy": "2022-09-12T13:17:01.020080Z",
     "iopub.status.idle": "2022-09-12T13:17:51.197392Z",
     "shell.execute_reply": "2022-09-12T13:17:51.196320Z",
     "shell.execute_reply.started": "2022-09-12T13:17:01.021456Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with parallel_backend('threading',n_jobs=24):\n",
    "    for clf in SC_nb.estimators_:\n",
    "        X_train, X_test, y_train, y_test = gen_train_test(bayesian_model_data, y, test_size=0.3)\n",
    "    #     y_pred_base = clf.fit(X_train, y_train).predict(X_test)\n",
    "        cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=2)\n",
    "        for train_index, test_index in cv.split(X_train, y_train):\n",
    "            X_train_, X_test_ = bayesian_model_data[train_index, :], bayesian_model_data[test_index, :]\n",
    "            y_train_, y_test_ = y.loc[train_index], y.loc[test_index]\n",
    "            #     plug.fit(X_test_, y_test_)\n",
    "            clf.partial_fit(X_train_, y_train_, classes=[0,1,2])\n",
    "            clf.partial_fit(X_test_, y_test_, classes=[0,1,2])\n",
    "            \n",
    "    \n",
    "\n",
    "    for clf in SC_tree.estimators_:\n",
    "        X_train, X_test, y_train, y_test = gen_train_test(tree_model_data, y, test_size=0.3)\n",
    "        cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=2)\n",
    "        for train_index, test_index in cv.split(X_train, y_train):\n",
    "            X_train_, X_test_ = tree_model_data[train_index, :], tree_model_data[test_index, :]\n",
    "            y_train_, y_test_ = y.loc[train_index], y.loc[test_index]\n",
    "            #     plug.fit(X_test_, y_test_)\n",
    "            clf.fit(X_train_, y_train_)\n",
    "            clf.fit(X_test_, y_test_)\n",
    "            \n",
    "\n",
    "    for clf in SC_ll.estimators_:\n",
    "        X_train, X_test, y_train, y_test = gen_train_test(linear_model_data, y, test_size=0.3)\n",
    "        cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=2)\n",
    "        for train_index, test_index in cv.split(X_train, y_train):\n",
    "            X_train_, X_test_ = linear_model_data[train_index, :], linear_model_data[test_index, :]\n",
    "            y_train_, y_test_ = y.loc[train_index], y.loc[test_index]\n",
    "            clf.fit(X_train_, y_train_)\n",
    "            clf.fit(X_test_, y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "266c2062-c3bc-43a5-a0d3-b2e17d31013b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:17:51.198732Z",
     "iopub.status.busy": "2022-09-12T13:17:51.198591Z",
     "iopub.status.idle": "2022-09-12T13:17:51.204921Z",
     "shell.execute_reply": "2022-09-12T13:17:51.204256Z",
     "shell.execute_reply.started": "2022-09-12T13:17:51.198715Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MultinomialNB(alpha=1e-10), ComplementNB(alpha=1e-10)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SC_nb.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f5798def-e47b-453c-b7d7-4c8dc4d14dc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:54:52.323708Z",
     "iopub.status.busy": "2022-09-12T13:54:52.323286Z",
     "iopub.status.idle": "2022-09-12T13:55:10.542348Z",
     "shell.execute_reply": "2022-09-12T13:55:10.541777Z",
     "shell.execute_reply.started": "2022-09-12T13:54:52.323664Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC_NB\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "SC_LL\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "vsc_tree\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stack Predicting\n",
    "\"\"\"\n",
    "\n",
    "# X_train, X_test, y_train, y_test = gen_train_test(bayesian_model_data, y, test_size=0.3)\n",
    "print(\"SC_NB\")\n",
    "bayesian_model_eval = pd.DataFrame(bayesian_model_eval).fillna(0)\n",
    "\n",
    "y_pred_base_1 = SC_nb.fit(bayesian_model_data, y).predict(bayesian_model_eval)\n",
    "# print(classification_report(y_test, y_pred_base))\n",
    "print(\":\"*80)\n",
    "\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = gen_train_test(linear_model_data, y, test_size=0.3)\n",
    "print(\"SC_LL\")\n",
    "linear_model_eval = pd.DataFrame(linear_model_eval).fillna(0)\n",
    "y_pred_base_2 = SC_ll.fit(linear_model_data, y).predict(linear_model_eval)\n",
    "# print(classification_report(y_test, y_pred_base))\n",
    "print(\":\"*80)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = gen_train_test(tree_model_data, y, test_size=0.3)\n",
    "print(\"vsc_tree\")\n",
    "tree_model_eval = pd.DataFrame(tree_model_eval).fillna(0)\n",
    "\n",
    "y_pred_base_3 = SC_tree.fit(tree_model_data, y).predict(tree_model_eval)\n",
    "# print(classification_report(y_test, y_pred_base))\n",
    "print(\":\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa44443f-c0f3-46f1-9fb7-f31844501680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:18:04.483453Z",
     "iopub.status.busy": "2022-09-12T13:18:04.483313Z",
     "iopub.status.idle": "2022-09-12T13:18:04.486399Z",
     "shell.execute_reply": "2022-09-12T13:18:04.485877Z",
     "shell.execute_reply.started": "2022-09-12T13:18:04.483434Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_proba = [\n",
    "#     SC_nb.predict_proba(bayesian_model_eval),\n",
    "#     SC_ll.predict_proba(linear_model_eval),\n",
    "#     SC_tree.predict_proba(tree_model_eval),\n",
    "#     vc_nb.predict_proba(bayesian_model_eval),\n",
    "#     vc_ll.predict_proba(linear_model_eval),\n",
    "#     vc_tree.predict_proba(tree_model_eval),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "195f4e18-e94a-435b-b9c8-65b7b0d9fc99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:08:25.640117Z",
     "iopub.status.busy": "2022-09-12T13:08:25.639758Z",
     "iopub.status.idle": "2022-09-12T13:08:25.648718Z",
     "shell.execute_reply": "2022-09-12T13:08:25.647490Z",
     "shell.execute_reply.started": "2022-09-12T13:08:25.640076Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(tree_model_eval).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a52378cd-4b4a-401b-acb4-ea259644b57e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:02:13.366743Z",
     "iopub.status.busy": "2022-09-12T13:02:13.366388Z",
     "iopub.status.idle": "2022-09-12T13:02:13.376204Z",
     "shell.execute_reply": "2022-09-12T13:02:13.375158Z",
     "shell.execute_reply.started": "2022-09-12T13:02:13.366702Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(bayesian_model_eval).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0340e652-110e-4c24-bff3-6c95b171faf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:09:55.098882Z",
     "iopub.status.busy": "2022-09-12T13:09:55.098500Z",
     "iopub.status.idle": "2022-09-12T13:09:55.111460Z",
     "shell.execute_reply": "2022-09-12T13:09:55.110634Z",
     "shell.execute_reply.started": "2022-09-12T13:09:55.098842Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "     ..\n",
       "76    0\n",
       "77    0\n",
       "78    0\n",
       "79    0\n",
       "80    0\n",
       "Length: 81, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model_eval = pd.DataFrame(linear_model_eval).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a8d410b-99ca-4431-b5b4-3ab454155d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:55:54.081969Z",
     "iopub.status.busy": "2022-09-12T13:55:54.081566Z",
     "iopub.status.idle": "2022-09-12T13:55:54.088219Z",
     "shell.execute_reply": "2022-09-12T13:55:54.087012Z",
     "shell.execute_reply.started": "2022-09-12T13:55:54.081925Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tes = np.c_[y_pred_base_1,y_pred_base_2,y_pred_base_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aa1fe06d-d5be-405f-8144-b7abc21f0f15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:56:31.043414Z",
     "iopub.status.busy": "2022-09-12T13:56:31.043005Z",
     "iopub.status.idle": "2022-09-12T13:56:31.049554Z",
     "shell.execute_reply": "2022-09-12T13:56:31.048464Z",
     "shell.execute_reply.started": "2022-09-12T13:56:31.043370Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "q = list(tes.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "636f644d-d7eb-4d78-9e99-2e210377b0b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:57:13.296627Z",
     "iopub.status.busy": "2022-09-12T13:57:13.296279Z",
     "iopub.status.idle": "2022-09-12T13:57:13.317757Z",
     "shell.execute_reply": "2022-09-12T13:57:13.316946Z",
     "shell.execute_reply.started": "2022-09-12T13:57:13.296588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1169"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iw = 0\n",
    "for r in q:\n",
    "    if r==0 or r==3 or r==6:\n",
    "        iw+=1\n",
    "iw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ef489d7e-effe-43c1-b4c0-bd7a485adaca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:57:25.368988Z",
     "iopub.status.busy": "2022-09-12T13:57:25.368584Z",
     "iopub.status.idle": "2022-09-12T13:57:25.376587Z",
     "shell.execute_reply": "2022-09-12T13:57:25.375571Z",
     "shell.execute_reply.started": "2022-09-12T13:57:25.368945Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7180589680589681"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1169/1628\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
