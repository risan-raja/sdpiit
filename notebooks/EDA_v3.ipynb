{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9948c5b4-a453-4944-b39f-344e510366da",
   "metadata": {},
   "source": [
    "## Library Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2e6c7-05dc-4363-8c66-77bbf264db2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import (\n",
    "    BackwardDifferenceEncoder,\n",
    "    BaseNEncoder,\n",
    "    BinaryEncoder,\n",
    "    CatBoostEncoder,\n",
    "    CountEncoder,\n",
    "    GLMMEncoder,\n",
    "    HelmertEncoder,\n",
    "    JamesSteinEncoder,\n",
    "    LeaveOneOutEncoder,\n",
    "    MEstimateEncoder,\n",
    "    SummaryEncoder,\n",
    "    TargetEncoder,\n",
    "    WOEEncoder,\n",
    ")\n",
    "from sklearn import set_config\n",
    "from sklearn.base import clone as model_clone\n",
    "from sklearn.cluster import *\n",
    "from sklearn.compose import *\n",
    "from sklearn.cross_decomposition import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.gaussian_process import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.multioutput import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.utils import *\n",
    "from sklearn.dummy import *\n",
    "from sklearn.semi_supervised import *\n",
    "from sklearn.discriminant_analysis import *\n",
    "from sklearn.covariance import *\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "\n",
    "from sklearn.calibration import *\n",
    "import joblib\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "set_config(display=\"diagram\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline as make_imb_pipeline\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "from imblearn.over_sampling import ADASYN,SMOTE,RandomOverSampler,SVMSMOTE,SMOTENC,SMOTEN,BorderlineSMOTE,KMeansSMOTE\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "from joblib import parallel_backend\n",
    "from joblib.memory import Memory\n",
    "\n",
    "\n",
    "def allow_stopping(func):\n",
    "    def wrapper():\n",
    "        try:\n",
    "            value = func()\n",
    "            return value\n",
    "            # gc.collect()\n",
    "        except KeyboardInterrupt as e:\n",
    "            print(\"Program Stopped\")\n",
    "        gc.collect()\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "__refresh__ = 1\n",
    "\n",
    "\n",
    "def run_if_refresh(func):\n",
    "    def wrapper():\n",
    "        global __refresh__\n",
    "        if __refresh__ == 1:\n",
    "            value = func()\n",
    "            return value\n",
    "        else:\n",
    "            print(\n",
    "                \"Using Cache, Set Refresh to '__refresh__=1' to regenerate \"\n",
    "                \"the output of this  function\"\n",
    "            )\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def gen_train_test(X, y, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=10\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def categorise_input(X: pd.DataFrame):\n",
    "    X.loc[:, nominal + ordinal] = X.loc[:, nominal + ordinal].astype(\"category\")\n",
    "    return X\n",
    "\n",
    "KAGGLE_ENV = 1\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "if \"mlop3n/Pycharm\" in cwd:\n",
    "    KAGGLE_ENV = 0\n",
    "    \n",
    "if KAGGLE_ENV==1:\n",
    "    \n",
    "    categoriser = FunctionTransformer(categorise_input, feature_names_out=\"one-to-one\")\n",
    "categoriser = FunctionTransformer(categorise_input, feature_names_out=\"one-to-one\")\n",
    "\n",
    "\n",
    "def quick_test(X):\n",
    "    clfs = [\n",
    "        RandomForestClassifier(class_weight=\"balanced_subsample\", random_state=42),\n",
    "        DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "        HistGradientBoostingClassifier(random_state=42),\n",
    "        LogisticRegressionCV(max_iter=1000, class_weight=\"balanced\", random_state=42),\n",
    "    ]\n",
    "    y = raw_data.target\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(X, y, test_size=0.5)\n",
    "    for clf in clfs:\n",
    "        y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "        score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        print(f\"{clf.__class__.__name__} :: {score}\")\n",
    "\n",
    "\n",
    "def find_correlated_features(df, threshold=0.8):\n",
    "    correlated_features = set()\n",
    "    correlation_matrix = df.corr()\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                colname = correlation_matrix.columns[i]\n",
    "                correlated_features.add(colname)\n",
    "    return list(correlated_features)\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "PCA MCA Equivalent\n",
    "\"\"\"\n",
    "\n",
    "# df = raw_data\n",
    "def MCA(df):\n",
    "    tmp_nom = np.zeros(df.shape[0])\n",
    "    for c in nominal:\n",
    "        nom_f = df[c]\n",
    "        nom_f_vc = nom_f.value_counts()    \n",
    "        nom_f_ohe = pd.get_dummies(nom_f)\n",
    "        nom_f_vc_pk = nom_f_vc/df.shape[0]\n",
    "        for ci in nom_f_ohe.columns:\n",
    "            nom_f_ohe[ci] = (nom_f_ohe[ci]/nom_f_vc_pk[ci])-1\n",
    "        tmp_nom = np.c_[tmp_nom,nom_f_ohe.to_numpy()]\n",
    "        # break\n",
    "    final_tmp_nom = tmp_nom[:,1:]\n",
    "    dime = PCA(n_components=50,svd_solver='full',)\n",
    "    final_nome_t  = dime.fit_transform(final_tmp_nom)\n",
    "    return final_nome_t\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "Binary Target Distributions\n",
    "\"\"\"\n",
    "\n",
    "def binary_target_dist(df):\n",
    "    bin_target_0 = pd.DataFrame(columns=binary, index=[0,1,2])\n",
    "    bin_target_1 = pd.DataFrame(columns=binary, index=[0,1,2])\n",
    "    for c in binary:\n",
    "        target_census = df.target.value_counts().to_dict()    \n",
    "        f = df.groupby([c])['target'].value_counts().sort_index().unstack().transpose()\n",
    "        # print(f[0])\n",
    "        bin_target_0[c] = f[0]\n",
    "        bin_target_1[c] = f[1]\n",
    "    for i in range(3):\n",
    "        bin_target_0.loc[i,:] = bin_target_0.loc[i,:] / target_census[i]\n",
    "        bin_target_1.loc[i,:] = bin_target_1.loc[i,:] / target_census[i]\n",
    "    return bin_target_0, bin_target_1\n",
    "\n",
    "def m_VI(df):\n",
    "    VI = np.linalg.inv(np.cov(df, rowvar=False))\n",
    "    return VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c267c-1657-4739-86fa-4c940edb4a76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ColumnSelectors:\n",
    "    def __init__(self, default=None):\n",
    "        self.dtype_info = {\n",
    "            \"binary__v_1\": \"Binary\",\n",
    "            \"binary__v_11\": \"Binary\",\n",
    "            \"binary__v_14\": \"Binary\",\n",
    "            \"binary__v_26\": \"Binary\",\n",
    "            \"binary__v_27\": \"Binary\",\n",
    "            \"binary__v_28\": \"Binary\",\n",
    "            \"binary__v_30\": \"Binary\",\n",
    "            \"binary__v_9\": \"Binary\",\n",
    "            \"nominal__v_12\": \"Nominal\",\n",
    "            \"nominal__v_18\": \"Nominal\",\n",
    "            \"nominal__v_20\": \"Nominal\",\n",
    "            \"nominal__v_21\": \"Nominal\",\n",
    "            \"nominal__v_25\": \"Nominal\",\n",
    "            \"nominal__v_3\": \"Nominal\",\n",
    "            \"nominal__v_32\": \"Nominal\",\n",
    "            \"nominal__v_4\": \"Nominal\",\n",
    "            \"ordinal__v_0\": \"Ordinal\",\n",
    "            \"ordinal__v_10\": \"Ordinal\",\n",
    "            \"ordinal__v_13\": \"Ordinal\",\n",
    "            \"ordinal__v_15\": \"Ordinal\",\n",
    "            \"ordinal__v_17\": \"Ordinal\",\n",
    "            \"ordinal__v_19\": \"Ordinal\",\n",
    "            \"ordinal__v_22\": \"Ordinal\",\n",
    "            \"ordinal__v_23\": \"Ordinal\",\n",
    "            \"ordinal__v_24\": \"Ordinal\",\n",
    "            \"ordinal__v_29\": \"Ordinal\",\n",
    "            \"ordinal__v_31\": \"Ordinal\",\n",
    "            \"ordinal__v_33\": \"Ordinal\",\n",
    "            \"ordinal__v_5\": \"Ordinal\",\n",
    "            \"ordinal__v_6\": \"Ordinal\",\n",
    "            \"ratio__v_16\": \"Ratio\",\n",
    "            \"ratio__v_2\": \"Ratio\",\n",
    "            \"ratio__v_34\": \"Ratio\",\n",
    "            \"ratio__v_35\": \"Ratio\",\n",
    "            \"ratio__v_36\": \"Ratio\",\n",
    "            \"ratio__v_37\": \"Ratio\",\n",
    "            \"ratio__v_38\": \"Ratio\",\n",
    "            \"ratio__v_39\": \"Ratio\",\n",
    "            \"ratio__v_40\": \"Ratio\",\n",
    "            \"ratio__v_7\": \"Ratio\",\n",
    "            \"ratio__v_8\": \"Ratio\",\n",
    "        }\n",
    "\n",
    "        self.ordinal_cols = [\n",
    "            i for i in self.dtype_info if self.dtype_info[i] == \"Ordinal\"\n",
    "        ]\n",
    "        self.nominal_cols = [\n",
    "            i for i in self.dtype_info if self.dtype_info[i] == \"Nominal\"\n",
    "        ]\n",
    "        self.binary_cols = [\n",
    "            i for i in self.dtype_info if self.dtype_info[i] == \"Binary\"\n",
    "        ]\n",
    "        self.ratio_cols = [i for i in self.dtype_info if self.dtype_info[i] == \"Ratio\"]\n",
    "        self.ordinal = make_column_selector(\n",
    "            pattern=\"|\".join(self.ordinal_cols),\n",
    "        )\n",
    "        self.nominal = make_column_selector(\n",
    "            pattern=\"|\".join(self.nominal_cols),\n",
    "        )\n",
    "        self.binary = make_column_selector(\n",
    "            pattern=\"|\".join(self.binary_cols),\n",
    "        )\n",
    "        self.ratio = make_column_selector(\n",
    "            pattern=\"|\".join(self.ratio_cols),\n",
    "        )\n",
    "\n",
    "    def ordinal_selector(self):\n",
    "        return self.ordinal\n",
    "\n",
    "    def nominal_selector(self):\n",
    "        return self.nominal\n",
    "\n",
    "    def binary_selector(self):\n",
    "        return self.binary\n",
    "\n",
    "    def ratio_selector(self):\n",
    "        return self.ratio\n",
    "\n",
    "    def categorise_data(self, df: pd.DataFrame = None):\n",
    "        \"\"\"\n",
    "        Categorise Data based on given data\n",
    "        :params df : pandas.Dataframe\n",
    "        \"\"\"\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            ordinal_data = df.loc[:, self.ordinal_cols]\n",
    "            nominal_data = df.loc[:, self.nominal_cols]\n",
    "            binary_data = df.loc[:, self.binary_cols]\n",
    "            ratio_data = df.loc[:, self.ratio_cols]\n",
    "        else:\n",
    "            print(\"Please provide valid Data\")\n",
    "        return ordinal_data, nominal_data, binary_data, ratio_data\n",
    "\n",
    "\n",
    "column_directory = ColumnSelectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e3009-68c8-42d4-ab61-9682bc3b6992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"/kaggle/input/students-drop-out-prediction/\"\n",
    "DATA_SAVE_PATH = \"/kaggle/working/\"\n",
    "TRAIN_DATA = \"train.csv\"\n",
    "TEST_DATA = \"test.csv\"\n",
    "KAGGLE_ENV = 1\n",
    "BENCHMARK_INPUT = '/kaggle/input/sub-sample/'\n",
    "\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "if \"mlop3n/Pycharm\" in cwd:\n",
    "    KAGGLE_ENV = 0\n",
    "\n",
    "if KAGGLE_ENV == 0:\n",
    "    LOCAL_PATH = \"../data\"\n",
    "    DATA_PATH = LOCAL_PATH + DATA_PATH\n",
    "    DATA_SAVE_PATH = LOCAL_PATH + DATA_SAVE_PATH\n",
    "    BENCHMARK_INPUT = LOCAL_PATH + BENCHMARK_INPUT\n",
    "\n",
    "\n",
    "def write_raw_data(raw_data: pd.DataFrame, raw_data_eval: pd.DataFrame):\n",
    "    global DATA_SAVE_PATH\n",
    "    raw_data.to_parquet(DATA_SAVE_PATH + \"train.parquet\")\n",
    "    raw_data_eval.to_parquet(DATA_SAVE_PATH + \"test.parquet\")\n",
    "\n",
    "\n",
    "# TODO rerun this script to rename the target column\n",
    "\n",
    "\n",
    "def reload_raw_data():\n",
    "    global DATA_SAVE_PATH\n",
    "    raw_data = pd.read_parquet(DATA_SAVE_PATH + \"train.parquet\")\n",
    "    raw_data_eval = pd.read_parquet(DATA_SAVE_PATH + \"test.parquet\")\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "benchmark = pd.read_csv(BENCHMARK_INPUT+'sample.csv')\n",
    "\n",
    "def make_submission(y):\n",
    "    trial=0\n",
    "    y_df = pd.DataFrame(y, columns=[\"label\"])\n",
    "    y_df.index.rename('id',inplace=True)\n",
    "    y_df.to_csv(\n",
    "        DATA_SAVE_PATH + \"submission.csv\",\n",
    "    )\n",
    "    if trial==0:\n",
    "        benchmark.to_csv(DATA_SAVE_PATH+ \"submission.csv\")\n",
    "\n",
    "@run_if_refresh\n",
    "def rename_columns_with_dtype(DATA_PATH=DATA_PATH, DATA_SAVE_PATH=DATA_SAVE_PATH):\n",
    "    raw_data = pd.read_csv(DATA_PATH + TRAIN_DATA, index_col=0)\n",
    "    raw_data_eval = pd.read_csv(DATA_PATH + TEST_DATA, index_col=0)\n",
    "\n",
    "    raw_dtypes_info = {}\n",
    "    saved_dtypes_info = column_directory.dtype_info\n",
    "    for k, v in saved_dtypes_info.items():\n",
    "        tmp = k.split(\"__\")\n",
    "        data_type = tmp[0]\n",
    "        column_name = tmp[1]\n",
    "        raw_dtypes_info[column_name] = k\n",
    "\n",
    "    raw_data.rename(columns=raw_dtypes_info, inplace=True)\n",
    "    raw_data_eval.rename(columns=raw_dtypes_info, inplace=True)\n",
    "    raw_data.rename(columns={\"label\": \"target\"}, inplace=True)\n",
    "    write_raw_data(raw_data, raw_data_eval)\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "\n",
    "# rename_columns_with_dtype()\n",
    "def reset_data():\n",
    "    return rename_columns_with_dtype()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4325e065-c52e-4f79-b5e2-922e47f8878d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_data, raw_data_eval = reset_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508683ef-3ae1-447a-bd4b-aeee154a3e9b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = raw_data.drop([\"target\"], axis=1)\n",
    "y = raw_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd50658-be33-41b6-91cb-b17d90a91dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ordinal = column_directory.ordinal_cols\n",
    "nominal = column_directory.nominal_cols\n",
    "binary = column_directory.binary_cols\n",
    "ratios = column_directory.ratio_cols\n",
    "\n",
    "ordinal_data, nominal_data, binary_data, ratios_data = column_directory.categorise_data(\n",
    "    raw_data\n",
    ")\n",
    "(\n",
    "    ordinal_data_eval,\n",
    "    nominal_data_eval,\n",
    "    binary_data_eval,\n",
    "    ratios_data_eval,\n",
    ") = column_directory.categorise_data(raw_data_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a892c6-d412-4993-9f31-dd207a7babc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(raw_data=raw_data, test_size=0.5):\n",
    "    X = raw_data.drop([\"target\"], axis=1)\n",
    "    y = raw_data.target\n",
    "    # quick_test(X)\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(X, y, test_size=test_size)\n",
    "\n",
    "    random_forest = BalancedRandomForestClassifier(random_state=10)\n",
    "    tmp_clf = make_pipeline(\n",
    "        RFE(\n",
    "            estimator=BalancedRandomForestClassifier(random_state=42),\n",
    "            n_features_to_select=50,\n",
    "        ),\n",
    "        AdaBoostClassifier(\n",
    "            base_estimator=random_forest, n_estimators=4, random_state=10\n",
    "        ),\n",
    "    )\n",
    "    with parallel_backend(\"loky\"):\n",
    "        y_pred = tmp_clf.fit(X_train, y_train).predict(X_test)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        scores = cross_validate(\n",
    "            tmp_clf,\n",
    "            X,\n",
    "            y,\n",
    "            cv=StratifiedKFold(n_splits=4),\n",
    "            n_jobs=-1,\n",
    "            scoring=\"f1_macro\",\n",
    "            return_train_score=True,\n",
    "        )\n",
    "        print(\"F1_Train\", scores[\"train_score\"].mean())\n",
    "        print(\"F1_Test\", scores[\"test_score\"].mean())\n",
    "\n",
    "\n",
    "# baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea75110e-6f2d-4ef7-ab22-1c10548c135b",
   "metadata": {},
   "source": [
    "## Transformations\n",
    " - <mark>__Scale Down Ordinal Data By Substracting all samples with Global Feature Minimum__</mark>\n",
    " - <mark>__Sort Columns by Name for better visibility__</mark>\n",
    " - <mark>__Ordinally Encode Nominal Data to reduce categorical distance within a feature__</mark>\n",
    " - <mark>__Add a Column - Sum of all Binary Features along x-axis__</mark>\n",
    " - <mark>__Standardise the Ratio Features using MinMaxScaler__</mark>\n",
    " - <mark>__Group By binary Literal__</mark>\n",
    " - <mark>__Generate Ordinal Values L2 Norm__</mark>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1dce9-cd6f-446b-be69-2323f38bc51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "__refresh__ = 1\n",
    "raw_data, raw_data_eval = reset_data()\n",
    "def gen_categorical_mask(df):\n",
    "    categorical_f_mask =[]\n",
    "    # X = raw_data.columns\n",
    "    for f in df.columns:\n",
    "        if f in ordinal+nominal+binary:\n",
    "            categorical_f_mask.append(True)\n",
    "        else:\n",
    "            categorical_f_mask.append(False)\n",
    "    return categorical_f_mask\n",
    "# @run_if_refresh\n",
    "def update_ordinal_data(raw_data=raw_data, raw_data_eval=raw_data_eval):\n",
    "    categorical_columns = [ordinal, nominal]\n",
    "    ordinal_data = raw_data.loc[:, ordinal].copy()\n",
    "    ordinal_data_eval = raw_data_eval.loc[:, ordinal].copy()\n",
    "    raw_data.loc[:, ordinal] = ordinal_data - ordinal_data.min()\n",
    "    raw_data_eval.loc[:, ordinal] = ordinal_data_eval - ordinal_data.min()\n",
    "    # update_categorical_data()\n",
    "    # disp = raw_data.loc[:, ordinal].min().plot(color=\"r\", rot=90)\n",
    "    # disp = raw_data_eval.loc[:, ordinal].min().plot(rot=90)\n",
    "    # write_raw_data(raw_data, raw_data_eval)\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "# @run_if_refresh\n",
    "def sort_columns_by_name(raw_data=raw_data, raw_data_eval=raw_data_eval):\n",
    "    raw_data = raw_data.sort_index(axis=1)\n",
    "    raw_data_eval = raw_data_eval.sort_index(axis=1)\n",
    "    raw_data.to_parquet(DATA_SAVE_PATH + \"train.parquet\")\n",
    "    raw_data_eval.to_parquet(DATA_SAVE_PATH + \"test.parquet\")\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "# @run_if_refresh\n",
    "def ordinally_encode_nominal_data(raw_data=raw_data, raw_data_eval=raw_data_eval):\n",
    "    ordinal_enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=125)\n",
    "    nominal_enc_data = ordinal_enc.fit_transform(raw_data.loc[:, nominal])\n",
    "    nominal_enc_data_eval = ordinal_enc.transform(raw_data_eval.loc[:, nominal])\n",
    "    raw_data.loc[:, nominal] = nominal_enc_data\n",
    "    raw_data_eval.loc[:, nominal] = nominal_enc_data_eval\n",
    "    # write_raw_data(raw_data, raw_data_eval)\n",
    "    return raw_data, raw_data_eval\n",
    "# @run_if_refresh\n",
    "def binary_feature_sum(raw_data=raw_data, raw_data_eval=raw_data_eval):\n",
    "    raw_data[\"binary_sum\"] = raw_data.loc[:, binary].sum(axis=1)/7\n",
    "    raw_data_eval[\"binary_sum\"] = raw_data_eval.loc[:, binary].sum(axis=1)/7\n",
    "    # write_raw_data(raw_data, raw_data_eval)\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "# @run_if_refresh\n",
    "def standardise_ratio_features(\n",
    "    raw_data=raw_data.copy(), raw_data_eval=raw_data_eval.copy()\n",
    "):\n",
    "    # Load Data\n",
    "    payload = raw_data.loc[:, ratios]\n",
    "    payload_eval = raw_data_eval.loc[:, ratios]\n",
    "    mscaler = MinMaxScaler()\n",
    "    # Apply Scaling\n",
    "    payload_t = mscaler.fit_transform(payload)\n",
    "    payload_eval_t = mscaler.transform(payload_eval)\n",
    "    # New Column Names\n",
    "    new_ordinal_col_names = mscaler.get_feature_names_out()\n",
    "    new_ordinal_col_names = [x.replace(\".\", \"_\") for x in new_ordinal_col_names]\n",
    "    # Create Dataframe with new features to concatenate\n",
    "    tmp_raw_data = pd.DataFrame(\n",
    "        payload_t, columns=new_ordinal_col_names, index=raw_data.index\n",
    "    )\n",
    "    tmp_raw_data_eval = pd.DataFrame(\n",
    "        payload_eval_t, columns=new_ordinal_col_names, index=raw_data_eval.index\n",
    "    )\n",
    "    # Concatenate in place\n",
    "    raw_data = pd.concat([tmp_raw_data, raw_data.drop(ratios, axis=1)], axis=1)\n",
    "    raw_data_eval = pd.concat(\n",
    "        [tmp_raw_data_eval, raw_data_eval.drop(ratios, axis=1)], axis=1\n",
    "    )\n",
    "    # Write modified data to disk\n",
    "    # write_raw_data(raw_data, raw_data_eval)\n",
    "    # return raw_data\n",
    "    return raw_data, raw_data_eval\n",
    "    \n",
    "def ordinal_norm(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    tmp_ord = scaler.fit_transform(df.loc[:,ordinal])\n",
    "    tmp_ord_sq= tmp_ord @ tmp_ord.T \n",
    "    tmp_ord_nom = scaler.fit_transform(abs(np.sqrt(tmp_ord_sq.mean(axis=1))).reshape(-1,1))\n",
    "    return tmp_ord_nom\n",
    "\n",
    "def gen_ord_norm(raw_data=raw_data, raw_data_eval=raw_data_eval):\n",
    "    raw_data['ord_norm'] = ordinal_norm(raw_data)\n",
    "    raw_data_eval['ord_norm'] = ordinal_norm(raw_data_eval)\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "def gen_binary_literal(row):\n",
    "    bin_chr = [str(row[b]) for b in binary]\n",
    "    b_l = \"\"\n",
    "    for c in bin_chr:\n",
    "        b_l+=c\n",
    "    row.bin_literal = int(b_l,2)/255\n",
    "    return row\n",
    "\n",
    "def add_literal_to_data(raw_data, raw_data_eval):\n",
    "    b_lit_train = pd.DataFrame(columns=[\"bin_literal\"], index=raw_data.index)\n",
    "    b_lit_eval  = pd.DataFrame(columns=[\"bin_literal\"], index=raw_data_eval.index)\n",
    "    raw_data['bin_literal']  = \"0\"\n",
    "    raw_data_eval['bin_literal']  = \"0\"\n",
    "    raw_data = raw_data.apply(gen_binary_literal,axis=1)\n",
    "    raw_data_eval= raw_data_eval.apply(gen_binary_literal,axis=1)\n",
    "    return raw_data, raw_data_eval\n",
    "\n",
    "@run_if_refresh\n",
    "def apply_transformations():\n",
    "    raw_data, raw_data_eval = reset_data()\n",
    "\n",
    "    raw_data, raw_data_eval = update_ordinal_data(raw_data, raw_data_eval)\n",
    "    # raw_data, raw_data_eval = reload_raw_data()\n",
    "    raw_data, raw_data_eval = sort_columns_by_name(raw_data, raw_data_eval)\n",
    "    # raw_data, raw_data_eval = reload_raw_data()\n",
    "    raw_data, raw_data_eval = ordinally_encode_nominal_data(raw_data, raw_data_eval)    \n",
    "    # raw_data, raw_data_eval = reload_raw_data()\n",
    "    raw_data, raw_data_eval = binary_feature_sum(raw_data, raw_data_eval)\n",
    "    # raw_data, raw_data_eval = reload_raw_data()\n",
    "    raw_data, raw_data_eval = standardise_ratio_features(raw_data, raw_data_eval)\n",
    "    raw_data, raw_data_eval = gen_ord_norm(raw_data, raw_data_eval)\n",
    "    raw_data, raw_data_eval = add_literal_to_data(raw_data, raw_data_eval)\n",
    "    raw_data.sort_index(axis=1,inplace=True)\n",
    "    raw_data_eval.sort_index(axis=1,inplace=True)\n",
    "    write_raw_data(raw_data, raw_data_eval)\n",
    "    \n",
    "    \n",
    "apply_transformations()\n",
    "raw_data, raw_data_eval = reload_raw_data()\n",
    "\n",
    "\n",
    "def refresh_data():\n",
    "    global __refresh__\n",
    "    __refresh__ = 1\n",
    "    raw_data, raw_data_eval = reset_data()\n",
    "    # rename_columns_with_dtype()\n",
    "    apply_transformations()\n",
    "    raw_data, raw_data_eval = reload_raw_data()\n",
    "    __refresh__ = 0\n",
    "    print(\"Data Refresh Success\")\n",
    "    return raw_data, raw_data_eval\n",
    "    \n",
    "# raw_data, raw_data_eval = refresh_data()\n",
    "\n",
    "features = raw_data.drop(['target'],axis=1).columns\n",
    "\n",
    "def create_target_encoding():\n",
    "    new_column_names = []\n",
    "    stubs = [\"_0\",\"_1\",\"_2\"]\n",
    "    for c in ordinal+nominal:\n",
    "        for _suffix in stubs:\n",
    "            new_column_names.append(c+_suffix)\n",
    "    df_train = pd.DataFrame(index=raw_data.index,columns=new_column_names)\n",
    "    df_eval = pd.DataFrame(index=raw_data_eval.index,columns=new_column_names)\n",
    "    target_census = raw_data.target.value_counts().to_dict()    \n",
    "    for c in ordinal+nominal:\n",
    "        target_counts = raw_data.groupby(c)['target'].value_counts().unstack(fill_value=0)\n",
    "        tf: pd.DataFrame = target_counts.loc[:,[0,1,2]]\n",
    "        target_counts['sum'] = tf.sum(axis=1)\n",
    "        target_counts['std'] = tf.std(axis=1)\n",
    "        # target_counts['std'] = \n",
    "        # tf2=tf\n",
    "        # for i in range(3):\n",
    "        #     tf.loc[:,i] = tf.loc[:,i]/(target_census[i]-1)\n",
    "        # tf2 = tf.subtract(target_counts['std'],axis='rows')\n",
    "        tf2 = tf.divide((target_counts['sum']),axis='rows')\n",
    "        target_map = {0:0,1:1,2:2}\n",
    "        for val in target_map:\n",
    "            mapping = defaultdict(int,tf2.to_dict()[target_map[val]])\n",
    "            df_train[c+stubs[val]] = raw_data[c].map(mapping)\n",
    "            df_eval[c+stubs[val]] = raw_data_eval[c].map(mapping)\n",
    "    return df_train, df_eval\n",
    "\n",
    "target_encoded_train, target_encoded_eval  = create_target_encoding()\n",
    "\n",
    "\n",
    "ordinal = column_directory.ordinal_cols\n",
    "nominal = column_directory.nominal_cols\n",
    "binary = column_directory.binary_cols\n",
    "ratios = column_directory.ratio_cols\n",
    "\n",
    "ordinal_data, nominal_data, binary_data, ratios_data = column_directory.categorise_data(\n",
    "    raw_data\n",
    ")\n",
    "(\n",
    "    ordinal_data_eval,\n",
    "    nominal_data_eval,\n",
    "    binary_data_eval,\n",
    "    ratios_data_eval,\n",
    ") = column_directory.categorise_data(raw_data_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ab9c1-080b-4435-8e2e-cba1e599dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(class_weight='balanced')\n",
    "REPEAT=10\n",
    "disp = plt.figure(figsize=(20,10))\n",
    "transformed_bin_scores = cross_validate(clf,nominal_data,raw_data.target, cv = RepeatedStratifiedKFold(n_splits=3,n_repeats=REPEAT), scoring='f1_macro', return_train_score=True)\n",
    "disp = plt.plot(transformed_bin_scores['test_score'],label='Test Scores NOMINAL'.upper())\n",
    "transformed_bin_scores = cross_validate(clf,binary_data,raw_data.target, cv = RepeatedStratifiedKFold(n_splits=3,n_repeats=REPEAT), scoring='f1_macro', return_train_score=True)\n",
    "disp = plt.plot(transformed_bin_scores['test_score'],label='Test Scores BINARY'.upper())\n",
    "transformed_bin_scores = cross_validate(clf,ordinal_data,raw_data.target, cv = RepeatedStratifiedKFold(n_splits=3,n_repeats=REPEAT), scoring='f1_macro', return_train_score=True)\n",
    "disp = plt.plot(transformed_bin_scores['test_score'],label='Test Scores ORDINAL'.upper())\n",
    "transformed_bin_scores = cross_validate(clf,ratios_data,raw_data.target, cv = RepeatedStratifiedKFold(n_splits=3,n_repeats=REPEAT), scoring='f1_macro', return_train_score=True)\n",
    "disp = plt.plot(transformed_bin_scores['test_score'],label='Test Scores RATIOS'.upper())\n",
    "custom_cols = ['bin_literal','binary_sum','ord_norm']\n",
    "custom_data = raw_data.loc[:,custom_cols]\n",
    "transformed_bin_scores = cross_validate(clf,custom_data,raw_data.target, cv = RepeatedStratifiedKFold(n_splits=3,n_repeats=REPEAT), scoring='f1_macro', return_train_score=True)\n",
    "disp = plt.plot(transformed_bin_scores['test_score'],label='Test Scores CUSTOM'.upper())\n",
    "# disp = plt.plot(transformed_bin_scores['train_score'],label='Train Scores 2')\n",
    "s  = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d4cb7c-e092-43db-b5e4-71ae0347e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = raw_data.corrwith(raw_data.target, method='pearson').abs().sort_values().iloc[:-1].plot(title=\"Feature Corr Stat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9d003-93f3-4b29-99ac-8d48c3a73b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter(clst.labels_)\n",
    "\"\"\"\n",
    "CountEncoding\n",
    "\"\"\"\n",
    "enc = CountEncoder()\n",
    "transformer = make_pipeline(CountEncoder(cols=nominal),OneHotEncoder(handle_unknown='ignore'))\n",
    "transformer = make_pipeline(OneHotEncoder(handle_unknown='ignore'))\n",
    "n_enc = transformer.fit_transform(nominal_data)\n",
    "svd = TruncatedSVD(n_components=100, random_state=42,algorithm='arpack')\n",
    "nenc_t = svd.fit_transform(n_enc)\n",
    "# X = nenc_t\n",
    "X = nominal_data\n",
    "y = raw_data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_train_test(X,y, 0.5)\n",
    "sampler_estimator = DecisionTreeClassifier(random_state=10, class_weight='balanced')\n",
    "under_sampler = InstanceHardnessThreshold(random_state=42,estimator=sampler_estimator,cv=5,n_jobs=-1)\n",
    "over_sampler = ADASYN(n_neighbors=2,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee485f9-62e8-461e-bde7-e90fcf94842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VI = m_VI(X_train)\n",
    "model = KNeighborsClassifier(n_jobs=-1,n_neighbors=8,leaf_size=10,weights='distance', metric='mahalanobis', metric_params={'VI': VI})\n",
    "model = BalancedRandomForestClassifier(random_state=10,class_weight='balanced_subsample',max_depth=4,n_estimators=100)\n",
    "# model = HistGradientBoostingClassifier(random_state=10,max_depth=5)\n",
    "\n",
    "clf = make_imb_pipeline(under_sampler,over_sampler,model)\n",
    "clf = make_imb_pipeline(OneHotEncoder(handle_unknown='ignore'), svd,over_sampler,under_sampler,model)\n",
    "# clf = make_imb_pipeline(under_sampler,model)\n",
    "\n",
    "with parallel_backend('loky'):\n",
    "    # scores = cross_validate(clf,X,y,cv=RepeatedStratifiedKFold(n_repeats=3),scoring='f1_macro',)\n",
    "    y_pred = clf.fit(X_train,y_train).predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# scores\n",
    "# clf = KNeighborsClassifier(n_jobs=-1,n_neighbors=18,leaf_size=3,weights='distance', metric='mahalanobis', metric_params={'VI': np.cov(X_train, rowvar=False)})\n",
    "# scores = cross_validate(clf_bag,X,y,cv=RepeatedStratifiedKFold(n_repeats=3),scoring='f1_macro',)\n",
    "# model = KNeighborsClassifier(n_jobs=-1,n_neighbors=5,leaf_size=100)\n",
    "# clf_ba# scores = cross_validate(clf_bag,X,y,cv=RepeatedStratifiedKFold(n_repeats=3),scoring='f1_macro',)\n",
    "# scores['test_score'].mean()g = BaggingClassifier(base_estimator=clf,random_state=20,n_jobs=-1,n_estimators=50)\n",
    "# print(scores['test_score'])\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test,y_pred)\n",
    "# disp = plt.figure(figsize=(15,15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e98672-eddd-4600-9f23-6d5262b34cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X= raw_data.drop(['target'],axis=1)\n",
    "X = raw_data.loc[:,nominal+ordinal+binary+['bin_literal','ord_norm','binary_sum']+ratios]\n",
    "\n",
    "X_eval = raw_data_eval\n",
    "# X = nominal_data\n",
    "# X_eval = nominal_data_eval\n",
    "cov_estimators = [EmpiricalCovariance(),MinCovDet(),GraphicalLassoCV(alphas=10,cv=4,n_jobs=-1),OAS(),LedoitWolf(),ShrunkCovariance()]\n",
    "# cov_raw_data = cov_estimator.fit(X)\n",
    "with parallel_backend('threading'):\n",
    "    fitted_cov_estimators = [x.fit(X) for x in cov_estimators]\n",
    "    \n",
    "\n",
    "cov_scores_eval = [x.score(X_eval) for x in fitted_cov_estimators]\n",
    "cov_matrices  = [pd.DataFrame(est.covariance_, columns=est.feature_names_in_,index = est.feature_names_in_)\n",
    "                for est in cov_estimators]\n",
    "\n",
    "# cov_scores_eval\n",
    "# cov_matrices[2]\n",
    "\n",
    "import copy\n",
    "# pd.DataFrame.to_csv\n",
    "# pd.Series.sort_values()\n",
    "X = raw_data.loc[:,nominal+ordinal+binary+['bin_literal','ord_norm','binary_sum']+ratios]\n",
    "T = 0.5\n",
    "pos_variable_f = [{c:[] for c in X.columns} for _ in range(len(cov_matrices))]\n",
    "neg_variable_f = [{c:[] for c in X.columns} for _ in range(len(cov_matrices))]\n",
    "for idx,C in enumerate(cov_matrices):\n",
    "    cols = list(C.columns)\n",
    "    for f in cols:\n",
    "        # not_same_f = cols.remove('f')\n",
    "        not_same_f = copy.deepcopy(cols)\n",
    "        not_same_f.remove(f)\n",
    "        cov_f = C[f].loc[not_same_f]\n",
    "        pos_imp_co_f = cov_f[cov_f>T]\n",
    "        neg_imp_co_f = cov_f[cov_f<-T]\n",
    "        pos_variable_f[idx][f].extend(list(pos_imp_co_f.index))\n",
    "        neg_variable_f[idx][f].extend(list(neg_imp_co_f.index))\n",
    "pos_variable_f = [{prel_key:prel[prel_key] for prel_key in prel if len(prel[prel_key])!=0} for prel in pos_variable_f]\n",
    "neg_variable_f = [{nrel_key:nrel[nrel_key] for nrel_key in nrel if len(nrel[nrel_key])!=0} for nrel in neg_variable_f]\n",
    "\n",
    "\n",
    "prel_coll = [set() for _ in range(len(cov_matrices))]\n",
    "nrel_coll = [set() for _ in range(len(cov_matrices))]\n",
    "for idx,rel in enumerate(pos_variable_f):\n",
    "    for features in rel.values():\n",
    "        for f in features:\n",
    "            prel_coll[idx].add(f)\n",
    "    for feature in rel.keys():\n",
    "        # for f in features:\n",
    "        prel_coll[idx].add(feature)\n",
    "for idx,rel in enumerate(neg_variable_f):\n",
    "    for features in rel.values():\n",
    "        for f in features:\n",
    "            nrel_coll[idx].add(f)\n",
    "    for feature in rel.keys():\n",
    "        # for f in features:\n",
    "        nrel_coll[idx].add(feature)\n",
    "\n",
    "nrel_coll = [list(L) for L in nrel_coll]\n",
    "prel_coll = [list(L) for L in prel_coll]\n",
    "high_variable_f = [list(set(prel_coll[idx]+nrel_coll[idx])) for idx in range(len(cov_matrices))]\n",
    "total_feature_set = set(raw_data.drop(['target'],axis=1).columns)\n",
    "low_variable_f = [list(total_feature_set-set(high_f)) for high_f in high_variable_f]\n",
    "# low_var_master_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a464e9de-4c12-4bb7-84c9-90a601cb1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(f) for f in high_variable_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d5361-3928-4d3b-bd67-28ae41bdfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_scores_nrel = []\n",
    "with parallel_backend('loky'):\n",
    "    for idx in range(len(prel_coll)):\n",
    "        X = raw_data.loc[:,nrel_coll[idx]]\n",
    "        y = raw_data.target\n",
    "        model = DecisionTreeClassifier(random_state=42, class_weight='balanced',max_depth=5)\n",
    "        clf = OneVsRestClassifier(model,n_jobs=-1)\n",
    "        scores = cross_validate(clf,X,y,cv=RepeatedStratifiedKFold(),n_jobs=-1,scoring='f1_macro',return_train_score=True)\n",
    "        master_scores_nrel.append({'test_score':scores['test_score'].mean(),'train_score':scores['train_score'].mean()})\n",
    "master_scores_nrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64b7abf-93a4-4fd2-bffa-b32f4b07ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_scores_prel = []\n",
    "with parallel_backend('loky'):\n",
    "    for idx in range(len(prel_coll)):\n",
    "        X = raw_data.loc[:,prel_coll[idx]]\n",
    "        y = raw_data.target\n",
    "        model = DecisionTreeClassifier(random_state=42, class_weight='balanced',max_depth=5)\n",
    "        clf = OneVsRestClassifier(model,n_jobs=-1)\n",
    "        scores = cross_validate(clf,X,y,cv=RepeatedStratifiedKFold(),n_jobs=-1,scoring='f1_macro',return_train_score=True)\n",
    "        master_scores_prel.append({'test_score':scores['test_score'].mean(),'train_score':scores['train_score'].mean()})\n",
    "\n",
    "master_scores_prel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0bf5fa-1326-41b7-a415-49ca929eb698",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_scores = []\n",
    "with parallel_backend('loky'):\n",
    "    for idx in range(len(prel_coll)):\n",
    "        X = raw_data.loc[:,list(set(prel_coll[idx]+nrel_coll[idx]))]\n",
    "        y = raw_data.target\n",
    "        mask = gen_categorical_mask(X)\n",
    "        \n",
    "        # model = HistGradientBoostingClassifier(random_state=42,categorical_features=mask,min_samples_leaf=3,max_leaf_nodes=3,max_iter=100, learning_rate=0.12)\n",
    "        # clf = OneVsOneClassifier(model,n_jobs=-1)\n",
    "        model = HistGradientBoostingClassifier(random_state=42,categorical_features=mask,)\n",
    "        # model = DecisionTreeClassifier(random_state=42, class_weight='balanced',max_depth=5)\n",
    "        clf = OneVsRestClassifier(model,n_jobs=-1)\n",
    "        # clf = model\n",
    "        scores = cross_validate(clf,X,y,cv=RepeatedStratifiedKFold(n_splits=2,n_repeats=3),n_jobs=-1,scoring='f1_macro',return_train_score=True)\n",
    "        master_scores.append({'test_score':scores['test_score'].mean(),'train_score':scores['train_score'].mean()})\n",
    "\n",
    "master_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7556c-8be2-4f69-9277-5a53250c8c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_variable_f = []\n",
    "# total_feature_set = set(raw_data.drop(['target'],axis=1).columns)\n",
    "# low_variable_f = [list(total_feature_set-set(high_f)) for high_f in high_variable_f]\n",
    "low_var_master_scores = []\n",
    "with parallel_backend('loky'):\n",
    "    for idx in range(len(prel_coll)):\n",
    "        X = raw_data.loc[:,low_variable_f[idx]]\n",
    "        y = raw_data.target\n",
    "        # mask = gen_categorical_mask(X)\n",
    "        # model = HistGradientBoostingClassifier(random_state=42,categorical_features=mask,min_samples_leaf=3,max_leaf_nodes=3,max_iter=100, learning_rate=0.12)\n",
    "        # clf = OneVsOneClassifier(model,n_jobs=-1)\n",
    "        model = DecisionTreeClassifier(random_state=42, class_weight='balanced',max_depth=5)\n",
    "        clf = OneVsRestClassifier(model,n_jobs=-1)\n",
    "        # clf = model\n",
    "        scores = cross_validate(clf,X,y,cv=RepeatedStratifiedKFold(),n_jobs=-1,scoring='f1_macro',return_train_score=True)\n",
    "        low_var_master_scores.append({'test_score':scores['test_score'].mean(),'train_score':scores['train_score'].mean()})\n",
    "\n",
    "low_var_master_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad442a-d2bc-4783-8d78-af606bd26fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = ['ordinal__v_10', 'ordinal__v_29', 'bin_literal', 'nominal__v_21', 'ratio__v_2', 'ordinal__v_24', 'ordinal__v_15', 'binary_sum', 'ordinal__v_22', 'ratio__v_36', 'nominal__v_18', 'ratio__v_16', 'binary__v_27', 'ordinal__v_23', 'binary__v_9', 'nominal__v_20', 'ordinal__v_19', 'binary__v_1', 'ord_norm', 'ratio__v_7', 'nominal__v_4', 'ratio__v_8', 'ratio__v_34', 'nominal__v_25', 'ordinal__v_31', 'nominal__v_3', 'ordinal__v_13', 'binary__v_30', 'ordinal__v_6', 'nominal__v_32', 'nominal__v_12', 'ordinal__v_33', 'binary__v_11', 'binary__v_26', 'ordinal__v_0', 'ordinal__v_5', 'binary__v_14', 'ordinal__v_17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876db9e-0ff1-413e-99f4-5ba66ef4a464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# low_variable_f = []\n",
    "# total_feature_set = set(raw_data.drop(['target'],axis=1).columns)\n",
    "# low_variable_f = [list(total_feature_set-set(high_f)) for high_f in high_variable_f]\n",
    "spec_master_scores = []\n",
    "with parallel_backend('loky'):\n",
    "    # for idx in range(len(prel_coll)):\n",
    "    X = raw_data.drop(['target'],axis=1)\n",
    "    X = raw_data.loc[:,spec]\n",
    "    y = raw_data.target\n",
    "    mask = gen_categorical_mask(X)\n",
    "    model = HistGradientBoostingClassifier(random_state=42,categorical_features=mask,)\n",
    "    clf = model\n",
    "    scores = cross_validate(clf,X,y,cv=RepeatedStratifiedKFold(n_repeats=2),n_jobs=-1,scoring='f1_macro',return_train_score=True)\n",
    "    spec_master_scores.append({'test_score':scores['test_score'].mean(),'train_score':scores['train_score'].mean()})\n",
    "\n",
    "spec_master_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb793935-84ac-4dda-806b-ab821c6ad1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063f3b0-138b-4273-a037-5872a55a765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crr_high_var_f = pd.DataFrame(columns=list(range(len(cov_matrices))), index = raw_data.drop(['target'],axis=1).columns )\n",
    "for idx,f_list in enumerate(high_variable_f):\n",
    "    data_ = raw_data.loc[:, f_list]\n",
    "    correlations = data_.corrwith(raw_data.target)\n",
    "    # crr.append(correlations)\n",
    "    crr_high_var_f.loc[:,idx] = correlations\n",
    "# crr[1]\n",
    "f_target_corr_hi = crr_high_var_f.fillna(0).mean(axis=1).abs().sort_values(ascending=False)\n",
    "f_target_corr_hi[f_target_corr_hi>0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1758acc9-1163-4ff0-a9f8-f7567dadc56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crr_low_var_f = pd.DataFrame(columns=list(range(len(cov_matrices))), index = raw_data.drop(['target'],axis=1).columns )\n",
    "for idx,f_list in enumerate(low_variable_f):\n",
    "    data_ = raw_data.loc[:, f_list]\n",
    "    correlations = data_.corrwith(raw_data.target)\n",
    "    # crr.append(correlations)\n",
    "    crr_low_var_f.loc[:,idx] = correlations\n",
    "# crr[1]\n",
    "f_target_corr_low = crr_low_var_f.fillna(0).mean(axis=1).abs().sort_values(ascending=False)\n",
    "f_target_corr_low[f_target_corr_low>0.01].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ecd86-2385-4924-86b9-0bf57df457ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X= raw_data.drop(['target'],axis=1)\n",
    "Q = pd.get_dummies(raw_data,columns=nominal+ordinal,drop_first=True,)\n",
    "cols= Q.columns\n",
    "s_cols = [c for c in cols if 'nominal' in c or 'ordinal' in c]\n",
    "X = Q.loc[:, s_cols]\n",
    "# X = nominal_data\n",
    "# X = raw_data.loc[:,nominal+ordinal+binary+['bin_literal','ord_norm','binary_sum']+ratios]\n",
    "\n",
    "y = raw_data.target\n",
    "clf = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "clf.fit(X,y)\n",
    "from sklearn import inspection\n",
    "with parallel_backend('threading'):\n",
    "    permutation_importance = inspection.permutation_importance(clf,X,y,scoring='f1_macro',random_state=42,n_jobs=-1,n_repeats=2)\n",
    "\n",
    "imp_mean_pd = pd.Series(permutation_importance['importances_mean'],index=X.columns)\n",
    "imp_std_pd = pd.Series(permutation_importance['importances_std'],index=X.columns)\n",
    "\n",
    "importance = pd.DataFrame()\n",
    "importance['mean'] = imp_mean_pd\n",
    "importance['std'] = imp_std_pd\n",
    "importance = importance.sort_values(by='mean',ascending=False)\n",
    "importance[importance['mean']>0.004]\n",
    "importance\n",
    "f_ohe_cols = list(importance[importance['mean']>0.02].index)\n",
    "X = Q.loc[:, f_ohe_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "591c1b84-09f5-4883-8043-c37240134f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6076980957579783"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_ohe_cols = list(importance[importance['mean']>0.000001].index)\n",
    "X = Q.loc[:, f_ohe_cols]\n",
    "\n",
    "# X = Q.loc[:, f_ohe_cols]\n",
    "y = raw_data.target\n",
    "model = DecisionTreeClassifier(class_weight='balanced',random_state=42)\n",
    "model = BernoulliNB(binarize=False,fit_prior=False,alpha=0.1234401)\n",
    "model = LogisticRegressionCV(class_weight='balanced',max_iter=100000)\n",
    "# pca = RFE(estimator=LogisticRegression(class_weight='balanced',max_iter=100000),n_features_to_select=250)\n",
    "pca = PCA()\n",
    "# model = RandomForestClassifier(random_state=42,class_weight='balanced_subsample')\n",
    "clf = make_pipeline(pca,OneVsRestClassifier(model),memory=Memory(DATA_SAVE_PATH,verbose=0))\n",
    "scores = cross_validate(clf,X,y,cv=RepeatedStratifiedKFold(n_splits=2,n_repeats=2),n_jobs=-1,scoring='f1_macro',return_train_score=True)\n",
    "scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a798e-99fe-45ba-a256-15b1c6b425ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca =PCA()\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70cc33c-81b7-4d14-be61-bd98107ab9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argsort(pca.explained_variance_,)\n",
    "# pca.explained_variance_ratio_\n",
    "pca.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b99e1-153b-45ed-9477-4f15fbfe8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in nominal+ordinal:\n",
    "    e = X.mean(axis=1).corr(raw_data[c])\n",
    "    print(c,' ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc52282-a7f5-49fa-9b97-554e5d603682",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581e722-a468-425c-8027-daa66797d3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autoskl)",
   "language": "python",
   "name": "autoskl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
