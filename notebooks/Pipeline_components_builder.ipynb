{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c0459-482d-4867-8f09-ba1723a13fac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import os\n",
    "os.environ['NEPTUNE_PROJECT']=\"mlop3n/SDP\"\n",
    "os.environ['NEPTUNE_NOTEBOOK_PATH']=\"PycharmProjects/sdpiit/notebooks/Pipeline_components_builder.ipynb\"\n",
    "import warnings\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "patch_sklearn()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import (\n",
    "    BackwardDifferenceEncoder,\n",
    "    BaseNEncoder,\n",
    "    BinaryEncoder,\n",
    "    CatBoostEncoder,\n",
    "    CountEncoder,\n",
    "    GLMMEncoder,\n",
    "    HelmertEncoder,\n",
    "    JamesSteinEncoder,\n",
    "    LeaveOneOutEncoder,\n",
    "    MEstimateEncoder,\n",
    "    QuantileEncoder,\n",
    "    SummaryEncoder,\n",
    "    TargetEncoder,\n",
    "    WOEEncoder,\n",
    ")\n",
    "from sklearn import set_config\n",
    "from sklearn.base import clone as model_clone\n",
    "from sklearn.cluster import *\n",
    "from sklearn.compose import *\n",
    "from sklearn.cross_decomposition import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.gaussian_process import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.multioutput import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.utils import *\n",
    "from sklearn.dummy import *\n",
    "from sklearn.semi_supervised import *\n",
    "from sklearn.discriminant_analysis import *\n",
    "import sklearnex, daal4py\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from BorutaShap import BorutaShap\n",
    "\n",
    "from sklearn.calibration import *\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pd.options.display.max_columns = 50\n",
    "set_config(display=\"diagram\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import parallel_backend\n",
    "from joblib.memory import Memory\n",
    "\n",
    "sns.set()\n",
    "from pprint import pprint\n",
    "from helpers import PolynomialWrapper as PWrapper\n",
    "from helpers import NestedCVWrapper as NCVWrapper\n",
    "from helpers import ColumnSelectors\n",
    "import sklearn\n",
    "from helpers import DFCollection\n",
    "from helpers import plot_mean_std_max\n",
    "from helpers import CustomMetrics\n",
    "\n",
    "CACHE_DIR = Memory(location='../data/joblib_memory/')\n",
    "OPTUNA_DB = \"postgresql+psycopg2://postgres:302492@localhost:5433/optuna\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebac6f9-6fad-4c50-bdc9-ab04d2613cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = DFCollection()\n",
    "column_selector = ColumnSelectors()\n",
    "# classifiers = [f() for f in cls_names]\n",
    "dtype_info = column_selector.dtype_info\n",
    "ordinal = column_selector.ordinal_cols\n",
    "nominal = column_selector.nominal_cols\n",
    "binary = column_selector.binary_cols\n",
    "ratio = column_selector.ratio_cols\n",
    "\n",
    "\n",
    "final_data = db.final_data\n",
    "final_pred_data = db.final_pred_data\n",
    "baseline_prediction_data = db.baseline_prediction_data\n",
    "data_logit = db.data_logits\n",
    "prediction_data = db.prediction_data\n",
    "master_data = db.master\n",
    "given_data = db.data\n",
    "\n",
    "ordinal_data, nominal_data, binary_data, ratio_data = db.categorise_data()\n",
    "nominal_categories = db.nominal_categories\n",
    "ordinal_categories = db.ordinal_categories\n",
    "class_labels, n_classes, class_priors = class_distribution(final_data.target.to_numpy().reshape(-1,1))\n",
    "\n",
    "def gen_balanced_trained_test(data, p):\n",
    "    Y = data.target\n",
    "    X_2 = Y_2 = Y[Y == 2].index\n",
    "    X_0 = Y_0 = Y[Y == 0].index\n",
    "    X_1 = Y_1 = Y[Y == 1].index\n",
    "    train_size = int(p * Y_2.shape[0])\n",
    "    test_size = int((1 - p) * Y_2.shape[0])\n",
    "\n",
    "    train_idx_2 = np.random.choice(Y_2, (train_size,))\n",
    "    train_idx_1 = np.random.choice(Y_1, (train_size,))\n",
    "    train_idx_0 = np.random.choice(Y_0, (train_size,))\n",
    "    train_idx = np.r_[train_idx_0, train_idx_1, train_idx_2]\n",
    "    # train_idx.shape\n",
    "\n",
    "    test_idx_2 = np.random.choice(np.setdiff1d(Y_2, train_idx_2), (test_size,))\n",
    "    test_idx_1 = np.random.choice(np.setdiff1d(Y_1, train_idx_1), (test_size,))\n",
    "    test_idx_0 = np.random.choice(np.setdiff1d(Y_0, train_idx_0), (test_size,))\n",
    "    test_idx = np.r_[test_idx_0, test_idx_1, test_idx_2]\n",
    "    # test_idx.shape\n",
    "    return train_idx, test_idx\n",
    "\n",
    "\n",
    "def gen_nominal_maps(bs: pd.DataFrame = master_data) -> tuple[defaultdict, defaultdict]:\n",
    "    nominal_master_db = bs.loc[:, nominal]\n",
    "    nominal_cont_map = defaultdict(dict)\n",
    "    nominal_indvl_map = defaultdict(dict)\n",
    "    for c in nominal:\n",
    "        un = sorted(nominal_master_db[c].unique().tolist())\n",
    "        n = len(un)\n",
    "        new_id = list(range(n))\n",
    "        nominal_indvl_map[c] = dict(zip(un, new_id))\n",
    "    start = 0\n",
    "    for c in nominal:\n",
    "        un = sorted(nominal_master_db[c].unique().tolist())\n",
    "        n = len(un)\n",
    "        new_id = list(range(start, start + n))\n",
    "        nominal_cont_map[c] = dict(zip(un, new_id))\n",
    "        start += n\n",
    "    return nominal_indvl_map, nominal_cont_map\n",
    "\n",
    "\n",
    "# nominal_indvl_map, nominal_cont_map = gen_nominal_maps()\n",
    "# nominal_master_db = bs.loc[:, nominal]\n",
    "\n",
    "# nominal_master_db_indvl = nominal_master_db.copy()\n",
    "# nominal_master_db_cont = nominal_master_db.copy()\n",
    "\n",
    "\n",
    "# nominal_indvl_map\n",
    "def nm_indvl_data_trnsform(row):\n",
    "    for c in nominal:\n",
    "        curr = row[c]\n",
    "        row[c] = nominal_indvl_map[c][curr]\n",
    "    return row\n",
    "\n",
    "\n",
    "# test1_nominal = nominal_master_db_indvl.apply(nm_indvl_data_trnsform, axis=1)\n",
    "\n",
    "\n",
    "def nm_cont_data_trnsform(row):\n",
    "    for c in nominal:\n",
    "        curr = row[c]\n",
    "        row[c] = nominal_cont_map[c][curr]\n",
    "    return row\n",
    "\n",
    "\n",
    "# test2_nominal = nominal_master_db_cont.apply(nm_cont_data_trnsform, axis=1)\n",
    "# prediction_data = pd.read_pickle(\"../data/pred_data.pkl\")\n",
    "# est_ = [(\"cnb\",CategoricalNB()),]\n",
    "\n",
    "\n",
    "def wf_create(cat_encoder=TargetEncoder, model=None):\n",
    "    \"\"\"\n",
    "    :param cat_encoder: category_encoders\n",
    "    :param model: scikit-learn Model\n",
    "    :return pipe: sklearn.pipeline.Pipline\n",
    "    Examples of model param:\n",
    "\n",
    "    model = ComplementNB(norm=True,fit_prior=True,)\n",
    "    model = MultinomialNB()\n",
    "    model = LogisticRegression(n_jobs=-1, max_iter=10000,random_state=19)\n",
    "    \"\"\"\n",
    "    _steps = []\n",
    "    encoder__name = cat_encoder.__class__.__name__\n",
    "    _steps.append(\n",
    "        (\"PW\" + encoder__name, PolynomialWrapper(feature_encoder=cat_encoder))\n",
    "    )\n",
    "    if model is None:\n",
    "        passordinal_columns\n",
    "    else:\n",
    "        model__name = model.__class__.__name__\n",
    "        _steps.append((model__name, model))\n",
    "    pipe = Pipeline(steps=_steps)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be34c5-7688-44ca-a1ff-997c325ac06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    AdaBoostClassifier(),\n",
    "    BaggingClassifier(),\n",
    "    BernoulliNB(),\n",
    "    CalibratedClassifierCV(),\n",
    "    CategoricalNB(),\n",
    "    ComplementNB(),\n",
    "    DecisionTreeClassifier(),\n",
    "    DummyClassifier(),\n",
    "    ExtraTreeClassifier(),\n",
    "    ExtraTreesClassifier(),\n",
    "    GaussianNB(),\n",
    "    GaussianProcessClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    HistGradientBoostingClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    LabelPropagation(),\n",
    "    LabelSpreading(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    LinearSVC(),\n",
    "    LogisticRegression(),\n",
    "    LogisticRegressionCV(),\n",
    "    MLPClassifier(),\n",
    "    MultinomialNB(),\n",
    "    NearestCentroid(),\n",
    "    NuSVC(),\n",
    "    XGBClassifier(),\n",
    "    XGBRFClassifier(),\n",
    "    PassiveAggressiveClassifier(),\n",
    "    Perceptron(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    RadiusNeighborsClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    RidgeClassifier(),\n",
    "    RidgeClassifierCV(),\n",
    "    SGDClassifier(),\n",
    "    SVC(),\n",
    "]\n",
    "\n",
    "\n",
    "# combiners = [sklearn.multioutput.ClassifierChain,\n",
    "#  sklearn.multioutput.MultiOutputClassifier,\n",
    "#  sklearn.multiclass.OneVsOneClassifier,\n",
    "#  sklearn.multiclass.OneVsRestClassifier,\n",
    "#  sklearn.multiclass.OutputCodeClassifier,\n",
    "#  sklearn.ensemble._stacking.StackingClassifier,\n",
    "#  sklearn.ensemble._voting.VotingClassifier\n",
    "# ]\n",
    "\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7324839-adc8-499e-8d0e-a4959a233483",
   "metadata": {},
   "source": [
    "```python\n",
    "Template\n",
    "X = final_data.loc[:,categories]\n",
    "y = final_data.target.to_numpy().reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=10, test_size=0.3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99833584-d6fd-4fa7-9a45-79d42c05b025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ohe = OneHotEncoder(min_frequency=0.0001, handle_unknown='infrequent_if_exist', sparse=False,dtype=np.int32)\n",
    "# X_train_t = ohe.fit_transform(nominal_data)\n",
    "reports = []\n",
    "fig, ax = plt.subplots(3,1,figsize=(10,10))\n",
    "\n",
    "def analyze_model(ax=ax,i = i,X=X,y=y, pipe=pipe, feature_names=categories):\n",
    "    with parallel_backend('multiprocessing'):\n",
    "        cv_model = cross_validate(pipe, X, y, cv = cv_,return_train_score=True,n_jobs=-1)\n",
    "        ax[i].plot(np.arange(15), cv_model['test_score'], label=f'{feature_names[0].split(\"__\")[0].upper()}  Data')\n",
    "        ax[i].legend()\n",
    "        ax[2].plot(np.arange(15), cv_model['test_score'], label=f'{feature_names[0].split(\"__\")[0].upper()}  Data')\n",
    "        ax[2].legend()\n",
    "\n",
    "i = 0\n",
    "for categories in [nominal, ordinal]:\n",
    "    X = final_data.loc[:,categories]\n",
    "    y = final_data.target.to_numpy().reshape(-1,1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=10, test_size=0.3)\n",
    "    feature_names = categories\n",
    "    model = QuadraticDiscriminantAnalysis(priors=class_priors,store_covariance=True,reg_param=0.0001 )\n",
    "    cv_= RepeatedStratifiedKFold(n_splits=3,n_repeats=5, random_state=10)\n",
    "    pipe =  Pipeline(steps=[('polynomialwrapper',\n",
    "                 PWrapper(feature_encoder=WOEEncoder())),\n",
    "                (model.__class__.__name__,\n",
    "                 QuadraticDiscriminantAnalysis(priors=class_priors,\n",
    "                                               reg_param=0.0001,\n",
    "                                               store_covariance=True))], memory=CACHE_DIR)\n",
    "    \n",
    "    analyze_model(ax=ax,i = i,X=X,y=y, pipe=pipe, feature_names=categories)\n",
    "    y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "    print(classification_report(y_test,y_pred, output_dict=False))\n",
    "    c_report = classification_report(y_test,y_pred, output_dict=True)\n",
    "    reports.append(c_report)\n",
    "    # break\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b148e6c-f970-48df-97e9-1ba8678460fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ordinal_proc_data = data_logit.drop(binary+nominal+ratio+['label'], axis=1)\n",
    "\n",
    "# reports\n",
    "ordinal_proc_data.corrwith(final_data.target,method='kendall').sort_index().plot()\n",
    "\n",
    "# feature_correls = ordinal_proc_data.corrwith(final_data.target, method='kendall').sort_values(ascending=False).plot()\n",
    "ordinal_proc_data.loc[:,ordinal] = ordinal_proc_data.loc[:,ordinal].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab000d-1e9e-4ff5-8671-c4dbdede2b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ordinal_data\n",
    "cols = ordinal\n",
    "reports = []\n",
    "transformed_f = {}\n",
    "from category_encoders import OneHotEncoder as OHE\n",
    "category_encoders = [TargetEncoder(cols = ordinal), \n",
    "                     WOEEncoder(cols = ordinal),\n",
    "                     JamesSteinEncoder(cols = ordinal),\n",
    "                     HelmertEncoder(cols = ordinal),\n",
    "                     # category_encoders(handle_unknown='infrequent_if_exist',min_frequency=0.0001,sparse=False,drop='first'),\n",
    "                     SummaryEncoder(cols = ordinal),\n",
    "                     LeaveOneOutEncoder(cols = ordinal),\n",
    "                     BackwardDifferenceEncoder(cols = ordinal)]\n",
    "for ce in category_encoders:\n",
    "    fail = 0\n",
    "    encoder = PWrapper(ce)\n",
    "    try:\n",
    "        f_tra = encoder.fit_transform(ordinal_proc_data,final_data.target)\n",
    "    except:\n",
    "        print(f'{ce} failed')\n",
    "        print('Trying to run Solo')\n",
    "        fail=1\n",
    "    try:\n",
    "        if fail == 1:\n",
    "            f_tra = ce.fit_transform(ordinal_proc_data,final_data.target)\n",
    "    except:\n",
    "        print(\"Even Solo Failed !!! SKIPPING!!!!!\")\n",
    "        fail=0\n",
    "        continue\n",
    "    fcorr = f_tra.corrwith(final_data.target,method='kendall').sort_values(ascending=False)\n",
    "    reports.append(fcorr)\n",
    "    fail=0\n",
    "    transformed_f[ce.__class__.__name__]= f_tra \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531dd63-b126-46dc-bb08-566e5b446b56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = 0.25\n",
    "for i in range(7):\n",
    "    p_l: pd.Series = reports[i]\n",
    "    print(list(transformed_f.keys())[i])\n",
    "    print(p_l[(p_l>threshold)|(p_l<-threshold)].index)\n",
    "    print(p_l[(p_l>threshold)|(p_l<-threshold)].shape)\n",
    "    print(p_l[(p_l>threshold)|(p_l<-threshold)].mean())\n",
    "    print(p_l[(p_l>threshold)|(p_l<-threshold)])\n",
    "    print()\n",
    "    print('-'*40)\n",
    "# plt.hlines(0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5862d-e7e2-4d07-a228-fd823f626144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chosen_metrics(y_pred,y_test,chosen_encoder=None,chosen_pipe=None):\n",
    "    print('-'*90)\n",
    "    print(chosen_encoder)\n",
    "    print('-'*30)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('-'*90)\n",
    "    print(\"cohen_kappa_score\\t|\")\n",
    "    print('-'*30)\n",
    "    print(sklearn.metrics.cohen_kappa_score(y_pred, y_test))\n",
    "    print('-'*90)\n",
    "    print(\"balanced_accuracy_score\\t|\")\n",
    "    print('-'*30)\n",
    "    print(sklearn.metrics.balanced_accuracy_score(y_test, y_pred))\n",
    "    print('-'*90)\n",
    "    print(\"accuracy_score\\t|\")\n",
    "    print('-'*30)\n",
    "    print(\n",
    "        sklearn.metrics.accuracy_score(\n",
    "            y_test,\n",
    "            y_pred,\n",
    "        )\n",
    "        * 0.85\n",
    "    )\n",
    "    print('-'*90)\n",
    "    print(\"f1_score_micro\\t|\")\n",
    "    print('-'*30)\n",
    "    print(sklearn.metrics.f1_score(y_test, y_pred, average=\"micro\"))\n",
    "    print('-'*90)\n",
    "    print(\"f1_score_macro\\t|\")\n",
    "    print('-'*30)\n",
    "    print(sklearn.metrics.f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    print('-'*90)\n",
    "    print(\"f1_score_weighted\\t|\")\n",
    "    print('-'*30)\n",
    "    print(sklearn.metrics.f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "    print('-'*90)\n",
    "\n",
    "    \n",
    "category_encoders = [\n",
    "    PWrapper(TargetEncoder(cols = ordinal,smoothing=0)), \n",
    "    PWrapper(WOEEncoder(cols = ordinal,regularization=0.00000001)),\n",
    "    PWrapper(JamesSteinEncoder(cols = ordinal,model='pooled')),\n",
    "    HelmertEncoder(cols = ordinal),\n",
    "    # OrdinalEncoder(),\n",
    "    OHE(cols=ordinal,handle_missing='indicator',handle_unknown='indicator',),\n",
    "    SummaryEncoder(cols = ordinal,quantiles=np.linspace(0.01,1,num=20),m=0),\n",
    "    PWrapper(LeaveOneOutEncoder(cols = ordinal)),\n",
    "    BackwardDifferenceEncoder(cols = ordinal)\n",
    "]\n",
    "\n",
    "\n",
    "numeric_prob =list( set(X.columns) - set(ordinal))\n",
    "cat_ct = []\n",
    "for encoder in category_encoders:\n",
    "    c_ct = ColumnTransformer([\n",
    "        # ('cat_encoder', PWrapper(encoder),ordinal),\n",
    "        ('cat_encoder', encoder,ordinal),\n",
    "        ('numeric',MaxAbsScaler(),numeric_prob)\n",
    "    ], remainder='drop',n_jobs=-1,sparse_threshold=0,)\n",
    "    cat_ct.append(c_ct)\n",
    "\n",
    "\n",
    "# model = LogisticRegressionCV(n_jobs=-1,fit_intercept=False,max_iter=10000000,random_state=0,scoring='f1_macro')\n",
    "# model = XGBRFClassifier(n_jobs=-1,max_bin=256,verbosity=0,tree_method='exact',)\n",
    "\n",
    "ovo_model = OneVsOneClassifier(estimator=model, n_jobs=-1) \n",
    "ovr_model = OneVsRestClassifier(estimator=model, n_jobs=-1)\n",
    "# Loading Data\n",
    "X = data_logit.drop(binary+nominal+ratio+['label'], axis=1)\n",
    "y = final_data.target.to_numpy().reshape(-1,1)\n",
    "# Numeric Columns Isolator\n",
    "# c_ct\n",
    "final_model = ovo_model\n",
    "try:\n",
    "    with parallel_backend('loky'):\n",
    "        for i, ct in tqdm(enumerate(cat_ct)):\n",
    "            curr_flow = Pipeline([(\"Column_transformer\",ct),\n",
    "                            (\"model\",final_model)], memory=CACHE_DIR)\n",
    "            y_pred = curr_flow.fit(X_train, y_train).predict(X_test)\n",
    "            chosen_metrics(y_test,y_pred,\n",
    "                           chosen_encoder=category_encoders[i].__class__.__name__,\n",
    "                          chosen_pipe=curr_flow)\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print('STOPPED')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5099f2c1-77bd-4b44-91a4-14f169be7d33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "import neptune.new.integrations.sklearn as npt_utils\n",
    "import neptune.new.integrations.optuna as optuna_utils\n",
    "\n",
    "# A default connection mode is the asynchronous mode\n",
    "# Other possible values are \"async\", \"sync\", \"offline\", \"read-only\", and \"debug\"\n",
    "CONNECTION_MODE = \"offline\"\n",
    "run = neptune.init(project='mlop3n/SDP',custom_run_id='CAT_ENC_CLF__1.0',description=\"Experiment: Check Influence of Categorical Encoding on Predictive Performance of Vanilla Estimators\", mode=CONNECTION_MODE)\n",
    "neptune_callback = optuna_utils.NeptuneCallback(run)\n",
    "\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377ee65-a988-4e72-82ee-38281aa8ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e4ac8-becf-443f-b04d-3d283238bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelDB:\n",
    "    __model = XGBRFClassifier(n_jobs=-1,verbosity=0,tree_method='approx',learning_rate=1, random_state=42,base_score=0.82002,importance_type='total_gain', num_parallel_trees=50,subsample=1.0, grow_policy=1)\n",
    "    class_labels, n_classes, __priors = class_distribution(final_data.target.to_numpy().reshape(-1,1))\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.default_base_estimator = model_clone(self.__model)\n",
    "        self.help = 0\n",
    "        self.db ={}\n",
    "        self._defaults = dict(random_state=42, n_jobs=-1)\n",
    "        self.clfs = [\n",
    "            AdaBoostClassifier(base_estimator=self.__model,random_state=42),\n",
    "            BaggingClassifier(base_estimator=self.__model,**self._defaults),\n",
    "            # Init signature: BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n",
    "            BernoulliNB(class_prior=self.__priors),\n",
    "            CalibratedClassifierCV(),\n",
    "            CategoricalNB(),\n",
    "            ComplementNB(),\n",
    "            DecisionTreeClassifier(),\n",
    "            DummyClassifier(),\n",
    "            ExtraTreeClassifier(),\n",
    "            ExtraTreesClassifier(),\n",
    "            GaussianNB(),\n",
    "            GaussianProcessClassifier(),\n",
    "            GradientBoostingClassifier(),\n",
    "            HistGradientBoostingClassifier(),\n",
    "            KNeighborsClassifier(),\n",
    "            LabelPropagation(),\n",
    "            LabelSpreading(),\n",
    "            LinearDiscriminantAnalysis(),\n",
    "            LinearSVC(),\n",
    "            LogisticRegression(),\n",
    "            LogisticRegressionCV(),\n",
    "            MLPClassifier(),\n",
    "            MultinomialNB(),\n",
    "            NearestCentroid(),\n",
    "            NuSVC(),\n",
    "            XGBClassifier(),\n",
    "            XGBRFClassifier(),\n",
    "            PassiveAggressiveClassifier(),\n",
    "            Perceptron(),\n",
    "            QuadraticDiscriminantAnalysis(),\n",
    "            RadiusNeighborsClassifier(),\n",
    "            RandomForestClassifier(),\n",
    "            RidgeClassifier(),\n",
    "            RidgeClassifierCV(),\n",
    "            SGDClassifier(),\n",
    "            SVC(),\n",
    "        ]\n",
    "    def __call__(self, model_name = None):\n",
    "        pass\n",
    "\n",
    "mdb = ModelDB()\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae76b5a1-b5cb-4431-9b32-4fb44817eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcf4750-b5c3-47fb-8437-9c245451d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRFClassifier(n_jobs=-1,\n",
    "                        verbosity=0,\n",
    "                        n_estimators = 1000,\n",
    "                        tree_method='hist',\n",
    "                        # enable_categorical=True,\n",
    "                        learning_rate=1,\n",
    "                        random_state=42,\n",
    "                        base_score=0.82002,\n",
    "                        importance_type='total_gain',\n",
    "                        num_parallel_trees=50,\n",
    "                        subsample=1.0,\n",
    "                        # objective='multi:softmax',\n",
    "                        grow_policy=\"lossguide\",\n",
    "                        # max_cat_to_one_hot=1000\n",
    "                       )\n",
    "# model = SVC(random_state=42,probability=True,break_ties=True)\n",
    "# model = LogisticRegression(fit_intercept=False, random_state=42, n_jobs=-1)\n",
    "# model = RandomForestClassifier(n_jobs=-1,max_depth=30, random_state=42, ccp_alpha=0.001, max_features=None,bootstrap=False)\n",
    "# model = NuSVC(probability=True, break_ties=True, random_state=42, cache_size=1000)\n",
    "default_base_estimator = OneVsOneClassifier(model_clone(model), n_jobs=-1)\n",
    "# default_base_estimator is wrapped with multiclass trainer\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    ccV = cross_validate(default_base_estimator, final_data.loc[:,binary],final_data.target, n_jobs=-1,cv=RepeatedStratifiedKFold(n_repeats=5,n_splits=3,random_state=42), return_train_score=True, scoring='f1_macro')\n",
    "# plt.ioff\n",
    "fig,ax = plt.subplots(2,1) \n",
    "ax[0].plot(ccV['test_score'],'r--', label=\"Test Scores\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(ccV['train_score'],  label = \"Train Scores\")\n",
    "ax[1].legend()\n",
    "disp = fig.suptitle(\"Train Vs Test Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ec1ba2-3400-4e39-8563-dcbcdb6cff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_data.astype('int').skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73929bde-a2a5-4c68-99a8-152d8b312d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a439149-6aef-4990-ac5b-eb4f84ee4209",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b7b53-9025-4d74-b737-8efc3d51e9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd4c8b-9bf9-40e8-8c25-f6029b022d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optuna example that optimizes a classifier configuration for Iris dataset using sklearn.\n",
    "In this example, we optimize a classifier configuration for Iris dataset. Classifiers are from\n",
    "scikit-learn. We optimize both the choice of classifier (among SVC and RandomForest) and their\n",
    "hyperparameters.\n",
    "\"\"\"\n",
    "\n",
    "import optuna\n",
    "\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import sklearn.model_selection\n",
    "import sklearn.svm\n",
    "\n",
    "\n",
    "# FYI: Objective functions can take additional arguments\n",
    "# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\n",
    "def objective(trial):\n",
    "    iris = sklearn.datasets.load_iris()\n",
    "    x, y = iris.data, iris.target\n",
    "\n",
    "    classifier_name = trial.suggest_categorical(\"classifier\", [\"SVC\", \"RandomForest\"])\n",
    "    if classifier_name == \"SVC\":\n",
    "        svc_c = trial.suggest_float(\"svc_c\", 1e-10, 1e10, log=True)\n",
    "        classifier_obj = sklearn.svm.SVC(C=svc_c, gamma=\"auto\")\n",
    "    else:\n",
    "        rf_max_depth = trial.suggest_int(\"rf_max_depth\", 2, 32, log=True)\n",
    "        classifier_obj = sklearn.ensemble.RandomForestClassifier(\n",
    "            max_depth=rf_max_depth, n_estimators=10\n",
    "        )\n",
    "\n",
    "    score = sklearn.model_selection.cross_val_score(classifier_obj, x, y, n_jobs=-1, cv=3)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # study_name=study_name, storage=storage_name, load_if_exists=True\n",
    "    study = optuna.create_study(study_name=\"SDP-IITM\", storage=OPTUNA_DB,  direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dfb7cd-0ba1-4d70-ba3f-f151ee1195f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optuna example that demonstrates a pruner for CatBoost.\n",
    "\n",
    "In this example, we optimize the validation accuracy of cancer detection using CatBoost.\n",
    "We optimize both the choice of booster models and their hyperparameters. Throughout\n",
    "training of models, a pruner observes intermediate results and stop unpromising trials.\n",
    "\n",
    "You can run this example as follows:\n",
    "    $ python catboost_pruning.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.integration import CatBoostPruningCallback\n",
    "\n",
    "import catboost as cb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    data, target = load_breast_cancer(return_X_y=True)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n",
    "\n",
    "    param = {\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\n",
    "            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "        ),\n",
    "        \"used_ram_limit\": \"3gb\",\n",
    "        \"eval_metric\": \"Accuracy\",\n",
    "    }\n",
    "\n",
    "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1, log=True)\n",
    "\n",
    "    gbm = cb.CatBoostClassifier(**param)\n",
    "\n",
    "    pruning_callback = CatBoostPruningCallback(trial, \"Accuracy\")\n",
    "    gbm.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        eval_set=[(valid_x, valid_y)],\n",
    "        verbose=0,\n",
    "        early_stopping_rounds=100,\n",
    "        callbacks=[pruning_callback],\n",
    "    )\n",
    "\n",
    "    # evoke pruning manually.\n",
    "    pruning_callback.check_pruned()\n",
    "\n",
    "    preds = gbm.predict(valid_x)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = accuracy_score(valid_y, pred_labels)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
    "        storage=OPTUNA_DB,\n",
    "        direction=\"maximize\"\n",
    "    )\n",
    "    study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278062f-7211-4882-a3aa-0d6717b61365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optuna example that demonstrates a pruner for XGBoost.\n",
    "In this example, we optimize the validation accuracy of cancer detection using XGBoost.\n",
    "We optimize both the choice of booster model and their hyperparameters. Throughout\n",
    "training of models, a pruner observes intermediate results and stop unpromising trials.\n",
    "You can run this example as follows:\n",
    "    $ python xgboost_integration.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# FYI: Objective functions can take additional arguments\n",
    "# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\n",
    "def objective(trial):\n",
    "    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    # Add a callback for pruning.\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-auc\")\n",
    "    bst = xgb.train(param, dtrain, evals=[(dvalid, \"validation\")], callbacks=[pruning_callback])\n",
    "    preds = bst.predict(dvalid)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), \n",
    "        storage=OPTUNA_DB,\n",
    "        direction=\"maximize\"\n",
    "    )\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    print(study.best_trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00bbdc-73dc-4c8d-bd8c-ab4e701006d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import logging\n",
    "# optuna.logging.set_verbosity(0)\n",
    "LOG_FILE_PATH = \"../data/optuna.log\"\n",
    "# logger.setLevel(logging.INFO)  # Setup the root logger.\n",
    "# optuna.logging.disable_default_handler()\n",
    "logger.addHandler(logging.FileHandler(LOG_FILE_PATH, mode=\"w\"))\n",
    "\n",
    "optuna.logging.enable_propagation()  # Propagate logs to the root logger.\n",
    "optuna.logging.disable_default_handler()  # Stop showing logs in sys.stderr.\n",
    "\n",
    "        \n",
    "study = optuna.create_study(storage=OPTUNA_DB,)\n",
    "\n",
    "logger.info(\"Start optimization.\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "with open(LOG_FILE_PATH) as f:\n",
    "    assert f.readline().startswith(\"A new study created\")\n",
    "    assert f.readline() == \"Start optimization.\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0525561-3b7a-45b7-8f0f-bf51454d7f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import optuna\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    trial.set_user_attr(\"BATCHSIZE\", 128)\n",
    "    momentum = trial.suggest_float(\"momentum\", 0, 1.0)\n",
    "    clf = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        batch_size=trial.user_attrs[\"BATCHSIZE\"],\n",
    "        momentum=momentum,\n",
    "        solver=\"sgd\",\n",
    "        random_state=0,\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    return clf.score(X_valid, y_valid)\n",
    "\n",
    "\n",
    "study = optuna.create_study(storage=OPTUNA_DB,direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32827fc2-3e2f-4813-a02b-0d3b17f1a03a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autoskl)",
   "language": "python",
   "name": "autoskl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "neptune": {
   "notebookId": "96c56ed5-cedc-489d-b67b-a97867268074",
   "projectVersion": 2
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
