{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f3c0459-482d-4867-8f09-ba1723a13fac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import warnings\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "patch_sklearn()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import (BackwardDifferenceEncoder, BaseNEncoder,\n",
    "                               BinaryEncoder, CatBoostEncoder, CountEncoder,\n",
    "                               GLMMEncoder, HelmertEncoder, JamesSteinEncoder,\n",
    "                               LeaveOneOutEncoder, MEstimateEncoder,\n",
    "                               QuantileEncoder, SummaryEncoder, TargetEncoder,\n",
    "                               WOEEncoder)\n",
    "from sklearn import set_config\n",
    "from sklearn.base import clone as model_clone\n",
    "from sklearn.cluster import *\n",
    "from sklearn.compose import *\n",
    "from sklearn.cross_decomposition import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.gaussian_process import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.multioutput import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.utils import *\n",
    "from tqdm import tqdm, trange\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from BorutaShap import BorutaShap\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pd.options.display.max_columns = 50\n",
    "set_config(display=\"diagram\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import parallel_backend\n",
    "from joblib.memory import Memory\n",
    "\n",
    "sns.set()\n",
    "from pprint import pprint\n",
    "from helpers import PolynomialWrapper as PWrapper\n",
    "from helpers import NestedCVWrapper as NCVWrapper\n",
    "from helpers import ColumnSelectors\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e75faf5-1cae-4898-b359-b179dc8546a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import DFCollection\n",
    "db = DFCollection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51d56e4-ae71-4c66-b71a-543fb11933de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save /home/mlop3n/PycharmProjects/sdpiit/data/train?(Yes/No/Exit) Yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mlop3n/PycharmProjects/sdpiit/data/trainSaved Successfully\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save /home/mlop3n/PycharmProjects/sdpiit/data/test?(Yes/No/Exit) Yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mlop3n/PycharmProjects/sdpiit/data/testSaved Successfully\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save /home/mlop3n/PycharmProjects/sdpiit/data/data_with_ridit?(Yes/No/Exit) Yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mlop3n/PycharmProjects/sdpiit/data/data_with_riditSaved Successfully\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save /home/mlop3n/PycharmProjects/sdpiit/data/final_data?(Yes/No/Exit) Yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mlop3n/PycharmProjects/sdpiit/data/final_dataSaved Successfully\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save /home/mlop3n/PycharmProjects/sdpiit/data/final_pred_data?(Yes/No/Exit) Yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mlop3n/PycharmProjects/sdpiit/data/final_pred_dataSaved Successfully\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save /home/mlop3n/PycharmProjects/sdpiit/data/baseline?(Yes/No/Exit) Yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mlop3n/PycharmProjects/sdpiit/data/baselineSaved Successfully\n"
     ]
    }
   ],
   "source": [
    "db.save_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aebac6f9-6fad-4c50-bdc9-ab04d2613cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "def __table__(rows, margin=10, columns=[]):\n",
    "        \"\"\"\n",
    "        Return string representing table content, returns table as string and as a list of strings.\n",
    "        It is okay for rows to have different sets of keys, table will show union of columns with\n",
    "        missing values being empty spaces.\n",
    "        :param rows: list of dictionaries as rows\n",
    "        :param margin: left space padding to apply to each row, default is 0\n",
    "        :param columns: extract listed columns in provided order, other columns will be ignored\n",
    "        :return: table content as string and as list\n",
    "        \"\"\"\n",
    "\n",
    "        def projection(cols, columns):\n",
    "            return (\n",
    "                [(x, cols[x]) for x in columns if x in cols]\n",
    "                if columns\n",
    "                else cols.items()\n",
    "            )\n",
    "\n",
    "        def row_to_string(row, columns):\n",
    "            values = [\n",
    "                (row[name] if name in row else \"\").rjust(size) for name, size in columns\n",
    "            ]\n",
    "            return \"|%s|\" % (\"|\".join(values))\n",
    "\n",
    "        def header(columns):\n",
    "            return \"|%s|\" % (\"|\".join([name.rjust(size) for name, size in columns]))\n",
    "\n",
    "        def divisor(columns):\n",
    "            return \"+%s+\" % (\"+\".join([\"-\" * size for name, size in columns]))\n",
    "\n",
    "        data = [dict([(str(a), str(b)) for a, b in row.items()]) for row in rows]\n",
    "        cols = (\n",
    "            dict([(x, len(x) + 1) for row in data for x in row.keys()]) if data else {}\n",
    "        )\n",
    "        for row in data:\n",
    "            for key in row.keys():\n",
    "                cols[key] = max(cols[key], len(row[key]) + 1)\n",
    "        proj = projection(\n",
    "            cols, columns\n",
    "        )  # extract certain columns to display (or all if not provided)\n",
    "        table = (\n",
    "            [divisor(proj), header(proj), divisor(proj)]\n",
    "            + [row_to_string(row, proj) for row in data]\n",
    "            + [divisor(proj)]\n",
    "        )\n",
    "        table = [\"%s%s\" % (\" \" * margin, tpl) for tpl in table] if margin > 0 else table\n",
    "        table_text = \"\\n\".join(table)\n",
    "        return (table_text, table)\n",
    "\n",
    "def tablify( rows, margin=10, columns=[]):\n",
    "    \"\"\"\n",
    "    Print table in console for list of rows.\n",
    "    \"\"\"\n",
    "    txt, _ = __table__(rows, margin, columns)\n",
    "    # txt = txt\n",
    "    print(txt)\n",
    "cls_names = [\n",
    "    sklearn.ensemble._weight_boosting.AdaBoostClassifier,\n",
    "    sklearn.naive_bayes.BernoulliNB,\n",
    "    # sklearn.naive_bayes.CategoricalNB,\n",
    "    # sklearn.naive_bayes.ComplementNB,\n",
    "    sklearn.tree._classes.DecisionTreeClassifier,\n",
    "    sklearn.tree._classes.ExtraTreeClassifier,\n",
    "    sklearn.ensemble._forest.ExtraTreesClassifier,\n",
    "    sklearn.naive_bayes.GaussianNB,\n",
    "    # sklearn.gaussian_process._gpc.GaussianProcessClassifier,\n",
    "    sklearn.ensemble._gb.GradientBoostingClassifier,\n",
    "    sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
    "    sklearn.neighbors.KNeighborsClassifier,\n",
    "    sklearn.svm._classes.LinearSVC,\n",
    "    sklearn.linear_model.LogisticRegression,\n",
    "    # sklearn.linear_model._logistic.LogisticRegressionCV,\n",
    "    # sklearn.neural_network._multilayer_perceptron.MLPClassifier,\n",
    "    sklearn.naive_bayes.MultinomialNB,\n",
    "    # sklearn.neighbors._nearest_centroid.NearestCentroid,\n",
    "    sklearn.svm.NuSVC,\n",
    "    sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier,\n",
    "    sklearn.linear_model._perceptron.Perceptron,\n",
    "    # sklearn.neighbors._classification.RadiusNeighborsClassifier,\n",
    "    sklearn.ensemble._forest.RandomForestClassifier,\n",
    "    sklearn.linear_model._ridge.RidgeClassifier,\n",
    "    # sklearn.linear_model._ridge.RidgeClassifierCV,\n",
    "    sklearn.linear_model._stochastic_gradient.SGDClassifier,\n",
    "    sklearn.svm.SVC,\n",
    "]\n",
    "column_selector = ColumnSelectors()\n",
    "classifiers = [f() for f in cls_names]\n",
    "dtype_info = column_selector.dtype_info\n",
    "\n",
    "ordinal = column_selector.ordinal_cols\n",
    "nominal = column_selector.nominal_cols\n",
    "binary = column_selector.binary_cols\n",
    "ratio = column_selector.ratio_cols\n",
    "\n",
    "\n",
    "final_data = pd.read_parquet(\"../data/final_data.parquet\", engine=\"fastparquet\")\n",
    "final_pred_data = pd.read_parquet(\n",
    "    \"../data/final_pred_data.parquet\", engine=\"fastparquet\"\n",
    ")\n",
    "baseline_prediction_data = pd.read_parquet(\"../data/baseline.parquet\")\n",
    "data__ = pd.read_parquet(\"../data/data_with_ridit.hdfs\", engine=\"fastparquet\")\n",
    "prediction_data = pd.read_parquet(\"../data/test.parquet\", engine=\"fastparquet\")\n",
    "data = pd.read_parquet(\"../data/train.parquet\", engine=\"fastparquet\")\n",
    "\n",
    "\n",
    "\n",
    "def categorise_data(data):\n",
    "    ordinal_data = data.loc[:, ordinal]\n",
    "    nominal_data = data.loc[:, nominal]\n",
    "    binary_data = data.loc[:, binary]\n",
    "    ratio_data = data.loc[:, ratio]\n",
    "    return ordinal_data, nominal_data, binary_data, ratio_data\n",
    "\n",
    "\n",
    "# ordinal_data, nominal_data, binary_data, ratio_data = categorise_data(final_data)\n",
    "# final_data.rename(columns={\"label\": \"target\"}, inplace=True)\n",
    "# data.rename(columns={\"label\": \"target\"}, inplace=True)\n",
    "# baseline_prediction_data\n",
    "bs = pd.concat([final_data, baseline_prediction_data], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "def gen_balanced_trained_test(data, p):\n",
    "    Y = data.target\n",
    "    X_2 = Y_2 = Y[Y == 2].index\n",
    "    X_0 = Y_0 = Y[Y == 0].index\n",
    "    X_1 = Y_1 = Y[Y == 1].index\n",
    "    train_size = int(p * Y_2.shape[0])\n",
    "    test_size = int((1 - p) * Y_2.shape[0])\n",
    "\n",
    "    train_idx_2 = np.random.choice(Y_2, (train_size,))\n",
    "    train_idx_1 = np.random.choice(Y_1, (train_size,))\n",
    "    train_idx_0 = np.random.choice(Y_0, (train_size,))\n",
    "    train_idx = np.r_[train_idx_0, train_idx_1, train_idx_2]\n",
    "    # train_idx.shape\n",
    "\n",
    "    test_idx_2 = np.random.choice(np.setdiff1d(Y_2, train_idx_2), (test_size,))\n",
    "    test_idx_1 = np.random.choice(np.setdiff1d(Y_1, train_idx_1), (test_size,))\n",
    "    test_idx_0 = np.random.choice(np.setdiff1d(Y_0, train_idx_0), (test_size,))\n",
    "    test_idx = np.r_[test_idx_0, test_idx_1, test_idx_2]\n",
    "    # test_idx.shape\n",
    "    return train_idx, test_idx\n",
    "\n",
    "\n",
    "def gen_nominal_maps(bs: pd.DataFrame = bs) -> tuple[defaultdict, defaultdict]:\n",
    "    nominal_master_db = bs.loc[:, nominal]\n",
    "    nominal_cont_map = defaultdict(dict)\n",
    "    nominal_indvl_map = defaultdict(dict)\n",
    "    for c in nominal:\n",
    "        un = sorted(nominal_master_db[c].unique().tolist())\n",
    "        n = len(un)\n",
    "        new_id = list(range(n))\n",
    "        nominal_indvl_map[c] = dict(zip(un, new_id))\n",
    "    start = 0\n",
    "    for c in nominal:\n",
    "        un = sorted(nominal_master_db[c].unique().tolist())\n",
    "        n = len(un)\n",
    "        new_id = list(range(start, start + n))\n",
    "        nominal_cont_map[c] = dict(zip(un, new_id))\n",
    "        start += n\n",
    "    return nominal_indvl_map, nominal_cont_map\n",
    "\n",
    "\n",
    "# nominal_indvl_map, nominal_cont_map = gen_nominal_maps()\n",
    "# nominal_master_db = bs.loc[:, nominal]\n",
    "\n",
    "# nominal_master_db_indvl = nominal_master_db.copy()\n",
    "# nominal_master_db_cont = nominal_master_db.copy()\n",
    "\n",
    "\n",
    "# nominal_indvl_map\n",
    "def nm_indvl_data_trnsform(row):\n",
    "    for c in nominal:\n",
    "        curr = row[c]\n",
    "        row[c] = nominal_indvl_map[c][curr]\n",
    "    return row\n",
    "\n",
    "\n",
    "# test1_nominal = nominal_master_db_indvl.apply(nm_indvl_data_trnsform, axis=1)\n",
    "\n",
    "\n",
    "def nm_cont_data_trnsform(row):\n",
    "    for c in nominal:\n",
    "        curr = row[c]\n",
    "        row[c] = nominal_cont_map[c][curr]\n",
    "    return row\n",
    "\n",
    "\n",
    "# test2_nominal = nominal_master_db_cont.apply(nm_cont_data_trnsform, axis=1)\n",
    "\n",
    "\n",
    "def best_n_features(n, data=pd.DataFrame(), target=pd.DataFrame):\n",
    "    if (data.shape[0] > 0) and (target.shape[0] > 0):\n",
    "        mic = mutual_info_classif(data, target, discrete_features=True)\n",
    "    else:\n",
    "        default_data = pd.read_parquet(\n",
    "            \"../data/final_data.parquet\", engine=\"fastparquet\"\n",
    "        )\n",
    "        data_X = default_data.drop([\"target\"], axis=1)\n",
    "        data_y = default_data.target\n",
    "    return ohe.get_feature_names_out()[mic.argsort()[-n:]]\n",
    "\n",
    "\n",
    "# prediction_data = pd.read_pickle(\"../data/pred_data.pkl\")\n",
    "# est_ = [(\"cnb\",CategoricalNB()),]\n",
    "def iv_woe(data, target, bins=10, show_woe=False):\n",
    "    \"\"\"\n",
    "    :params data: pandas.DataFrame\n",
    "    :params target: str\n",
    "    :params bins: int\n",
    "    :params show_woe: bool\n",
    "    :returns newDF: pandas.DataFrame, woeDF: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    # Empty Dataframe\n",
    "    newDF, woeDF = pd.DataFrame(), pd.DataFrame()\n",
    "    # Extract Column Names\n",
    "    cols = data.columns\n",
    "    # Run WOE and IV on all the independent variables\n",
    "    for ivars in cols[~cols.isin([target])]:\n",
    "        if (data[ivars].dtype.kind in \"bifc\") and (len(np.unique(data[ivars])) > 1000):\n",
    "            binned_x = pd.qcut(data[ivars], bins, duplicates=\"drop\")\n",
    "            d0 = pd.DataFrame({\"x\": binned_x, \"y\": data[target]})\n",
    "        else:\n",
    "            d0 = pd.DataFrame({\"x\": data[ivars], \"y\": data[target]})\n",
    "        d0 = d0.astype({\"x\": str})\n",
    "        d = d0.groupby(\"x\", as_index=False, dropna=False).agg({\"y\": [\"count\", \"sum\"]})\n",
    "        d.columns = [\"Cutoff\", \"N\", \"Events\"]\n",
    "        d[\"% of Events\"] = np.maximum(d[\"Events\"], 0.5) / d[\"Events\"].sum()\n",
    "        d[\"Non-Events\"] = d[\"N\"] - d[\"Events\"]\n",
    "        d[\"% of Non-Events\"] = np.maximum(d[\"Non-Events\"], 0.5) / d[\"Non-Events\"].sum()\n",
    "        d[\"WoE\"] = np.log(d[\"% of Non-Events\"] / d[\"% of Events\"])\n",
    "        d[\"IV\"] = d[\"WoE\"] * (d[\"% of Non-Events\"] - d[\"% of Events\"])\n",
    "        d.insert(loc=0, column=\"Variable\", value=ivars)\n",
    "        print(\"Information value of \" + ivars + \" is \" + str(round(d[\"IV\"].sum(), 6)))\n",
    "        temp = pd.DataFrame(\n",
    "            {\"Variable\": [ivars], \"IV\": [d[\"IV\"].sum()]}, columns=[\"Variable\", \"IV\"]\n",
    "        )\n",
    "        newDF = pd.concat([newDF, temp], axis=0)\n",
    "        woeDF = pd.concat([woeDF, d], axis=0)\n",
    "        # Show WOE Table\n",
    "        if show_woe == True:\n",
    "            print(d)\n",
    "    return newDF, woeDF\n",
    "\n",
    "\n",
    "def wf_create(cat_encoder=TargetEncoder, model=None):\n",
    "    \"\"\"\n",
    "    :param cat_encoder: category_encoders\n",
    "    :param model: scikit-learn Model\n",
    "    :return pipe: sklearn.pipeline.Pipline\n",
    "    Examples of model param:\n",
    "\n",
    "    model = ComplementNB(norm=True,fit_prior=True,)\n",
    "    model = MultinomialNB()\n",
    "    model = LogisticRegression(n_jobs=-1, max_iter=10000,random_state=19)\n",
    "    \"\"\"\n",
    "    _steps = []\n",
    "    encoder__name = cat_encoder.__class__.__name__\n",
    "    _steps.append(\n",
    "        (\"PW\" + encoder__name, PolynomialWrapper(feature_encoder=cat_encoder))\n",
    "    )\n",
    "    if model is None:\n",
    "        passordinal_columns\n",
    "    else:\n",
    "        model__name = model.__class__.__name__\n",
    "        _steps.append((model__name, model))\n",
    "    pipe = Pipeline(steps=_steps)\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def plot_m_variance(d_coll: list[tuple[float, float, float, float]]):\n",
    "    \"\"\"\n",
    "    :param d_coll: list[tuple[float, float, float, float]\n",
    "    list of data points in the format\n",
    "    (abcissa, mean, std, max)\n",
    "    \"\"\"\n",
    "    sns.set()\n",
    "    ddx = [x for x, y, u, r in d_coll]\n",
    "    ddc = [y for x, y, u, r in d_coll]\n",
    "    ddep = [y + u for x, y, u, r in d_coll]\n",
    "    dden = [y - u for x, y, u, r in d_coll]\n",
    "    ddem = [r for x, y, u, r in d_coll]\n",
    "    plt.plot(ddx, ddc, \"b\", label=\"\\u00b5\")\n",
    "    plt.plot(ddx, ddep, \"r\", label=\"\\u03c3\" + \"+\")\n",
    "    plt.plot(ddx, dden, \"g\", label=\"\\u03c3\" + \"-\")\n",
    "    plt.plot(ddx, ddem, \"y\", label=\"\\u03c3\" + \"max\")\n",
    "    fig = plt.fill_between(ddx, ddep, dden, alpha=0.5)\n",
    "    fig = plt.legend()\n",
    "    plt.show()\n",
    "    # return plt\n",
    "\n",
    "\n",
    "# # print(pipe)\n",
    "# # wf_create(TargetEncoder(),RandomForestClassifier())\n",
    "# final_data_indvl = final_data.copy()\n",
    "# final_pred_data_indvl_test = final_pred_data.copy()\n",
    "# final_data_indvl.loc[:, nominal] = test1_nominal.loc[final_data.index, :]\n",
    "# # final_pred_data_indvl_test.loc[:, nominal] = test1_nominal.loc[3796:, :]\n",
    "# pred_data_indvl = test1_nominal.loc[3796:,nominal].copy()\n",
    "# pred_data_indvl.index = final_pred_data.index\n",
    "# final_pred_data_indvl_test.loc[:, nominal] = pred_data_indvl\n",
    "# # final_pred_data_indvl_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37e79903-8319-449d-afb6-ed413cdf9c6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m {final_data:\u001b[38;5;241m1\u001b[39m}\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'DataFrame'"
     ]
    }
   ],
   "source": [
    "{final_data:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaf7b26-a53f-4da9-868a-1e21f78c3ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autoskl)",
   "language": "python",
   "name": "autoskl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
