{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0516406-127e-4861-a1aa-1fce771ecb38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "os.environ[\"NEPTUNE_PROJECT\"] = \"mlop3n/SDP\"\n",
    "os.environ[\n",
    "    \"NEPTUNE_NOTEBOOK_PATH\"\n",
    "] = \"PycharmProjects/sdpiit/notebooks/Pipeline_components_builder.ipynb\"\n",
    "import warnings\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "patch_sklearn()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import (\n",
    "    BackwardDifferenceEncoder,\n",
    "    BaseNEncoder,\n",
    "    BinaryEncoder,\n",
    "    CatBoostEncoder,\n",
    "    CountEncoder,\n",
    "    GLMMEncoder,\n",
    "    HelmertEncoder,\n",
    "    JamesSteinEncoder,\n",
    "    LeaveOneOutEncoder,\n",
    "    MEstimateEncoder,\n",
    "    QuantileEncoder,\n",
    "    SummaryEncoder,\n",
    "    TargetEncoder,\n",
    "    WOEEncoder,\n",
    ")\n",
    "from sklearn import set_config\n",
    "from sklearn.base import clone as model_clone\n",
    "from sklearn.cluster import *\n",
    "from sklearn.compose import *\n",
    "from sklearn.cross_decomposition import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.gaussian_process import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.multioutput import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.utils import *\n",
    "from sklearn.dummy import *\n",
    "from sklearn.semi_supervised import *\n",
    "from sklearn.discriminant_analysis import *\n",
    "import sklearnex, daal4py\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from BorutaShap import BorutaShap\n",
    "\n",
    "from sklearn.calibration import *\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pd.options.display.max_columns = 50\n",
    "set_config(display=\"diagram\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import parallel_backend\n",
    "from joblib.memory import Memory\n",
    "\n",
    "sns.set()\n",
    "from pprint import pprint\n",
    "from helpers import PolynomialWrapper as PWrapper\n",
    "from helpers import NestedCVWrapper as NCVWrapper\n",
    "from helpers import ColumnSelectors\n",
    "import sklearn\n",
    "\n",
    "from helpers import DFCollection\n",
    "from helpers import plot_mean_std_max\n",
    "from helpers import CustomMetrics\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "CACHE_DIR = Memory(location=\"../data/joblib_memory/\")\n",
    "# OPTUNA_DB = \"postgresql+psycopg2://postgres:302492@localhost:5433/optuna\"\n",
    "from REDIS_CONFIG import REDIS_URL\n",
    "\n",
    "OPTUNA_DB = REDIS_URL\n",
    "\n",
    "\n",
    "def allow_stopping(func):\n",
    "    def wrapper():\n",
    "        try:\n",
    "            value = func()\n",
    "            return value\n",
    "            # gc.collect()\n",
    "        except KeyboardInterrupt as e:\n",
    "            print(\"Program Stopped\")\n",
    "        gc.collect()\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb4014-05a9-45d4-a630-615dc8cada89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = DFCollection()\n",
    "column_selector = ColumnSelectors()\n",
    "# classifiers = [f() for f in cls_names]\n",
    "dtype_info = column_selector.dtype_info\n",
    "ordinal = column_selector.ordinal_cols\n",
    "nominal = column_selector.nominal_cols\n",
    "binary = column_selector.binary_cols\n",
    "ratio = column_selector.ratio_cols\n",
    "\n",
    "\n",
    "final_data = db.final_data\n",
    "final_pred_data = db.final_pred_data\n",
    "baseline_prediction_data = db.baseline_prediction_data\n",
    "data_logit = db.data_logits\n",
    "prediction_data = db.prediction_data\n",
    "master_data = db.master\n",
    "given_data = db.data\n",
    "\n",
    "ordinal_data, nominal_data, binary_data, ratio_data = db.categorise_data()\n",
    "nominal_categories = db.nominal_categories\n",
    "ordinal_categories = db.ordinal_categories\n",
    "class_labels, n_classes, class_priors = class_distribution(\n",
    "    final_data.target.to_numpy().reshape(-1, 1)\n",
    ")\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, drop=\"first\")\n",
    "variance_thr = VarianceThreshold(0.001)\n",
    "nominal_ohe_pipe = Pipeline(\n",
    "    steps=[(\"ohe\", encoder), (\"var_th\", variance_thr)],\n",
    "    memory=Memory(location=CACHE_DIR),\n",
    ")\n",
    "ohe_nominal_data = nominal_ohe_pipe.fit_transform(\n",
    "    nominal_data.drop([\"nominal__v_12\", \"nominal__v_21\"], axis=1)\n",
    ")\n",
    "n1df = pd.DataFrame(\n",
    "    ohe_nominal_data,\n",
    "    columns=nominal_ohe_pipe.get_feature_names_out(),\n",
    "    index=nominal_data.index,\n",
    ")\n",
    "\n",
    "\n",
    "def train_test(X, y, test_size):\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, random_state=10, test_size=test_size, stratify=y\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024db51-05c1-454f-a3a1-2011edadae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = final_data.target\n",
    "encoder = OneHotEncoder(sparse=False, drop=\"first\", min_frequency=0.0001)\n",
    "# nominal_data_test = final_pred_data.loc[:,nominal]\n",
    "variance_thr = VarianceThreshold(0.001)\n",
    "nominal_ohe_pipe = Pipeline(\n",
    "    steps=[(\"ohe\", encoder), (\"var_th\", variance_thr)],\n",
    "    memory=Memory(location=CACHE_DIR),\n",
    ")\n",
    "\n",
    "\n",
    "scf = {\"F\": f_classif, \"CHI\": chi2}\n",
    "\n",
    "scf2 = {\"MIC\": mutual_info_classif, \"F\": f_classif, \"CHI\": chi2}\n",
    "\n",
    "\n",
    "K_BEST = 40\n",
    "ALPHA = 0.05\n",
    "all_selectors = {}\n",
    "PERCENTILE = 5\n",
    "\n",
    "\n",
    "def update_selectors(alpha_=ALPHA, k_best=K_BEST, percentile=PERCENTILE):\n",
    "    global all_selectors, scf, scf2\n",
    "    for criterion, _scf in scf.items():\n",
    "        selectors = [\n",
    "            SelectFpr(_scf, alpha=alpha_),\n",
    "            SelectFdr(_scf, alpha=alpha_),\n",
    "            SelectFwe(_scf, alpha=alpha_),\n",
    "        ]\n",
    "\n",
    "        for slctr in selectors:\n",
    "            all_selectors[criterion + \"-\" + slctr.__class__.__name__] = slctr\n",
    "    for criterion, _scf in scf2.items():\n",
    "        selectors = [\n",
    "            SelectKBest(_scf, k=K_BEST),\n",
    "            SelectPercentile(_scf, percentile=PERCENTILE),\n",
    "        ]\n",
    "        for slctr in selectors:\n",
    "            all_selectors[criterion + \"-\" + slctr.__class__.__name__] = slctr\n",
    "\n",
    "\n",
    "# Initialize the selectors\n",
    "update_selectors(alpha_=ALPHA, k_best=K_BEST)\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[(\"one_hot_enc\", nominal_ohe_pipe, nominal)],\n",
    "    sparse_threshold=0,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "ohe_nominal_feature_selection = make_pipeline(\n",
    "    ct, FeatureUnion(transformer_list=list(all_selectors.items()), n_jobs=-1)\n",
    ")\n",
    "# with open('../data/pipelines/ohe_nominal_features.pkl', 'wb') as fp:\n",
    "#     pickle.dump(ohe_nominal_feature_selection, fp, protocol=-1)\n",
    "# ohe_nominal_feature_selection\n",
    "# n1df_test = pd.DataFrame\n",
    "\n",
    "with parallel_backend(\"loky\"):\n",
    "    elite_ohe_nominal_features_train = ohe_nominal_feature_selection.fit_transform(\n",
    "        final_data, target\n",
    "    )\n",
    "    elite_ohe_nominal_features_test = ohe_nominal_feature_selection.transform(\n",
    "        final_pred_data\n",
    "    )\n",
    "    f_names = ohe_nominal_feature_selection.get_feature_names_out()\n",
    "    elite_onf_df = pd.DataFrame(elite_ohe_nominal_features_train, columns=f_names)\n",
    "    elite_onf_df = (\n",
    "        elite_onf_df.transpose()\n",
    "        .drop_duplicates(\n",
    "            ignore_index=False,\n",
    "        )\n",
    "        .transpose()\n",
    "    )\n",
    "\n",
    "    f_names_t = {\n",
    "        x: x.split(\"__\")[1] + \"__\" + x.split(\"__\")[3] for x in elite_onf_df.columns\n",
    "    }\n",
    "\n",
    "    # elite_ohe_nominal_features_train = elite_ohe_nominal_features_train.transpose().drop_duplicates(ignore_index=False,).transpose()\n",
    "    elite_onf_df.rename(columns=f_names_t, inplace=True)\n",
    "    elite_onf_df_test = pd.DataFrame(elite_ohe_nominal_features_test, columns=f_names)\n",
    "    elite_onf_df_test = (\n",
    "        elite_onf_df_test.transpose()\n",
    "        .drop_duplicates(\n",
    "            ignore_index=False,\n",
    "        )\n",
    "        .transpose()\n",
    "    )\n",
    "    elite_onf_df_test.rename(columns=f_names_t, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29009dc-d510-46ba-8ad1-0d41d72da79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_onf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10340065-6a87-455f-9d49-5292033ca89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_ohe = pd.concat([final_data,elite_onf_df],axis=1)\n",
    "final_data_ohe_test = pd.concat([final_pred_data,elite_onf_df_test],axis=1)\n",
    "final_data_ohe.to_parquet('../data/final_data_ohe', engine='fastparquet',compression='brotli')\n",
    "final_data_ohe_test.to_parquet('../data/final_data_ohe_test', engine='fastparquet',compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d97c9c-b483-4375-9fa4-bd48f30fc3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = final_data.target\n",
    "\n",
    "categorical_target_encoders_1 = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        (\n",
    "            \"target_enc\",\n",
    "            PWrapper(\n",
    "                TargetEncoder(cols=nominal + ordinal, drop_invariant=True, smoothing=0)\n",
    "            ),\n",
    "        ),\n",
    "        (\"woe_enc\", PWrapper(WOEEncoder(cols=nominal + ordinal, drop_invariant=True))),\n",
    "        (\n",
    "            \"jame_enc\",\n",
    "            PWrapper(JamesSteinEncoder(cols=nominal + ordinal, drop_invariant=True)),\n",
    "        ),\n",
    "    ],\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "categorical_target_encoders_2 = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        (\"summary_enc\", SummaryEncoder(cols=nominal + ordinal, drop_invariant=True)),\n",
    "        # ('woe_enc', PWrapper(WOEEncoder(cols=nominal+ordinal,drop_invariant=True))),\n",
    "        # ('backward_diff', BackwardDifferenceEncoder(cols=nominal+ordinal,drop_invariant=True)),\n",
    "        (\n",
    "            \"glmm_enc\",\n",
    "            PWrapper(GLMMEncoder(cols=nominal + ordinal, drop_invariant=True)),\n",
    "        ),\n",
    "    ],\n",
    "    n_jobs=-1,\n",
    ")\n",
    "# feature_selector = RFECV(estimator=LogisticRegression(max_iter=1000000,random_state=10), scoring='f1_macro', cv=RepeatedStratifiedKFold(n_repeats=2), step=6,n_jobs=-1)\n",
    "feature_selector = RFECV(\n",
    "    estimator=DecisionTreeClassifier(random_state=10),\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=RepeatedStratifiedKFold(n_repeats=2),\n",
    "    step=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "\n",
    "cat_feature_embedding_1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat_enc_1\", categorical_target_encoders_1, nominal + ordinal),\n",
    "    ],\n",
    "    sparse_threshold=0,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "cat_feature_embedding_2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat_enc_2\", categorical_target_encoders_2, nominal + ordinal),\n",
    "    ],\n",
    "    sparse_threshold=0,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# feature_selection = FeatureUnion(transformer_list=list(all_selectors.items()),n_jobs=-1)\n",
    "\n",
    "# f_gen_workflow_1 = make_pipeline(cat_feature_embedding_1,clone(feature_selector))\n",
    "# f_gen_workflow_2 = make_pipeline(cat_feature_embedding_2,clone(feature_selector))\n",
    "ohe_features = make_column_selector(pattern='one_hot_enc*')\n",
    "ohe_passthrough = make_column_transformer(('passthrough',ohe_features))\n",
    "cat_features_best = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('ohe', ohe_passthrough),\n",
    "        (\"set1_cat_encs\", cat_feature_embedding_1),\n",
    "        (\"set2_cat_encs\", cat_feature_embedding_2),\n",
    "    ],\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "cat_features_la_creme = make_pipeline(cat_features_best, feature_selector)\n",
    "cat_features_with_ohe = cat_features_la_creme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df44a8-5e54-46d8-8dcb-6df882f54bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../transformers/cat_features_with_ohe','wb') as fp:\n",
    "    pickle.dump(cat_features_with_ohe,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86475b04-c676-47c7-9c09-e654bef0eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with parallel_backend('loky'):\n",
    "    X_t = cat_features_with_ohe.fit_transform(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e8f314-ff71-469f-8e42-f124615ac6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_t = cat_features_with_ohe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf33d1-ac54-4ccf-980b-65bed5e3c9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "clf.load_model(fname='../models/xgb_clf_ohe_cat.json')\n",
    "clf2 = clone(clf)\n",
    "# clf2 = HistGradientBoostingClassifier(max_iter=300,categorical_features=cats,random_state=9)\n",
    "# clf2 = OneVsRestClassifier(clf2_, n_jobs=-1)\n",
    "X_train, X_test, y_train, y_test = train_test(final_data_ohe.drop(['target'],axis=1).loc[:,nominal+ordinal], target, test_size=0.2)\n",
    "# f1_scores = sklearn.metrics.f1_score(clf2.fit(X_train.to_numpy(),y_train,eval_set=[(X_test.to_numpy(),y_test)]).predict(X_test.to_numpy()), y_test, average='macro')\n",
    "with parallel_backend('loky'):\n",
    "    f1_scores = sklearn.metrics.f1_score(clf2.fit(X_train.to_numpy(),y_train).predict(X_test.to_numpy()), y_test, average='macro')\n",
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e66ca61-191f-4e5e-afad-b0e9e5e547c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_scores = sklearn.metrics.f1_score(tmp.fit(X_train,y_train).predict(X_test), y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83675ca-5f23-4e1d-bce7-4a816297dd09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09413d-9a76-4ff2-8b18-a98cf041f71f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "clf.load_model(fname='../models/xgb_clf_ohe_cat.json')\n",
    "clf2 = clone(clf)\n",
    "tmp = Pipeline([\n",
    "    ('categorical_features_with_ohe', cat_features_with_ohe),\n",
    "    ('estimator', clf2)\n",
    "], memory=Memory('../data/joblib_memory/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa8b90-43b9-4ebe-a992-7f14ca7e6b77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(tmp,'../models/clf_xgb_ohe_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d2ea5-6bac-44fa-8668-8253e7fa5781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test(final_data_ohe.drop(['target'],axis=1), target, test_size=0.2)\n",
    "with parallel_backend('loky'):\n",
    "    tmp.fit(X=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d953e-5a1d-4970-a19e-4a2cbc8258a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# workflow\n",
    "\"\"\"\n",
    "TEST\n",
    "\"\"\"\n",
    "target = final_data.target\n",
    "X_train, X_test, y_train, y_test = train_test(final_data, target, test_size=0.2)\n",
    "\n",
    "ohe_ref_train = X_train.index\n",
    "ohe_ref_test = X_test.index\n",
    "ohe_data_train = elite_onf_df.loc[ohe_ref_train,:].to_numpy()\n",
    "ohe_data_test = elite_onf_df.loc[ohe_ref_test,:].to_numpy()\n",
    "\n",
    "\n",
    "with parallel_backend(\"loky\"):\n",
    "    X_train_enc = cat_features_la_creme.fit_transform(X_train, y_train)\n",
    "    X_test_enc = cat_features_la_creme.transform(X_test)\n",
    "# cat_features_la_creme\n",
    "X_train_complete = np.c_[ohe_data_train,X_train_enc]\n",
    "X_test_complete = np.c_[ohe_data_test,X_test_enc]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60d260b9-08d0-4e09-964e-d24e02fb99fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Init signature:\n",
    "HistGradientBoostingClassifier(\n",
    "    loss='log_loss',\n",
    "    *,\n",
    "    learning_rate=0.1,\n",
    "    max_iter=100,\n",
    "    max_leaf_nodes=31,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=20,\n",
    "    l2_regularization=0.0,\n",
    "    max_bins=255,\n",
    "    categorical_features=None,\n",
    "    monotonic_cst=None,\n",
    "    warm_start=False,\n",
    "    early_stopping='auto',\n",
    "    scoring='loss',\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    tol=1e-07,\n",
    "    verbose=0,\n",
    "    random_state=None,\n",
    ")\n",
    "Docstring:     \n",
    "Histogram-based Gradient Boosting Classification Tree.\n",
    "\n",
    "This estimator is much faster than\n",
    ":class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\n",
    "for big datasets (n_samples >= 10 000).\n",
    "\n",
    "This estimator has native support for missing values (NaNs). During\n",
    "training, the tree grower learns at each split point whether samples\n",
    "with missing values should go to the left or right child, based on the\n",
    "potential gain. When predicting, samples with missing values are\n",
    "assigned to the left or right child consequently. If no missing values\n",
    "were encountered for a given feature during training, then samples with\n",
    "missing values are mapped to whichever child has the most samples.\n",
    "\n",
    "This implementation is inspired by\n",
    "`LightGBM <https://github.com/Microsoft/LightGBM>`_.\n",
    "\n",
    "Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n",
    "\n",
    ".. versionadded:: 0.21\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "loss : {'log_loss', 'auto', 'binary_crossentropy', 'categorical_crossentropy'},             default='log_loss'\n",
    "    The loss function to use in the boosting process.\n",
    "\n",
    "    For binary classification problems, 'log_loss' is also known as logistic loss,\n",
    "    binomial deviance or binary crossentropy. Internally, the model fits one tree\n",
    "    per boosting iteration and uses the logistic sigmoid function (expit) as\n",
    "    inverse link function to compute the predicted positive class probability.\n",
    "\n",
    "    For multiclass classification problems, 'log_loss' is also known as multinomial\n",
    "    deviance or categorical crossentropy. Internally, the model fits one tree per\n",
    "    boosting iteration and per class and uses the softmax function as inverse link\n",
    "    function to compute the predicted probabilities of the classes.\n",
    "\n",
    "    .. deprecated:: 1.1\n",
    "        The loss arguments 'auto', 'binary_crossentropy' and\n",
    "        'categorical_crossentropy' were deprecated in v1.1 and will be removed in\n",
    "        version 1.3. Use `loss='log_loss'` which is equivalent.\n",
    "\n",
    "learning_rate : float, default=0.1\n",
    "    The learning rate, also known as *shrinkage*. This is used as a\n",
    "    multiplicative factor for the leaves values. Use ``1`` for no\n",
    "    shrinkage.\n",
    "max_iter : int, default=100\n",
    "    The maximum number of iterations of the boosting process, i.e. the\n",
    "    maximum number of trees for binary classification. For multiclass\n",
    "    classification, `n_classes` trees per iteration are built.\n",
    "max_leaf_nodes : int or None, default=31\n",
    "    The maximum number of leaves for each tree. Must be strictly greater\n",
    "    than 1. If None, there is no maximum limit.\n",
    "max_depth : int or None, default=None\n",
    "    The maximum depth of each tree. The depth of a tree is the number of\n",
    "    edges to go from the root to the deepest leaf.\n",
    "    Depth isn't constrained by default.\n",
    "min_samples_leaf : int, default=20\n",
    "    The minimum number of samples per leaf. For small datasets with less\n",
    "    than a few hundred samples, it is recommended to lower this value\n",
    "    since only very shallow trees would be built.\n",
    "l2_regularization : float, default=0\n",
    "    The L2 regularization parameter. Use 0 for no regularization.\n",
    "max_bins : int, default=255\n",
    "    The maximum number of bins to use for non-missing values. Before\n",
    "    training, each feature of the input array `X` is binned into\n",
    "    integer-valued bins, which allows for a much faster training stage.\n",
    "    Features with a small number of unique values may use less than\n",
    "    ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n",
    "    is always reserved for missing values. Must be no larger than 255.\n",
    "categorical_features : array-like of {bool, int} of shape (n_features)             or shape (n_categorical_features,), default=None\n",
    "    Indicates the categorical features.\n",
    "\n",
    "    - None : no feature will be considered categorical.\n",
    "    - boolean array-like : boolean mask indicating categorical features.\n",
    "    - integer array-like : integer indices indicating categorical\n",
    "      features.\n",
    "\n",
    "    For each categorical feature, there must be at most `max_bins` unique\n",
    "    categories, and each categorical value must be in [0, max_bins -1].\n",
    "\n",
    "    Read more in the :ref:`User Guide <categorical_support_gbdt>`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "monotonic_cst : array-like of int of shape (n_features), default=None\n",
    "    Indicates the monotonic constraint to enforce on each feature. -1, 1\n",
    "    and 0 respectively correspond to a negative constraint, positive\n",
    "    constraint and no constraint. Read more in the :ref:`User Guide\n",
    "    <monotonic_cst_gbdt>`.\n",
    "\n",
    "    .. versionadded:: 0.23\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to ``True``, reuse the solution of the previous call to fit\n",
    "    and add more estimators to the ensemble. For results to be valid, the\n",
    "    estimator should be re-trained on the same data only.\n",
    "    See :term:`the Glossary <warm_start>`.\n",
    "early_stopping : 'auto' or bool, default='auto'\n",
    "    If 'auto', early stopping is enabled if the sample size is larger than\n",
    "    10000. If True, early stopping is enabled, otherwise early stopping is\n",
    "    disabled.\n",
    "\n",
    "    .. versionadded:: 0.23\n",
    "\n",
    "scoring : str or callable or None, default='loss'\n",
    "    Scoring parameter to use for early stopping. It can be a single\n",
    "    string (see :ref:`scoring_parameter`) or a callable (see\n",
    "    :ref:`scoring`). If None, the estimator's default scorer\n",
    "    is used. If ``scoring='loss'``, early stopping is checked\n",
    "    w.r.t the loss value. Only used if early stopping is performed.\n",
    "validation_fraction : int or float or None, default=0.1\n",
    "    Proportion (or absolute size) of training data to set aside as\n",
    "    validation data for early stopping. If None, early stopping is done on\n",
    "    the training data. Only used if early stopping is performed.\n",
    "n_iter_no_change : int, default=10\n",
    "    Used to determine when to \"early stop\". The fitting process is\n",
    "    stopped when none of the last ``n_iter_no_change`` scores are better\n",
    "    than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n",
    "    tolerance. Only used if early stopping is performed.\n",
    "tol : float, default=1e-7\n",
    "    The absolute tolerance to use when comparing scores. The higher the\n",
    "    tolerance, the more likely we are to early stop: higher tolerance\n",
    "    means that it will be harder for subsequent iterations to be\n",
    "    considered an improvement upon the reference score.\n",
    "verbose : int, default=0\n",
    "    The verbosity level. If not zero, print some information about the\n",
    "    fitting process.\n",
    "random_state : int, RandomState instance or None, default=None\n",
    "    Pseudo-random number generator to control the subsampling in the\n",
    "    binning process, and the train/validation data split if early stopping\n",
    "    is enabled.\n",
    "    Pass an int for reproducible output across multiple function calls.\n",
    "    See :term:`Glossary <random_state>`.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "classes_ : array, shape = (n_classes,)\n",
    "    Class labels.\n",
    "do_early_stopping_ : bool\n",
    "    Indicates whether early stopping is used during training.\n",
    "n_iter_ : int\n",
    "    The number of iterations as selected by early stopping, depending on\n",
    "    the `early_stopping` parameter. Otherwise it corresponds to max_iter.\n",
    "n_trees_per_iteration_ : int\n",
    "    The number of tree that are built at each iteration. This is equal to 1\n",
    "    for binary classification, and to ``n_classes`` for multiclass\n",
    "    classification.\n",
    "train_score_ : ndarray, shape (n_iter_+1,)\n",
    "    The scores at each iteration on the training data. The first entry\n",
    "    is the score of the ensemble before the first iteration. Scores are\n",
    "    computed according to the ``scoring`` parameter. If ``scoring`` is\n",
    "    not 'loss', scores are computed on a subset of at most 10 000\n",
    "    samples. Empty if no early stopping.\n",
    "validation_score_ : ndarray, shape (n_iter_+1,)\n",
    "    The scores at each iteration on the held-out validation data. The\n",
    "    first entry is the score of the ensemble before the first iteration.\n",
    "    Scores are computed according to the ``scoring`` parameter. Empty if\n",
    "    no early stopping or if ``validation_fraction`` is None.\n",
    "is_categorical_ : ndarray, shape (n_features, ) or None\n",
    "    Boolean mask for the categorical features. ``None`` if there are no\n",
    "    categorical features.\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "See Also\n",
    "--------\n",
    "GradientBoostingClassifier : Exact gradient boosting method that does not\n",
    "    scale as good on datasets with a large number of samples.\n",
    "sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
    "RandomForestClassifier : A meta-estimator that fits a number of decision\n",
    "    tree classifiers on various sub-samples of the dataset and uses\n",
    "    averaging to improve the predictive accuracy and control over-fitting.\n",
    "AdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n",
    "    on the original dataset and then fits additional copies of the\n",
    "    classifier on the same dataset where the weights of incorrectly\n",
    "    classified instances are adjusted such that subsequent classifiers\n",
    "    focus more on difficult cases.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.ensemble import HistGradientBoostingClassifier\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> X, y = load_iris(return_X_y=True)\n",
    ">>> clf = HistGradientBoostingClassifier().fit(X, y)\n",
    ">>> clf.score(X, y)\n",
    "1.0\n",
    "File:           ~/Programs/autoskl/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\n",
    "Type:           ABCMeta\n",
    "Subclasses:     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "86045acf-9f0f-4bad-baaa-cb144f6864cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-23 19:59:30,399]\u001b[0m A new study created in Redis with name: Hist.1\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d1cb9c455744f7ba3446a3455a07ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-23 19:59:41,307]\u001b[0m Trial 0 finished with value: 0.6041831209317647 and parameters: {'early_stopping': 'TRUE', 'l2_regularization': 0.7771279313634673, 'learning_rate': 0.7482421650830148, 'max_depth': 20, 'max_iter': 47, 'max_leaf_nodes': 173, 'min_samples_leaf': 133}. Best is trial 0 with value: 0.6041831209317647.\u001b[0m\n",
      "\u001b[32m[I 2022-07-23 19:59:46,301]\u001b[0m Trial 1 finished with value: 0.6258272796956207 and parameters: {'early_stopping': 'FALSE', 'l2_regularization': 0.7002553503904348, 'learning_rate': 0.7935439307149024, 'max_depth': 4, 'max_iter': 478, 'max_leaf_nodes': 243, 'min_samples_leaf': 222}. Best is trial 1 with value: 0.6258272796956207.\u001b[0m\n",
      "\u001b[32m[I 2022-07-23 19:59:50,691]\u001b[0m Trial 2 finished with value: 0.6538355067439934 and parameters: {'early_stopping': 'TRUE', 'l2_regularization': 0.8749972627149966, 'learning_rate': 0.09584672171023134, 'max_depth': 9, 'max_iter': 36, 'max_leaf_nodes': 149, 'min_samples_leaf': 99}. Best is trial 2 with value: 0.6538355067439934.\u001b[0m\n",
      "\u001b[32m[I 2022-07-23 19:59:58,503]\u001b[0m Trial 3 finished with value: 0.598870309024144 and parameters: {'early_stopping': 'FALSE', 'l2_regularization': 0.32335510457733085, 'learning_rate': 0.821914161843866, 'max_depth': 12, 'max_iter': 113, 'max_leaf_nodes': 263, 'min_samples_leaf': 374}. Best is trial 2 with value: 0.6538355067439934.\u001b[0m\n",
      "\u001b[32m[I 2022-07-23 20:00:00,995]\u001b[0m Trial 4 finished with value: 0.5878771730784659 and parameters: {'early_stopping': 'TRUE', 'l2_regularization': 0.779722365183757, 'learning_rate': 0.7474895503745475, 'max_depth': 24, 'max_iter': 199, 'max_leaf_nodes': 203, 'min_samples_leaf': 314}. Best is trial 2 with value: 0.6538355067439934.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import parallel_backend\n",
    "import pickle\n",
    "import joblib\n",
    "from sklearn.cluster import *\n",
    "from sklearn.compose import *\n",
    "from sklearn.cross_decomposition import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.gaussian_process import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.multioutput import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.utils import *\n",
    "from sklearn.dummy import *\n",
    "from sklearn.semi_supervised import *\n",
    "from sklearn.discriminant_analysis import *\n",
    "from REDIS_CONFIG import REDIS_URL\n",
    "\n",
    "OPTUNA_DB = REDIS_URL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_test(X, y, test_size):\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, random_state=10, test_size=test_size, stratify=y\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "final_data_ohe = pd.read_parquet('../data/final_data_ohe')\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test(final_data_ohe, target, test_size=0.2)\n",
    "\n",
    "def objective(trial:optuna.trial.Trial):\n",
    "    categories = [True]*138\n",
    "    e_stop = {'TRUE': True, \"FALSE\":False}\n",
    "    hist_params = {'categorical_features': categories,\n",
    "                 'early_stopping': e_stop[trial.suggest_categorical(\"early_stopping\",[\"TRUE\",\"FALSE\"])],\n",
    "                     'l2_regularization': trial.suggest_float(\"l2_regularization\", 0.0, 1.0),\n",
    "                     'learning_rate': trial.suggest_float(\"learning_rate\", 0.0, 1.0),\n",
    "                     'loss': 'log_loss',\n",
    "                     'max_bins': 255,\n",
    "                     'max_depth': trial.suggest_int('max_depth',1,40),\n",
    "                     'max_iter': trial.suggest_int('max_iter',1,500),\n",
    "                     'max_leaf_nodes': trial.suggest_int('max_leaf_nodes',5,400),\n",
    "                     'min_samples_leaf': trial.suggest_int('min_samples_leaf',2,400),\n",
    "                     'monotonic_cst': None,\n",
    "                     'n_iter_no_change': 10,\n",
    "                     'random_state': 10,\n",
    "                     'scoring': 'f1_macro',\n",
    "                     'tol': 1e-07,\n",
    "                     'validation_fraction': trial.suggest_float(\"learning_rate\", 0.1, 0.3),\n",
    "                     'verbose': 0,\n",
    "                     'warm_start': False}\n",
    "    c_select = make_column_selector(pattern='one_hot_enc*|ordinal*|nominal*')\n",
    "    clf = HistGradientBoostingClassifier(**hist_params)\n",
    "    ct = make_column_transformer((('passthrough',c_select)),sparse_threshold=0)\n",
    "    wf = make_pipeline(ct,clf)\n",
    "    with parallel_backend('loky'):\n",
    "        y_pred = wf.fit(X_train,y_train).predict(X_test)\n",
    "        score = sklearn.metrics.f1_score(y_test,y_pred,average='macro')\n",
    "    return score\n",
    "\n",
    "\n",
    "# objective()\n",
    "study = optuna.create_study(\n",
    "        study_name=\"Hist.1\",\n",
    "        sampler=optuna.samplers.TPESampler(\n",
    "            warn_independent_sampling=False,\n",
    "        ),\n",
    "        storage=OPTUNA_DB,\n",
    "        direction=\"maximize\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "# with parallel_backend(\"loky\"):\n",
    "study.optimize(\n",
    "    objective,\n",
    "    show_progress_bar=True,\n",
    "    gc_after_trial=True,\n",
    "    # n_jobs=2,\n",
    "    n_trials=5,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d8e4bae3-d1f4-41e7-bd3c-f648c49433eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3796, 138)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_select = make_column_selector(pattern='one_hot_enc*|ordinal*|nominal*')\n",
    "ct = make_column_transformer((('passthrough',c_select)),sparse_threshold=0,)\n",
    "ct.fit_transform(final_data_ohe).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7e280286-7f7f-4021-9be5-9badff80d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = HistGradientBoostingClassifier(random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6e393450-826f-4e8f-bfaa-facafbff6e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'categorical_features': None,\n",
       " 'early_stopping': 'auto',\n",
       " 'l2_regularization': 0.0,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'log_loss',\n",
       " 'max_bins': 255,\n",
       " 'max_depth': None,\n",
       " 'max_iter': 100,\n",
       " 'max_leaf_nodes': 31,\n",
       " 'min_samples_leaf': 20,\n",
       " 'monotonic_cst': None,\n",
       " 'n_iter_no_change': 10,\n",
       " 'random_state': 10,\n",
       " 'scoring': 'loss',\n",
       " 'tol': 1e-07,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c2401-29a5-47c2-a201-0962dd5d71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_params = {'categorical_features': None,\n",
    " 'early_stopping': 'auto',\n",
    " 'l2_regularization': 0.0,\n",
    " 'learning_rate': 0.1,\n",
    " 'loss': 'log_loss',\n",
    " 'max_bins': 255,\n",
    " 'max_depth': None,\n",
    " 'max_iter': 100,\n",
    " 'max_leaf_nodes': 31,\n",
    " 'min_samples_leaf': 20,\n",
    " 'monotonic_cst': None,\n",
    " 'n_iter_no_change': 10,\n",
    " 'random_state': 10,\n",
    " 'scoring': 'loss',\n",
    " 'tol': 1e-07,\n",
    " 'validation_fraction': 0.1,\n",
    " 'verbose': 0,\n",
    " 'warm_start': False}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "df563976-4b8c-4921-ab62-a06d324bc0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_col_selector = make_column_selector(pattern=\"one_hot*|nominal*|ordinal*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1309d242-8f65-48a0-b768-4554133647f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(('passthrough', category_col_selector),sparse_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e417aae5-53a4-4ada-b87d-832f16366c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3036, 138)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ct.fit_transform(X_train).shape\n",
    "categories = [True]*138\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ccf7f-a357-4899-bf63-8ecc9abf035f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autoskl)",
   "language": "python",
   "name": "autoskl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0a6d5a8652dd48f2b5f5ca45493bd983": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1d71402aae274952889e67dd98dbcf07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d336d30e5a20439f8c2c111fbc510e11",
       "style": "IPY_MODEL_0a6d5a8652dd48f2b5f5ca45493bd983",
       "value": "100%"
      }
     },
     "32a905382643491eb9bbc553a970eb89": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "46d1cb9c455744f7ba3446a3455a07ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1d71402aae274952889e67dd98dbcf07",
        "IPY_MODEL_6c2cd4c431f44272a11c3d9143acf5ae",
        "IPY_MODEL_784224d49d9d49a681453fef9b7c1c25"
       ],
       "layout": "IPY_MODEL_f6a32295129b4e43af0bb636e76ae4d7"
      }
     },
     "507378aa50704558acf4292249cc1446": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6c2cd4c431f44272a11c3d9143acf5ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_fe8536f0ee174908b203d0b11493a2fb",
       "max": 5,
       "style": "IPY_MODEL_af15a74325d9496eabc52c33d24f3855",
       "value": 5
      }
     },
     "784224d49d9d49a681453fef9b7c1c25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_32a905382643491eb9bbc553a970eb89",
       "style": "IPY_MODEL_507378aa50704558acf4292249cc1446",
       "value": " 5/5 [00:30&lt;00:00,  5.21s/it]"
      }
     },
     "af15a74325d9496eabc52c33d24f3855": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d336d30e5a20439f8c2c111fbc510e11": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f6a32295129b4e43af0bb636e76ae4d7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fe8536f0ee174908b203d0b11493a2fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
