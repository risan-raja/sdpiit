{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b5de35-e0cd-43aa-8932-7a32f4c03429",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import widen_notebook\n",
    "from mysetup import NotebookFinder\n",
    "import sys\n",
    "\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "\n",
    "from setup_transform import *\n",
    "     \n",
    "%matplotlib inline\n",
    "cl_weight = sklearn.utils.compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=[0, 1, 2], y=y\n",
    ")\n",
    "CLASS_WEIGHTS = {i: cl_weight[i] for i in range(3)}\n",
    "def_cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=5, random_state=42)\n",
    "lgr_params = dict(\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    fit_intercept=False,\n",
    "    multi_class=\"ovr\",\n",
    "    max_iter=2000000,\n",
    "    random_state=42,\n",
    "    n_jobs=24,\n",
    "    #     penalty=\"elasticnet\",\n",
    "    cv=def_cv,\n",
    "    scoring=\"f1_macro\",\n",
    "    solver=\"lbfgs\",\n",
    "    Cs=100,\n",
    "    #     l1_ratios=np.linspace(0, 1, endpoint=False, num=100),\n",
    ")\n",
    "from sklearnex import unpatch_sklearn\n",
    "\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "discrete = nominal + discrete_ordinal + discrete_binary\n",
    "X_master = pd.concat(\n",
    "    [raw_data.loc[:, raw_data_eval.columns], raw_data_eval], ignore_index=True, axis=0\n",
    ")\n",
    "class_priors = (raw_data.target.value_counts() / 3796).to_numpy()\n",
    "raw_data[discrete] = raw_data[discrete].astype(np.uint32)\n",
    "X_master[discrete] = X_master[discrete].astype(np.uint32)\n",
    "numeric = list(\n",
    "    np.setdiff1d(raw_data_eval.columns, discrete_binary + discrete_ordinal + nominal+['target'])\n",
    ")\n",
    "for c in raw_data_eval.columns:\n",
    "    if c not in numeric:\n",
    "        raw_data[c] = raw_data[c].astype(np.uint32)\n",
    "        raw_data_eval[c] = raw_data_eval[c].astype(np.uint32)\n",
    "raw_data['target'] = raw_data['target'].astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78fd6db-b1a9-404a-8cd8-36e2fcf8a8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import has_fit_parameter\n",
    "\n",
    "# has_fit_parameter(clf, \"warm_start\")\n",
    "clf_list = sklearn.utils.all_estimators(type_filter=\"classifier\")\n",
    "props = {\"warm_start\": [], \"partial_fit\": []}\n",
    "errors = []\n",
    "for name, clf in clf_list:\n",
    "    try:\n",
    "        if \"warm_start\" in clf().get_params():\n",
    "            props[\"warm_start\"].append(clf)\n",
    "        try:\n",
    "            clf().partial_fit(\n",
    "                raw_data[raw_data_eval.columns].abs(),\n",
    "                raw_data.target,\n",
    "                classes=[0, 1, 2],\n",
    "            )\n",
    "            props[\"partial_fit\"].append(clf)\n",
    "        except AttributeError:\n",
    "            errors.append(name)\n",
    "\n",
    "    except TypeError:\n",
    "        errors.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3f589-23af-46aa-a275-4eacb4db0a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ffa0cdc-9808-4c19-b2de-db42ac8d5896",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T13:06:46.696021Z",
     "iopub.status.busy": "2022-09-12T13:06:46.695641Z",
     "iopub.status.idle": "2022-09-12T13:06:46.732945Z",
     "shell.execute_reply": "2022-09-12T13:06:46.732005Z",
     "shell.execute_reply.started": "2022-09-12T13:06:46.695971Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseNEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     probit_x\u001b[38;5;241m.\u001b[39mapply(transform_ordinal_to_logit, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m probit_x[discrete_ordinal]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m---> 25\u001b[0m base_n \u001b[38;5;241m=\u001b[39m \u001b[43mBaseNEncoder\u001b[49m(cols\u001b[38;5;241m=\u001b[39mnominal,handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     26\u001b[0m numeric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m     27\u001b[0m     np\u001b[38;5;241m.\u001b[39msetdiff1d(raw_data_eval\u001b[38;5;241m.\u001b[39mcolumns, discrete_binary \u001b[38;5;241m+\u001b[39m discrete_ordinal \u001b[38;5;241m+\u001b[39m nominal)\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m tree_ready \u001b[38;5;241m=\u001b[39m make_column_transformer(\n\u001b[1;32m     30\u001b[0m     (base_n, nominal),\n\u001b[1;32m     31\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseNEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "def get_logit_ordinal(X):\n",
    "    probit = {c: {} for c in discrete_ordinal}\n",
    "\n",
    "    for c in discrete_ordinal:\n",
    "        vc = X_master[c].value_counts()\n",
    "        N = X_master.shape[0]\n",
    "        vc = vc / N\n",
    "        probit_value = {val: 0 for val in X_master[c].unique()}\n",
    "        for val in X_master[c].unique():\n",
    "            probit_value[val] = np.log1p(\n",
    "                (vc[vc.index <= val].sum() )/ (vc[vc.index > val].sum() + 1e-05)\n",
    "            )\n",
    "        probit[c] = probit_value\n",
    "\n",
    "    def transform_ordinal_to_logit(row):\n",
    "        for c in discrete_ordinal:\n",
    "            row[c] = probit[c][row[c]]\n",
    "        return row\n",
    "\n",
    "    probit_x = pd.DataFrame(X, columns=discrete_ordinal)\n",
    "    probit_x.apply(transform_ordinal_to_logit, axis=1)\n",
    "    return probit_x[discrete_ordinal].to_numpy()\n",
    "\n",
    "\n",
    "base_n = BaseNEncoder(cols=nominal,handle_unknown=0)\n",
    "numeric = list(\n",
    "    np.setdiff1d(raw_data_eval.columns, discrete_binary + discrete_ordinal + nominal)\n",
    ")\n",
    "tree_ready = make_column_transformer(\n",
    "    (base_n, nominal),\n",
    "    (\n",
    "        Normalizer(),\n",
    "        numeric,\n",
    "    ),\n",
    "    (\"passthrough\", discrete_binary + discrete_ordinal),\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "\n",
    "linear_ready = make_column_transformer(\n",
    "    #     (OneHotEncoder(sparse=False),nominal),\n",
    "    (BaseNEncoder(cols=nominal,handle_unknown=0), nominal),\n",
    "    (\n",
    "        FunctionTransformer(get_logit_ordinal),\n",
    "        discrete_ordinal,\n",
    "    ),\n",
    "    (StandardScaler(), numeric),\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False,)\n",
    "ohe.fit(X_master[nominal])\n",
    "nom_categories = ohe.categories_\n",
    "bayesian_prep = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (OneHotEncoder(sparse=False,categories=nom_categories), nominal),\n",
    "        (\"passthrough\", discrete_binary + discrete_ordinal),\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0,\n",
    "    ),\n",
    "    FunctionTransformer(np.abs)\n",
    "    \n",
    ")\n",
    "# bayesian_prep.fit_transform(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171932c2-c7b5-4e21-8012-481417b0c1ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = raw_data[raw_data_eval.columns]\n",
    "y = raw_data.target\n",
    "X_eval = raw_data_eval\n",
    "X_train, X_test, y_train, y_test = gen_train_test(X, y, test_size=0.3)\n",
    "c_msk = np.array([True] * 44 + [False] * 15 + [True] * 22)\n",
    "min_c = pd.DataFrame(bayesian_prep.fit_transform(X)).max().astype(np.uint32).to_list()\n",
    "min_c = [mc+1 for mc in min_c]\n",
    "c_msk = c_msk.reshape(\n",
    "    81,\n",
    ")\n",
    "svc = SVC(\n",
    "    probability=True,\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    random_state=42,\n",
    "    tol=1e-06,\n",
    "    kernel=\"rbf\",\n",
    "    gamma=\"scale\",\n",
    "    break_ties=True,\n",
    ")\n",
    "sgd = SGDClassifier(\n",
    "    warm_start=True,\n",
    "    average=True,\n",
    "    random_state=42,\n",
    "    fit_intercept=False,\n",
    "    n_iter_no_change=50,\n",
    "    loss=\"squared_hinge\",\n",
    "    max_iter=10000,class_weight=CLASS_WEIGHTS\n",
    ")\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    warm_start=True,\n",
    "    categorical_features=c_msk,\n",
    "    random_state=42,\n",
    "    scoring=\"f1_macro\",\n",
    "    max_iter=200,\n",
    "    early_stopping=\"auto\",\n",
    "    n_iter_no_change=50,\n",
    "\n",
    ")\n",
    "rfc = RandomForestClassifier(\n",
    "    warm_start=True,\n",
    "    min_impurity_decrease=0.00055,\n",
    "    n_jobs=24,\n",
    "    random_state=42,\n",
    "    max_features=None,\n",
    "    class_weight=CLASS_WEIGHTS\n",
    ")\n",
    "efc = ExtraTreesClassifier(\n",
    "    warm_start=True,\n",
    "    min_impurity_decrease=0.00055,\n",
    "    n_jobs=24,\n",
    "    random_state=42,\n",
    "    max_features=None,\n",
    "    class_weight=CLASS_WEIGHTS\n",
    ")\n",
    "y_evals = []\n",
    "ALPHA = 1.0e-10\n",
    "cnb = CategoricalNB(alpha=ALPHA, fit_prior=True, min_categories=min_c)\n",
    "conb = ComplementNB(alpha=ALPHA, fit_prior=True, norm=False)\n",
    "benb = BernoulliNB(binarize=False, alpha=ALPHA, fit_prior=True)\n",
    "munb = MultinomialNB(alpha=ALPHA, fit_prior=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for bayesian_learner in [conb, benb, munb]:\n",
    "    b_clfs.append((bayesian_learner.__class__.__name__, bayesian_learner))\n",
    "\n",
    "for tree_learner in [hgb, rfc, efc]:\n",
    "    t_clfs.append((tree_learner.__class__.__name__, tree_learner))\n",
    "\n",
    "for linear_learner in [svc, sgd]:\n",
    "    l_clfs.append((linear_learner.__class__.__name__, linear_learner))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for clf_name,clf in b_clfs:\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(bayesian_model_data, y, test_size=0.3)\n",
    "    print(clf_name)\n",
    "    y_pred_base = clf.fit(X_train, y_train).predict(X_test)\n",
    "    print(classification_report(y_test, y_pred_base))\n",
    "    print(\":\"*80)\n",
    "    \n",
    "for clf_name,clf in t_clfs:\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(tree_model_data, y, test_size=0.3)\n",
    "    print(clf_name)\n",
    "    y_pred_base = clf.fit(X_train, y_train).predict(X_test)\n",
    "    print(classification_report(y_test, y_pred_base))\n",
    "    print(\":\"*80)\n",
    "    \n",
    "for clf_name,clf in l_clfs:\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(linear_model_data, y, test_size=0.3)\n",
    "    print(clf_name)\n",
    "    y_pred_base = clf.fit(X_train, y_train).predict(X_test)\n",
    "    print(classification_report(y_test, y_pred_base))\n",
    "    print(\":\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd61cd-03cd-480c-a84d-f52e823e20fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(\n",
    "    warm_start=True,\n",
    "    average=True,\n",
    "    random_state=42,\n",
    "    fit_intercept=False,\n",
    "    n_iter_no_change=50,\n",
    "    loss=\"log_loss\",\n",
    "    max_iter=10000,class_weight=CLASS_WEIGHTS\n",
    ")\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    warm_start=True,\n",
    "    categorical_features=c_msk,\n",
    "    random_state=42,\n",
    "    scoring=\"f1_macro\",\n",
    "    max_iter=200,\n",
    "    early_stopping=\"auto\",\n",
    "    n_iter_no_change=50,\n",
    "\n",
    ")\n",
    "rfc = RandomForestClassifier(\n",
    "    warm_start=True,\n",
    "    min_impurity_decrease=0.00055,\n",
    "    n_jobs=24,\n",
    "    random_state=42,\n",
    "    max_features=None,\n",
    "    class_weight=CLASS_WEIGHTS\n",
    ")\n",
    "efc = ExtraTreesClassifier(\n",
    "    warm_start=True,\n",
    "    min_impurity_decrease=0.00055,\n",
    "    n_jobs=24,\n",
    "    random_state=42,\n",
    "    max_features=None,\n",
    "    class_weight=CLASS_WEIGHTS\n",
    ")\n",
    "svc_ = SVC(\n",
    "    probability=True,\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    random_state=42,\n",
    "    tol=1e-06,\n",
    "    kernel=\"rbf\",\n",
    "    gamma=\"scale\",\n",
    "    break_ties=True,\n",
    ")\n",
    "\n",
    "svc =BaggingClassifier( base_estimator=svc_,n_estimators=10,\n",
    "        random_state=42,\n",
    "        n_jobs=24,\n",
    "        bootstrap=False,\n",
    "        warm_start=True, )\n",
    "cnb = CategoricalNB(alpha=ALPHA, fit_prior=True, min_categories=min_c)\n",
    "conb = ComplementNB(alpha=ALPHA, fit_prior=True, norm=False)\n",
    "benb = BernoulliNB(binarize=False, alpha=ALPHA, fit_prior=True)\n",
    "munb = MultinomialNB(alpha=ALPHA, fit_prior=True)\n",
    "def print_clf_perf(clf,X_test,y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\":\"*80)\n",
    "vc_tree = VotingClassifier(estimators = [\n",
    "    ('1',hgb),\n",
    "    ('3',rfc),\n",
    "    ('4',efc),\n",
    "],\n",
    "                           voting='soft' ,\n",
    "                           n_jobs=24,\n",
    ")\n",
    "\n",
    "vc_ll = VotingClassifier(estimators=[\n",
    "    ('2',sgd),\n",
    "    ('9',svc),\n",
    "    \n",
    "] ,voting='soft' ,\n",
    "                           n_jobs=24,)\n",
    "fe = LogisticRegressionCV(**lgr_params)\n",
    "fe = MLPClassifier(warm_start=True,random_state=42)\n",
    "\n",
    "SC_tree = StackingClassifier(estimators = [\n",
    "    ('1',hgb),\n",
    "    ('3',rfc),\n",
    "    ('4',efc),\n",
    "],\n",
    "                           cv='prefit',\n",
    "                             final_estimator=fe,\n",
    "                           n_jobs=24,\n",
    ")\n",
    "SC_nb = StackingClassifier(estimators=[\n",
    "    \n",
    "#     ('5',cnb),\n",
    "#     ('6',benb),\n",
    "    ('7',munb),\n",
    "    ('8',conb),\n",
    "], \n",
    "                           cv='prefit',\n",
    "                           final_estimator=fe,\n",
    "                           n_jobs=24,\n",
    "                        \n",
    "                        )\n",
    "SC_ll = StackingClassifier(estimators=[\n",
    "    ('2',sgd),\n",
    "    ('9',svc),\n",
    "    \n",
    "] , \n",
    "                           cv='prefit',\n",
    "                           final_estimator=fe,\n",
    "                           n_jobs=24,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c66ea3f-b5d3-4844-b458-e8d88749b140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with parallel_backend('threading',n_jobs=24):  \n",
    "    X_train, X_test, y_train, y_test = gen_train_test(bayesian_model_data, y, test_size=0.3)\n",
    "#     y_pred_base = clf.fit(X_train, y_train).predict(X_test)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=10)\n",
    "    for train_index, test_index in cv.split(X_train, y_train):\n",
    "        X_train_, X_test_ = bayesian_model_data[train_index, :], bayesian_model_data[test_index, :]\n",
    "        y_train_, y_test_ = y.loc[train_index], y.loc[test_index]\n",
    "        #     plug.fit(X_test_, y_test_)\n",
    "        cnb.partial_fit(X_train_, y_train_, classes=[0,1,2])\n",
    "        cnb.partial_fit(X_test_, y_test_, classes=[0,1,2])\n",
    "        conb.partial_fit(X_train_, y_train_, classes=[0,1,2])\n",
    "        conb.partial_fit(X_test_, y_test_, classes=[0,1,2])\n",
    "        benb.partial_fit(X_train_, y_train_, classes=[0,1,2])\n",
    "        benb.partial_fit(X_test_, y_test_, classes=[0,1,2])\n",
    "        munb.partial_fit(X_train_, y_train_, classes=[0,1,2])\n",
    "        munb.partial_fit(X_test_, y_test_, classes=[0,1,2])\n",
    "    print_clf_perf(cnb,X_test,y_test)\n",
    "    print_clf_perf(conb,X_test,y_test)\n",
    "    print_clf_perf(benb,X_test,y_test)\n",
    "    print_clf_perf(munb,X_test,y_test)\n",
    "    \n",
    "\n",
    "#     for clf_name,clf in t_clfs:\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(tree_model_data, y, test_size=0.3)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=10)\n",
    "    for train_index, test_index in cv.split(X_train, y_train):\n",
    "        X_train_, X_test_ = tree_model_data[train_index, :], tree_model_data[test_index, :]\n",
    "        y_train_, y_test_ = y.loc[train_index], y.loc[test_index]\n",
    "        #     plug.fit(X_test_, y_test_)\n",
    "        hgb.fit(X_train_, y_train_)\n",
    "        hgb.fit(X_test_, y_test_)\n",
    "        rfc.fit(X_train_, y_train_)\n",
    "        rfc.fit(X_test_, y_test_)\n",
    "        efc.fit(X_train_, y_train_)\n",
    "        efc.fit(X_test_, y_test_)\n",
    "    print_clf_perf(hgb,X_test,y_test)\n",
    "    print_clf_perf(rfc,X_test,y_test)\n",
    "    print_clf_perf(efc,X_test,y_test)\n",
    "    \n",
    "\n",
    "#     for clf_name,clf in l_clfs:\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(linear_model_data, y, test_size=0.3)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=10)\n",
    "    for train_index, test_index in cv.split(X_train, y_train):\n",
    "        X_train_, X_test_ = linear_model_data[train_index, :], linear_model_data[test_index, :]\n",
    "        y_train_, y_test_ = y.loc[train_index], y.loc[test_index]\n",
    "        #     plug.fit(X_test_, y_test_)\n",
    "        sgd.fit(X_train_, y_train_)\n",
    "        sgd.fit(X_test_, y_test_)\n",
    "        svc.fit(X_train_, y_train_)\n",
    "        svc.fit(X_test_, y_test_)\n",
    "    print_clf_perf(sgd,X_test,y_test)\n",
    "    print_clf_perf(svc,X_test,y_test)\n",
    "[hgb,sgd,rfc,efc,benb,munb,conb,cnb,hgb] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c0f6e7-e940-4e2a-80c4-9d427f96f1b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Voter_Training\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_train_test(bayesian_model_data, y, test_size=0.3)\n",
    "print(\"VC_NB\")\n",
    "y_pred_base = vc_nb.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "print(\":\"*80)\n",
    "X_train, X_test, y_train, y_test = gen_train_test(linear_model_data, y, test_size=0.3)\n",
    "print(\"LL\")\n",
    "y_pred_base = vc_ll.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "print(\":\"*80)\n",
    "X_train, X_test, y_train, y_test = gen_train_test(tree_model_data, y, test_size=0.3)\n",
    "print(\"vc_tree\")\n",
    "y_pred_base = vc_tree.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "print(\":\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b683de4-51fe-447e-8b60-8c87b48df1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877f145-f093-48c9-8c35-58fd289357dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stack Training\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_train_test(bayesian_model_data, y, test_size=0.3)\n",
    "print(\"VC_NB\")\n",
    "y_pred_base = SC_nb.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "print(\":\"*80)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_train_test(linear_model_data, y, test_size=0.3)\n",
    "print(\"LL\")\n",
    "y_pred_base = SC_ll.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "print(\":\"*80)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_train_test(tree_model_data, y, test_size=0.3)\n",
    "print(\"vc_tree\")\n",
    "y_pred_base = SC_tree.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "print(\":\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6745c7-c4dc-41d3-b0a6-92b65ada56f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with parallel_backend('threading',n_jobs=24):  \n",
    "    X_train, X_test, y_train, y_test = gen_train_test(bayesian_model_data, y, test_size=0.3)\n",
    "#     y_pred_base = clf.fit(X_train, y_train).predict(X_test)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=10)\n",
    "    for train_index, test_index in cv.split(X_train, y_train):\n",
    "        X_train_, X_test_ = bayesian_model_data[train_index, :], bayesian_model_data[test_index, :]\n",
    "        y_train_, y_test_ = y.loc[train_index], y.loc[test_index]\n",
    "        #     plug.fit(X_test_, y_test_)\n",
    "        cnb.partial_fit(X_train_, y_train_, classes=[0,1,2])\n",
    "        cnb.partial_fit(X_test_, y_test_, classes=[0,1,2])\n",
    "        conb.partial_fit(X_train_, y_train_, classes=[0,1,2])\n",
    "        conb.partial_fit(X_test_, y_test_, classes=[0,1,2])\n",
    "        benb.partial_fit(X_train_, y_train_, classes=[0,1,2])\n",
    "        benb.partial_fit(X_test_, y_test_, classes=[0,1,2])\n",
    "        munb.partial_fit(X_train_, y_train_, classes=[0,1,2])\n",
    "        munb.partial_fit(X_test_, y_test_, classes=[0,1,2])\n",
    "    print_clf_perf(cnb,X_test,y_test)\n",
    "    print_clf_perf(conb,X_test,y_test)\n",
    "    print_clf_perf(benb,X_test,y_test)\n",
    "    print_clf_perf(munb,X_test,y_test)\n",
    "    \n",
    "\n",
    "#     for clf_name,clf in t_clfs:\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(tree_model_data, y, test_size=0.3)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=10)\n",
    "    for train_index, test_index in cv.split(X_train, y_train):\n",
    "        X_train_, X_test_ = tree_model_data[train_index, :], tree_model_data[test_index, :]\n",
    "        y_train_, y_test_ = y.loc[train_index], y.loc[test_index]\n",
    "        #     plug.fit(X_test_, y_test_)\n",
    "        hgb.fit(X_train_, y_train_)\n",
    "        hgb.fit(X_test_, y_test_)\n",
    "        rfc.fit(X_train_, y_train_)\n",
    "        rfc.fit(X_test_, y_test_)\n",
    "        efc.fit(X_train_, y_train_)\n",
    "        efc.fit(X_test_, y_test_)\n",
    "    print_clf_perf(hgb,X_test,y_test)\n",
    "    print_clf_perf(rfc,X_test,y_test)\n",
    "    print_clf_perf(efc,X_test,y_test)\n",
    "    \n",
    "\n",
    "#     for clf_name,clf in l_clfs:\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(linear_model_data, y, test_size=0.3)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=10)\n",
    "    for train_index, test_index in cv.split(X_train, y_train):\n",
    "        X_train_, X_test_ = linear_model_data[train_index, :], linear_model_data[test_index, :]\n",
    "        y_train_, y_test_ = y.loc[train_index], y.loc[test_index]\n",
    "        #     plug.fit(X_test_, y_test_)\n",
    "        sgd.fit(X_train_, y_train_)\n",
    "        sgd.fit(X_test_, y_test_)\n",
    "        svc.fit(X_train_, y_train_)\n",
    "        svc.fit(X_test_, y_test_)\n",
    "    print_clf_perf(sgd,X_test,y_test)\n",
    "    print_clf_perf(svc,X_test,y_test)\n",
    "[hgb,sgd,rfc,efc,benb,munb,conb,cnb,hgb] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef352c-aa6f-4225-8c65-7b18e31f7c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shelve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f066461-362a-43f4-bb35-7da049a8d65b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with shelve.open('final_saved_models') as model_db:\n",
    "    model_db[hgb.__class__.__name__] = hgb \n",
    "    model_db[sgd.__class__.__name__] = sgd    \n",
    "    model_db[rfc.__class__.__name__] = rfc    \n",
    "    model_db[efc.__class__.__name__] = efc    \n",
    "    model_db[benb.__class__.__name__] = benb    \n",
    "    model_db[munb.__class__.__name__] = munb    \n",
    "    model_db[cnb.__class__.__name__] = cnb    \n",
    "    model_db[svc.__class__.__name__] = svc\n",
    "    model_db[conb.__class__.__name__] = conb\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c57a68f-78d5-4d21-ae9e-a54c5cce7dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vc_tree = VotingClassifier(estimators = [\n",
    "    ('1',hgb),\n",
    "    ('3',rfc),\n",
    "    ('4',efc),\n",
    "])\n",
    "vc_nb = VotingClassifier(estimators=[\n",
    "    ('5',cnb),\n",
    "    ('6',benb),\n",
    "    ('7',munb),\n",
    "    ('8',conb),])\n",
    "vc_ll = VotingClassifier(estimators=[\n",
    "    ('2',sgd),\n",
    "    ('9',svc),\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b87ec4-4cce-4e38-91b8-e64be944963a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vc_ll.predict(linear_model_eval)\n",
    "vc_tree.predict(tree_model_eval)\n",
    "# vc_nb.predict(bayesian_model_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9003492b-c507-45f6-8378-90320532c5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
