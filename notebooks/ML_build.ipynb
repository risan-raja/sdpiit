{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90f0379-dcfe-4789-98e1-095a9661cea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "from sklearnex import unpatch_sklearn\n",
    "\n",
    "patch_sklearn()\n",
    "from print_table import Tablify\n",
    "from featboost import FeatBoostClassifier\n",
    "%matplotlib inline\n",
    "from helpers.wrappers import PolynomialWrapper as prt\n",
    "# pd.options.plotting.backend='matplotlib'\n",
    "from helpers.wrappers import NestedCVWrapper as ncw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import (\n",
    "    Binarizer,\n",
    "    StandardScaler,\n",
    "    LabelBinarizer,\n",
    "    OneHotEncoder,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "from xgboost import XGBRFClassifier, XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import dtale\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "sns.set()\n",
    "from sklearn import set_config\n",
    "from sklearn.ensemble import StackingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import (\n",
    "    Product,\n",
    "    RBF,\n",
    "    CompoundKernel,\n",
    "    Exponentiation,\n",
    "    Matern,\n",
    "    Sum,\n",
    ")\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from category_encoders import (\n",
    "    TargetEncoder,\n",
    "    BackwardDifferenceEncoder,\n",
    "    BaseNEncoder,\n",
    "    BinaryEncoder,\n",
    "    CatBoostEncoder,\n",
    "    CatBoostEncoder,\n",
    "    GLMMEncoder,\n",
    "    HelmertEncoder,\n",
    "    JamesSteinEncoder,\n",
    "    PolynomialEncoder,\n",
    "    QuantileEncoder,\n",
    "    SumEncoder,\n",
    "    SummaryEncoder,\n",
    "    WOEEncoder,\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    MinMaxScaler,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\n",
    "import warnings\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import plotly.express as px\n",
    "\n",
    "# from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_validate,\n",
    "    # StratifiedGroupKFold,\n",
    "    StratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    HalvingGridSearchCV,\n",
    "    HalvingRandomSearchCV,\n",
    ")\n",
    "import sigopt\n",
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    CategoricalNB,\n",
    "    MultinomialNB,\n",
    "    ComplementNB,\n",
    "    GaussianNB,\n",
    ")\n",
    "from sklearnex.cluster import DBSCAN, KMeans\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.base import clone as model_clone\n",
    "from sklearnex.svm import NuSVC, SVR\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# nb_est = [CategoricalNB(), MultinomialNB(), ComplementNB(), GaussianNB()]\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.tree import export_graphviz, plot_tree\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_selection import (\n",
    "    mutual_info_classif,\n",
    "    SelectKBest,\n",
    "    f_classif,\n",
    "    chi2,\n",
    "    RFE,\n",
    "    SelectFdr,\n",
    "    SelectFpr,\n",
    "    SelectFwe,\n",
    "    SelectPercentile,\n",
    ")\n",
    "from tbb import Monkey\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from tqdm import trange\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold, RFECV\n",
    "from sklearn.base import clone as model_clone\n",
    "from joblib.memory import Memory\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from joblib import parallel_backend\n",
    "from category_encoders.wrapper import PolynomialWrapper, NestedCVWrapper\n",
    "\n",
    "# from autosklearn.automl import AutoMLClassifier\n",
    "import sklearn.metrics\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pd.options.display.max_columns = 50\n",
    "set_config(display=\"diagram\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# import sklearnex,daal4py\n",
    "def __table__(rows, margin=10, columns=[]):\n",
    "        \"\"\"\n",
    "        Return string representing table content, returns table as string and as a list of strings.\n",
    "        It is okay for rows to have different sets of keys, table will show union of columns with\n",
    "        missing values being empty spaces.\n",
    "        :param rows: list of dictionaries as rows\n",
    "        :param margin: left space padding to apply to each row, default is 0\n",
    "        :param columns: extract listed columns in provided order, other columns will be ignored\n",
    "        :return: table content as string and as list\n",
    "        \"\"\"\n",
    "\n",
    "        def projection(cols, columns):\n",
    "            return (\n",
    "                [(x, cols[x]) for x in columns if x in cols]\n",
    "                if columns\n",
    "                else cols.items()\n",
    "            )\n",
    "\n",
    "        def row_to_string(row, columns):\n",
    "            values = [\n",
    "                (row[name] if name in row else \"\").rjust(size) for name, size in columns\n",
    "            ]\n",
    "            return \"|%s|\" % (\"|\".join(values))\n",
    "\n",
    "        def header(columns):\n",
    "            return \"|%s|\" % (\"|\".join([name.rjust(size) for name, size in columns]))\n",
    "\n",
    "        def divisor(columns):\n",
    "            return \"+%s+\" % (\"+\".join([\"-\" * size for name, size in columns]))\n",
    "\n",
    "        data = [dict([(str(a), str(b)) for a, b in row.items()]) for row in rows]\n",
    "        cols = (\n",
    "            dict([(x, len(x) + 1) for row in data for x in row.keys()]) if data else {}\n",
    "        )\n",
    "        for row in data:\n",
    "            for key in row.keys():\n",
    "                cols[key] = max(cols[key], len(row[key]) + 1)\n",
    "        proj = projection(\n",
    "            cols, columns\n",
    "        )  # extract certain columns to display (or all if not provided)\n",
    "        table = (\n",
    "            [divisor(proj), header(proj), divisor(proj)]\n",
    "            + [row_to_string(row, proj) for row in data]\n",
    "            + [divisor(proj)]\n",
    "        )\n",
    "        table = [\"%s%s\" % (\" \" * margin, tpl) for tpl in table] if margin > 0 else table\n",
    "        table_text = \"\\n\".join(table)\n",
    "        return (table_text, table)\n",
    "\n",
    "def tablify( rows, margin=10, columns=[]):\n",
    "    \"\"\"\n",
    "    Print table in console for list of rows.\n",
    "    \"\"\"\n",
    "    txt, _ = __table__(rows, margin, columns)\n",
    "    # txt = txt\n",
    "    print(txt)\n",
    "cls_names = [\n",
    "    sklearn.ensemble._weight_boosting.AdaBoostClassifier,\n",
    "    sklearn.naive_bayes.BernoulliNB,\n",
    "    # sklearn.naive_bayes.CategoricalNB,\n",
    "    # sklearn.naive_bayes.ComplementNB,\n",
    "    sklearn.tree._classes.DecisionTreeClassifier,\n",
    "    sklearn.tree._classes.ExtraTreeClassifier,\n",
    "    sklearn.ensemble._forest.ExtraTreesClassifier,\n",
    "    sklearn.naive_bayes.GaussianNB,\n",
    "    # sklearn.gaussian_process._gpc.GaussianProcessClassifier,\n",
    "    sklearn.ensemble._gb.GradientBoostingClassifier,\n",
    "    sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
    "    sklearn.neighbors.KNeighborsClassifier,\n",
    "    sklearn.svm._classes.LinearSVC,\n",
    "    sklearn.linear_model.LogisticRegression,\n",
    "    # sklearn.linear_model._logistic.LogisticRegressionCV,\n",
    "    # sklearn.neural_network._multilayer_perceptron.MLPClassifier,\n",
    "    sklearn.naive_bayes.MultinomialNB,\n",
    "    # sklearn.neighbors._nearest_centroid.NearestCentroid,\n",
    "    sklearn.svm.NuSVC,\n",
    "    sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier,\n",
    "    sklearn.linear_model._perceptron.Perceptron,\n",
    "    # sklearn.neighbors._classification.RadiusNeighborsClassifier,\n",
    "    sklearn.ensemble._forest.RandomForestClassifier,\n",
    "    sklearn.linear_model._ridge.RidgeClassifier,\n",
    "    # sklearn.linear_model._ridge.RidgeClassifierCV,\n",
    "    sklearn.linear_model._stochastic_gradient.SGDClassifier,\n",
    "    sklearn.svm.SVC,\n",
    "]\n",
    "classifiers = [f() for f in cls_names]\n",
    "dtype_info = {\n",
    "    \"v_1\": \"Binary\",\n",
    "    \"v_26\": \"Binary\",\n",
    "    \"v_11\": \"Binary\",\n",
    "    \"v_14\": \"Binary\",\n",
    "    \"v_30\": \"Binary\",\n",
    "    \"v_28\": \"Binary\",\n",
    "    \"v_9\": \"Binary\",\n",
    "    \"v_27\": \"Binary\",\n",
    "    \"v_32\": \"Nominal\",\n",
    "    \"v_4\": \"Nominal\",\n",
    "    \"v_3\": \"Nominal\",\n",
    "    \"v_20\": \"Nominal\",\n",
    "    \"v_21\": \"Nominal\",\n",
    "    \"v_18\": \"Nominal\",\n",
    "    \"v_25\": \"Nominal\",\n",
    "    \"v_12\": \"Nominal\",\n",
    "    \"v_31\": \"Ordinal\",\n",
    "    \"v_15\": \"Ordinal\",\n",
    "    \"v_19\": \"Ordinal\",\n",
    "    \"v_13\": \"Ordinal\",\n",
    "    \"v_33\": \"Ordinal\",\n",
    "    \"v_17\": \"Ordinal\",\n",
    "    \"v_29\": \"Ordinal\",\n",
    "    \"v_23\": \"Ordinal\",\n",
    "    \"v_6\": \"Ordinal\",\n",
    "    \"v_24\": \"Ordinal\",\n",
    "    \"v_10\": \"Ordinal\",\n",
    "    \"v_5\": \"Ordinal\",\n",
    "    \"v_22\": \"Ordinal\",\n",
    "    \"v_0\": \"Ordinal\",\n",
    "    \"v_16\": \"Ratio\",\n",
    "    \"v_2\": \"Ratio\",\n",
    "    \"v_8\": \"Ratio\",\n",
    "    \"v_7\": \"Ratio\",\n",
    "    \"v_39\": \"Ratio\",\n",
    "    \"v_37\": \"Ratio\",\n",
    "    \"v_38\": \"Ratio\",\n",
    "    \"v_34\": \"Ratio\",\n",
    "    \"v_40\": \"Ratio\",\n",
    "    \"v_36\": \"Ratio\",\n",
    "    \"v_35\": \"Ratio\",\n",
    "}\n",
    "# data = pd.read_csv(\n",
    "#     \"../data/train.csv\",\n",
    "#     index_col=0,\n",
    "# )\n",
    "data__ = pd.read_parquet(\"../data/data_with_ridit.hdfs\", engine=\"fastparquet\")\n",
    "prediction_data = pd.read_parquet(\"../data/test.parquet\", engine=\"fastparquet\")\n",
    "data = pd.read_parquet(\"../data/train.parquet\", engine=\"fastparquet\")\n",
    "ordinal = [i for i in dtype_info if dtype_info[i] == \"Ordinal\"]\n",
    "nominal = [i for i in dtype_info if dtype_info[i] == \"Nominal\"]\n",
    "binary = [i for i in dtype_info if dtype_info[i] == \"Binary\"]\n",
    "ratio = [i for i in dtype_info if dtype_info[i] == \"Ratio\"]\n",
    "final_data = pd.read_parquet(\"../data/final_data.parquet\", engine=\"fastparquet\")\n",
    "final_pred_data = pd.read_parquet(\n",
    "    \"../data/final_pred_data.parquet\", engine=\"fastparquet\"\n",
    ")\n",
    "baseline_prediction_data = pd.read_parquet(\"../data/baseline.parquet\")\n",
    "\n",
    "\n",
    "def categorise_data(data):\n",
    "    ordinal_data = data.loc[:, ordinal]\n",
    "    nominal_data = data.loc[:, nominal]\n",
    "    binary_data = data.loc[:, binary]\n",
    "    ratio_data = data.loc[:, ratio]\n",
    "    return ordinal_data, nominal_data, binary_data, ratio_data\n",
    "\n",
    "\n",
    "ordinal_data, nominal_data, binary_data, ratio_data = categorise_data(final_data)\n",
    "baseline_prediction_data.rename(columns={\"label\": \"target\"}, inplace=True)\n",
    "final_data.rename(columns={\"label\": \"target\"}, inplace=True)\n",
    "data.rename(columns={\"label\": \"target\"}, inplace=True)\n",
    "# baseline_prediction_data\n",
    "\n",
    "bs = pd.concat([final_data, baseline_prediction_data], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "def gen_balanced_trained_test(data, p):\n",
    "    Y = data.target\n",
    "    X_2 = Y_2 = Y[Y == 2].index\n",
    "    X_0 = Y_0 = Y[Y == 0].index\n",
    "    X_1 = Y_1 = Y[Y == 1].index\n",
    "    train_size = int(p * Y_2.shape[0])\n",
    "    test_size = int((1 - p) * Y_2.shape[0])\n",
    "\n",
    "    train_idx_2 = np.random.choice(Y_2, (train_size,))\n",
    "    train_idx_1 = np.random.choice(Y_1, (train_size,))\n",
    "    train_idx_0 = np.random.choice(Y_0, (train_size,))\n",
    "    train_idx = np.r_[train_idx_0, train_idx_1, train_idx_2]\n",
    "    # train_idx.shape\n",
    "\n",
    "    test_idx_2 = np.random.choice(np.setdiff1d(Y_2, train_idx_2), (test_size,))\n",
    "    test_idx_1 = np.random.choice(np.setdiff1d(Y_1, train_idx_1), (test_size,))\n",
    "    test_idx_0 = np.random.choice(np.setdiff1d(Y_0, train_idx_0), (test_size,))\n",
    "    test_idx = np.r_[test_idx_0, test_idx_1, test_idx_2]\n",
    "    # test_idx.shape\n",
    "    return train_idx, test_idx\n",
    "\n",
    "\n",
    "def gen_nominal_maps(bs: pd.DataFrame = bs) -> tuple[defaultdict, defaultdict]:\n",
    "    nominal_master_db = bs.loc[:, nominal]\n",
    "    nominal_cont_map = defaultdict(dict)\n",
    "    nominal_indvl_map = defaultdict(dict)\n",
    "    for c in nominal:\n",
    "        un = sorted(nominal_master_db[c].unique().tolist())\n",
    "        n = len(un)\n",
    "        new_id = list(range(n))\n",
    "        nominal_indvl_map[c] = dict(zip(un, new_id))\n",
    "    start = 0\n",
    "    for c in nominal:\n",
    "        un = sorted(nominal_master_db[c].unique().tolist())\n",
    "        n = len(un)\n",
    "        new_id = list(range(start, start + n))\n",
    "        nominal_cont_map[c] = dict(zip(un, new_id))\n",
    "        start += n\n",
    "    return nominal_indvl_map, nominal_cont_map\n",
    "\n",
    "\n",
    "nominal_indvl_map, nominal_cont_map = gen_nominal_maps()\n",
    "nominal_master_db = bs.loc[:, nominal]\n",
    "\n",
    "nominal_master_db_indvl = nominal_master_db.copy()\n",
    "nominal_master_db_cont = nominal_master_db.copy()\n",
    "\n",
    "\n",
    "# nominal_indvl_map\n",
    "def nm_indvl_data_trnsform(row):\n",
    "    for c in nominal:\n",
    "        curr = row[c]\n",
    "        row[c] = nominal_indvl_map[c][curr]\n",
    "    return row\n",
    "\n",
    "\n",
    "test1_nominal = nominal_master_db_indvl.apply(nm_indvl_data_trnsform, axis=1)\n",
    "\n",
    "\n",
    "def nm_cont_data_trnsform(row):\n",
    "    for c in nominal:\n",
    "        curr = row[c]\n",
    "        row[c] = nominal_cont_map[c][curr]\n",
    "    return row\n",
    "\n",
    "\n",
    "test2_nominal = nominal_master_db_cont.apply(nm_cont_data_trnsform, axis=1)\n",
    "\n",
    "\n",
    "def best_n_features(n, data=pd.DataFrame(), target=pd.DataFrame):\n",
    "    if (data.shape[0] > 0) and (target.shape[0] > 0):\n",
    "        mic = mutual_info_classif(data, target, discrete_features=True)\n",
    "    else:\n",
    "        default_data = pd.read_parquet(\n",
    "            \"../data/final_data.parquet\", engine=\"fastparquet\"\n",
    "        )\n",
    "        data_X = default_data.drop([\"target\"], axis=1)\n",
    "        data_y = default_data.target\n",
    "    return ohe.get_feature_names_out()[mic.argsort()[-n:]]\n",
    "\n",
    "\n",
    "# prediction_data = pd.read_pickle(\"../data/pred_data.pkl\")\n",
    "# est_ = [(\"cnb\",CategoricalNB()),]\n",
    "def iv_woe(data, target, bins=10, show_woe=False):\n",
    "    \"\"\"\n",
    "    :params data: pandas.DataFrame\n",
    "    :params target: str\n",
    "    :params bins: int\n",
    "    :params show_woe: bool\n",
    "    :returns newDF: pandas.DataFrame, woeDF: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    # Empty Dataframe\n",
    "    newDF, woeDF = pd.DataFrame(), pd.DataFrame()\n",
    "    # Extract Column Names\n",
    "    cols = data.columns\n",
    "    # Run WOE and IV on all the independent variables\n",
    "    for ivars in cols[~cols.isin([target])]:\n",
    "        if (data[ivars].dtype.kind in \"bifc\") and (len(np.unique(data[ivars])) > 1000):\n",
    "            binned_x = pd.qcut(data[ivars], bins, duplicates=\"drop\")\n",
    "            d0 = pd.DataFrame({\"x\": binned_x, \"y\": data[target]})\n",
    "        else:\n",
    "            d0 = pd.DataFrame({\"x\": data[ivars], \"y\": data[target]})\n",
    "        d0 = d0.astype({\"x\": str})\n",
    "        d = d0.groupby(\"x\", as_index=False, dropna=False).agg({\"y\": [\"count\", \"sum\"]})\n",
    "        d.columns = [\"Cutoff\", \"N\", \"Events\"]\n",
    "        d[\"% of Events\"] = np.maximum(d[\"Events\"], 0.5) / d[\"Events\"].sum()\n",
    "        d[\"Non-Events\"] = d[\"N\"] - d[\"Events\"]\n",
    "        d[\"% of Non-Events\"] = np.maximum(d[\"Non-Events\"], 0.5) / d[\"Non-Events\"].sum()\n",
    "        d[\"WoE\"] = np.log(d[\"% of Non-Events\"] / d[\"% of Events\"])\n",
    "        d[\"IV\"] = d[\"WoE\"] * (d[\"% of Non-Events\"] - d[\"% of Events\"])\n",
    "        d.insert(loc=0, column=\"Variable\", value=ivars)\n",
    "        print(\"Information value of \" + ivars + \" is \" + str(round(d[\"IV\"].sum(), 6)))\n",
    "        temp = pd.DataFrame(\n",
    "            {\"Variable\": [ivars], \"IV\": [d[\"IV\"].sum()]}, columns=[\"Variable\", \"IV\"]\n",
    "        )\n",
    "        newDF = pd.concat([newDF, temp], axis=0)\n",
    "        woeDF = pd.concat([woeDF, d], axis=0)\n",
    "        # Show WOE Table\n",
    "        if show_woe == True:\n",
    "            print(d)\n",
    "    return newDF, woeDF\n",
    "\n",
    "\n",
    "def wf_create(cat_encoder=TargetEncoder, model=None):\n",
    "    \"\"\"\n",
    "    :param cat_encoder: category_encoders\n",
    "    :param model: scikit-learn Model\n",
    "    :return pipe: sklearn.pipeline.Pipline\n",
    "    Examples of model param:\n",
    "\n",
    "    model = ComplementNB(norm=True,fit_prior=True,)\n",
    "    model = MultinomialNB()\n",
    "    model = LogisticRegression(n_jobs=-1, max_iter=10000,random_state=19)\n",
    "    \"\"\"\n",
    "    _steps = []\n",
    "    encoder__name = cat_encoder.__class__.__name__\n",
    "    _steps.append(\n",
    "        (\"PW\" + encoder__name, PolynomialWrapper(feature_encoder=cat_encoder))\n",
    "    )\n",
    "    if model is None:\n",
    "        pass\n",
    "    else:\n",
    "        model__name = model.__class__.__name__\n",
    "        _steps.append((model__name, model))\n",
    "    pipe = Pipeline(steps=_steps)\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def plot_m_variance(d_coll: list[tuple[float, float, float, float]]):\n",
    "    \"\"\"\n",
    "    :param d_coll: list[tuple[float, float, float, float]\n",
    "    list of data points in the format\n",
    "    (abcissa, mean, std, max)\n",
    "    \"\"\"\n",
    "    sns.set()\n",
    "    ddx = [x for x, y, u, r in d_coll]\n",
    "    ddc = [y for x, y, u, r in d_coll]\n",
    "    ddep = [y + u for x, y, u, r in d_coll]\n",
    "    dden = [y - u for x, y, u, r in d_coll]\n",
    "    ddem = [r for x, y, u, r in d_coll]\n",
    "    plt.plot(ddx, ddc, \"b\", label=\"\\u00b5\")\n",
    "    plt.plot(ddx, ddep, \"r\", label=\"\\u03c3\" + \"+\")\n",
    "    plt.plot(ddx, dden, \"g\", label=\"\\u03c3\" + \"-\")\n",
    "    plt.plot(ddx, ddem, \"y\", label=\"\\u03c3\" + \"max\")\n",
    "    fig = plt.fill_between(ddx, ddep, dden, alpha=0.5)\n",
    "    fig = plt.legend()\n",
    "    plt.show()\n",
    "    # return plt\n",
    "\n",
    "\n",
    "# print(pipe)\n",
    "# wf_create(TargetEncoder(),RandomForestClassifier())\n",
    "final_data_indvl = final_data.copy()\n",
    "final_pred_data_indvl_test = final_pred_data.copy()\n",
    "final_data_indvl.loc[:, nominal] = test1_nominal.loc[final_data.index, :]\n",
    "# final_pred_data_indvl_test.loc[:, nominal] = test1_nominal.loc[3796:, :]\n",
    "pred_data_indvl = test1_nominal.loc[3796:,nominal].copy()\n",
    "pred_data_indvl.index = final_pred_data.index\n",
    "final_pred_data_indvl_test.loc[:, nominal] = pred_data_indvl\n",
    "# final_pred_data_indvl_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b428290-d847-44c9-a9aa-7acfa29177aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "mdb = MongoClient()\n",
    "# mdb.list_database_names()\n",
    "class evaluate_model:\n",
    "    collector = mdb.ml_results.cv_results\n",
    "\n",
    "    def __init__(self, model, data=pd.DataFrame(), features=[], target=None):\n",
    "        self.model = model\n",
    "        if data.shape[0] > 0:\n",
    "            self.data = data\n",
    "        else:\n",
    "            self.data = pd.read_parquet(\n",
    "                \"../data/final_data.parquet\", engine=\"fastparquet\"\n",
    "            )\n",
    "        if len(features) > 0:\n",
    "            self.features = features\n",
    "        else:\n",
    "            self.features = list(self.data.columns)\n",
    "        self.train_idx, self.test_idx = gen_balanced_trained_test(self.data, 1.0)\n",
    "        self.X_train = self.data.loc[self.train_idx, self.features]\n",
    "        self.y_train = self.data.target.loc[self.train_idx]\n",
    "        # label_encoder =\n",
    "        self.cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3)\n",
    "        self.cv_results = None\n",
    "        self.legacy = 0\n",
    "\n",
    "    def run_balanced(self, custom_model=None):\n",
    "        if custom_model:\n",
    "            selected_model = custom_model\n",
    "        else:\n",
    "            selected_model = self.model\n",
    "        with parallel_backend(\"threading\"):\n",
    "            cv_results = cross_validate(\n",
    "                selected_model,\n",
    "                self.X_train,\n",
    "                self.y_train,\n",
    "                cv=self.cv,\n",
    "                n_jobs=-1,\n",
    "                scoring=[\n",
    "                    \"f1_micro\",\n",
    "                    \"f1_macro\",\n",
    "                    \"f1_weighted\",\n",
    "                    \"precision_micro\",\n",
    "                    \"precision_macro\",\n",
    "                    \"precision_weighted\",\n",
    "                    \"recall_micro\",\n",
    "                    \"recall_macro\",\n",
    "                    \"recall_weighted\",\n",
    "                ],\n",
    "                return_train_score=True,\n",
    "                return_estimator=True,\n",
    "            )\n",
    "            # error_score='raise',return_estimator=True)\n",
    "            ## Save this results\n",
    "        self.cv_results = cv_results\n",
    "        # cv_db.insert_one(cv_results)\n",
    "        # cnst = {'estimator_params': selected_model.get_params()}\n",
    "        record = {}\n",
    "        record[\"estimator_params\"] = str(selected_model.get_params())\n",
    "\n",
    "        record[\"model_name\"] = selected_model.__class__.__name__\n",
    "        record[\"features\"] = list(self.features)\n",
    "        # record['data_idx'] = {'train_idx': list(self.train_idx),'test_idx': list(self.test_idx)}\n",
    "        # cnst['fit_time'] = list(cv_results['fit_time'])\n",
    "        for k, v in cv_results.items():\n",
    "            if k in [\"estimator\", \"fit_time\", \"score_time\"]:\n",
    "                continue\n",
    "            else:\n",
    "                record[k] = v.mean()\n",
    "        # for _ in cv_db.find():\n",
    "        #     print(_.keys())\n",
    "        # print(record)\n",
    "        record[\"is_legacy_run\"] = self.legacy\n",
    "        record[\"legacy_report\"] = None\n",
    "        if self.legacy == 1:\n",
    "            # print(self.legacy_run())\n",
    "            l_rpt = self.legacy_run()\n",
    "            for k, v in l_rpt.items():\n",
    "                print(k)\n",
    "                print(\"~\" * 20)\n",
    "                print(v)\n",
    "                print(\"#\" * 90)\n",
    "            record[\"legacy_report\"] = l_rpt\n",
    "\n",
    "        collector.insert_one(record)\n",
    "\n",
    "        return cv_results\n",
    "\n",
    "    def plot(self):\n",
    "        sns.set()\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "        # fig.layout='constrained'\n",
    "\n",
    "        metrics = [\n",
    "            \"f1_micro\",\n",
    "            \"f1_macro\",\n",
    "            \"f1_weighted\",\n",
    "            \"precision_micro\",\n",
    "            \"precision_macro\",\n",
    "            \"precision_weighted\",\n",
    "            \"recall_micro\",\n",
    "            \"recall_macro\",\n",
    "            \"recall_weighted\",\n",
    "        ]\n",
    "        # for m  in metrics:\n",
    "        #     n = len(self.cv_results['test_'+m])\n",
    "        idx = 0\n",
    "        for i in range(3):\n",
    "            n = len(self.cv_results[\"test_\" + metrics[0]])\n",
    "            for j in range(3):\n",
    "                axes[i, j].plot(\n",
    "                    range(n),\n",
    "                    self.cv_results[\"test_\" + metrics[idx]],\n",
    "                    label=[\"test_\" + metrics[idx]],\n",
    "                )\n",
    "                # axes[i,j].plot(range(n),self.cv_results['train_'+metrics[idx]],label=['train_'+metrics[idx]])\n",
    "                # axes[i,j].fill_between(range(n),self.cv_results['test_'+metrics[idx]],self.cv_results['train_'+metrics[idx]])\n",
    "                axes[i, j].legend()\n",
    "                axes[i, j].set_title(metrics[idx])\n",
    "                # axes[i,j].autoscale(enable=False)\n",
    "                idx += 1\n",
    "        plt.legend()\n",
    "\n",
    "    def plot_metric(self, metric):\n",
    "        ydata = self.cv_results[\"test_\" + metric]\n",
    "        n = ydata.shape[0]\n",
    "        ffig = plt.plot(\n",
    "            np.arange(n),\n",
    "            ydata,\n",
    "        )\n",
    "        fig = sns.regplot(x=np.arange(n), y=ydata)\n",
    "        plt.title(metric.upper())\n",
    "        return\n",
    "\n",
    "    def switch_to_default_data(self):\n",
    "        if self.legacy == 1:\n",
    "            print(\"#\" * 100)\n",
    "            print(\"ALREADY ON LEGACY MODE\")\n",
    "            print(\"#\" * 100)\n",
    "            return\n",
    "        self.legacy = 1\n",
    "        self.X_train_old = self.X_train\n",
    "        self.y_train_old = self.y_train\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.data.loc[:, self.features], self.data.target\n",
    "        )\n",
    "        print(\"#\" * 100)\n",
    "        print(\"SWITCHING TO LEGACY MODE\")\n",
    "        print(\"#\" * 100)\n",
    "\n",
    "    def reset(self):\n",
    "        if self.legacy == 0:\n",
    "            print(\"#\" * 100)\n",
    "            print(\"NOT ON LEGACY MODE\")\n",
    "            print(\"#\" * 100)\n",
    "            return\n",
    "        self.legacy = 0\n",
    "        self.X_train = self.X_train_old\n",
    "        self.y_train = self.y_train_old\n",
    "\n",
    "    def legacy_run(self):\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        self.y_pred = self.model.predict(self.X_test)\n",
    "        self.legacy_report = {}\n",
    "        self.legacy_report[\"classification_report\"] = classification_report(\n",
    "            self.y_test, self.y_pred\n",
    "        )\n",
    "        self.legacy_report[\n",
    "            \"balanced_accuracy\"\n",
    "        ] = sklearn.metrics.balanced_accuracy_score(self.y_test, self.y_pred)\n",
    "        return self.legacy_report\n",
    "\n",
    "    def print_last_run_stat(self):\n",
    "        N = collector.estimated_document_count()\n",
    "        idx = 0\n",
    "        for c in collector.find(skip=N - 1):\n",
    "            for k, v in c.items():\n",
    "                if \"f1_\" in k and \"test\" in k:\n",
    "                    print(k)\n",
    "                    print(\"~\" * 5)\n",
    "                    print(v)\n",
    "                    print(\"-\" * 20)\n",
    "            print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b68e8-224e-4459-a958-d44d7d1fe89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nnom = pd.get_dummies(nnom, columns=nominal, drop_first=True,dtype=np.uint8)\n",
    "# base_prediction_perf_variance = {i:[] for i in range(1,7)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cda5fe6-1786-4c98-bb48-d25eefd04082",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcategory_encoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LeaveOneOutEncoder\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpipe\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "from category_encoders import LeaveOneOutEncoder\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c26d37fb-74ec-44c3-9226-606c0e47d0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:02<00:00,  1.34it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nnom: pd.DataFrame = final_data_indvl.loc[:,nominal]\n",
    "dcol = []\n",
    "x =0\n",
    "steps = [('onevsoneclassifier',  OneVsRestClassifier(estimator=DecisionTreeClassifier(random_state=10),n_jobs=-1))]\n",
    "job_mem = Memory()\n",
    "pipe = Pipeline(steps, memory=job_mem)\n",
    "['pooled', 'beta', 'binary', 'independent']\n",
    "try:\n",
    "    for j in tqdm(['pooled', 'beta', 'binary', 'independent']):\n",
    "        bne = LeaveOneOutEncoder(cols=nominal,drop_invariant=False)\n",
    "        pw =prt(bne)\n",
    "        nc = ncw(pw)\n",
    "        bNnom = pw.fit_transform(nnom,data.target)\n",
    "        # print(bNnom.shape[1])z\n",
    "        # break\n",
    "        # print(bNnom.shape[1])\n",
    "        with parallel_backend('threading'):\n",
    "            results = cross_validate(pipe,\n",
    "                                     bNnom,data.target, \n",
    "                                     cv=RepeatedStratifiedKFold(n_splits=3,n_repeats=10,random_state=19),return_train_score=True,scoring='f1_micro',n_jobs=-1)\n",
    "            stat = results['test_score']\n",
    "            stat2 = results['train_score']\n",
    "            dcol.append((x,stat.mean(),stat.std(),stat2.max()))\n",
    "        x+=1\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted\")\n",
    "plot_m_variance(dcol)\n",
    "\n",
    "\n",
    "# # fig,ax=plt.subplots(13,4,squeeze=False)\n",
    "# # plt.figure(figsize=(20,30))\n",
    "# # fig.set_figheight = 30\n",
    "# # fig.set_figwidth = 30\n",
    "# # for i in np.linspace(0.1,1,num=4):\n",
    "# #     y =0\n",
    "# #     tr_c: pd.Series = bNnom.corrwith(final_data.target, method='kendall').sort_values()\n",
    "# #     tr_c.plot(kind='line',stacked=False,ax=ax[y][x],figsize=(30,50))\n",
    "# #     y+=1\n",
    "# # x+=1\n",
    "# # summary = [(i,v.mean(),v.std(),v.max()) for k,v in results.items() if k not in ['fit_time', 'score_time']]\n",
    "# stat = results['test_score']\n",
    "# dcol.append((j,stat.mean(),stat.std(),stat.max()))\n",
    "# # print(Tablify(summary))\n",
    "# # break\n",
    "# # base_prediction_perf_variance[i] = summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "19f58409-0811-4ea5-8788-6b5c6fe202e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2847 entries, 0 to 2846\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   v_32    2847 non-null   float64\n",
      " 1   v_4     2847 non-null   float64\n",
      " 2   v_3     2847 non-null   float64\n",
      " 3   v_20    2847 non-null   float64\n",
      " 4   v_21    2847 non-null   float64\n",
      " 5   v_18    2847 non-null   float64\n",
      " 6   v_25    2847 non-null   float64\n",
      " 7   v_12    2847 non-null   float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 178.1 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold, train_test_split\n",
    "\n",
    "X_train, X_test, y_train, _ = train_test_split(X, y)\n",
    "enc_nested = NestedCVWrapper(TargetEncoder(cols=nominal), random_state=42)\n",
    "X_train_enc, X_test_enc = enc_nested.fit_transform(X_train, y_train, X_test=(X_test,))\n",
    "print(X_train_enc.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bb41231-0ac7-4f9e-9a48-7c720c7e7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ee9f0139-36fd-4af4-a3f5-b6053e337405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vt', VarianceThreshold())\n",
      "('SelectKBest', SelectKBest(k=15, score_func=<function mutual_info_classif at 0x7fdff20f4d30>))\n",
      "('onevsoneclassifier', OneVsRestClassifier(estimator=RandomForestClassifier(max_features=None,\n",
      "                                                     min_impurity_decrease=0.001,\n",
      "                                                     random_state=10),\n",
      "                    n_jobs=-1))\n"
     ]
    }
   ],
   "source": [
    "# %pprint\n",
    "import pprint\n",
    "steps = [('onevsoneclassifier',  OneVsRestClassifier(estimator=RandomForestClassifier(min_impurity_decrease=0.001,max_features=None,random_state=10),n_jobs=-1))]\n",
    "steps = [('vt', VarianceThreshold()),('SelectKBest',SelectKBest(mutual_info_classif,k=15))] + steps\n",
    "for e in steps:\n",
    "    pprint.pprint(e,width=1000,compact=False,depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "028e6ea8-e1c0-44fa-a5db-2da6013f192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipe( X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # enc_nested = ncw(prt(LeaveOneOutEncoder(cols=nominal,random_state=10)), shuffle=False,)\n",
    "    # X_train_enc, X_test_enc = enc_nested.fit_transform(X_train, y_train, X_test=(X_test,))\n",
    "    # # enc = LeaveOneOutEncoder(cols=nominal)\n",
    "    # # X_train_enc = enc.fit_column_map \n",
    "    encoder = prt(LeaveOneOutEncoder(cols=nominal))\n",
    "    X_train_enc = encoder.fit_transform(X_train, y_train)\n",
    "    X_test_enc = encoder.transform(X_test)\n",
    "    experiment =pd.concat([X_train_enc,X_test_enc], axis=0)\n",
    "    print(f'shape transformed {X_train_enc.shape}')\n",
    "    feature_corr = experiment.corrwith(final_data.target).sort_values()\n",
    "    # print(feature_corr.mean())\n",
    "    # steps = [('onevsoneclassifier',  OneVsRestClassifier(estimator=RandomForestClassifier(min_impurity_decrease=0.001,max_features=None,random_state=10),n_jobs=-1))]\n",
    "    # steps = [('vt', VarianceThreshold()),('SelectKBest',SelectKBest(mutual_info_classif,k=15))] + steps\n",
    "    steps = [\n",
    "        ('vt', VarianceThreshold()),\n",
    "        # ('SelectKBest', SelectKBest(k=20, score_func=mutual_info_classif)),\n",
    "        ('onevsoneclassifier', OneVsOneClassifier(estimator=LogisticRegressionCV(random_state=10, max_iter=100000)))]\n",
    "                                                             # (max_features=None,\n",
    "                                                             # min_impurity_decrease=0.001,\n",
    "                                                             # random_state=10),n_jobs=-1))\n",
    "    job_mem = Memory()\n",
    "    pipe = Pipeline(steps, memory=job_mem)\n",
    "    with parallel_backend('loky'):\n",
    "        pipe.fit(X_train_enc, y_train)\n",
    "        y_pred = pipe.predict(X_test_enc)\n",
    "        print(classification_report(y_pred, y_test,))\n",
    "        print('cohen_kappa_score')\n",
    "        print(sklearn.metrics.cohen_kappa_score(y_pred,y_test))\n",
    "        print('balanced_accuracy_score')\n",
    "        print(sklearn.metrics.balanced_accuracy_score(y_test,y_pred))\n",
    "        print('accuracy_score')\n",
    "        print(sklearn.metrics.accuracy_score(y_test,y_pred,)*0.85)\n",
    "        print('f1_score_micro')\n",
    "        print(sklearn.metrics.f1_score(y_test, y_pred, average='micro'))\n",
    "        print('f1_score_macro')\n",
    "        print(sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n",
    "        print('f1_score_weighted')\n",
    "        print(sklearn.metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    # confusion_matrix = sklearn.metrics.confusion_matrix(y_test, p)\n",
    "    # sklearn.metrics.ConfusionMatrixDisplay()\n",
    "    return pipe,y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b7d3cc06-6c3b-4d0f-a042-0315948803f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape transformed (1656, 24)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.47      0.46       129\n",
      "           1       0.51      0.43      0.47       163\n",
      "           2       0.35      0.40      0.38       119\n",
      "\n",
      "    accuracy                           0.44       411\n",
      "   macro avg       0.44      0.44      0.43       411\n",
      "weighted avg       0.44      0.44      0.44       411\n",
      "\n",
      "cohen_kappa_score\n",
      "0.15328467153284675\n",
      "balanced_accuracy_score\n",
      "0.43552311435523117\n",
      "accuracy_score\n",
      "0.3701946472019465\n",
      "f1_score_micro\n",
      "0.43552311435523117\n",
      "f1_score_macro\n",
      "0.43343776106934007\n",
      "f1_score_weighted\n",
      "0.43343776106934007\n",
      "shape transformed (2657, 24)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.55      0.50       312\n",
      "           1       0.82      0.57      0.67       809\n",
      "           2       0.03      0.28      0.05        18\n",
      "\n",
      "    accuracy                           0.56      1139\n",
      "   macro avg       0.43      0.47      0.41      1139\n",
      "weighted avg       0.71      0.56      0.62      1139\n",
      "\n",
      "cohen_kappa_score\n",
      "0.21166412145976987\n",
      "balanced_accuracy_score\n",
      "0.4337206196581196\n",
      "accuracy_score\n",
      "0.47761194029850745\n",
      "f1_score_micro\n",
      "0.5618964003511853\n",
      "f1_score_macro\n",
      "0.40634000302495904\n",
      "f1_score_weighted\n",
      "0.5074809499329466\n"
     ]
    }
   ],
   "source": [
    "X = nnom = final_data_indvl.loc[:,nominal]\n",
    "D = X.copy()\n",
    "D['target'] = final_data.target\n",
    "train_idx, test_idx = gen_balanced_trained_test(D,0.8)\n",
    "X_train, y_train = X.loc[train_idx,:], final_data.target.loc[train_idx]\n",
    "X_test, y_test  = X.loc[test_idx,:], final_data.target.loc[test_idx]\n",
    "run_pipe(X_train, y_train, X_test, y_test)\n",
    "# # steps = [('onevsoneclassifier',  OneVsOneClassifier(estimator=DecisionTreeClassifier(ccp_alpha=0.0001,min_impurity_decrease=0.0001,min_samples_leaf=3,random_state=10),n_jobs=-1))]\n",
    "\n",
    "\n",
    "X = nnom = final_data_indvl.loc[:,nominal]\n",
    "y = final_data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,shuffle=True)\n",
    "pipe, y_pred=run_pipe(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b0d3f092-0742-48d8-a9c8-8c425cd4125b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape transformed (3796, 24)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.56      0.49       410\n",
      "           1       0.82      0.75      0.78      1195\n",
      "           2       0.00      0.00      0.00        23\n",
      "\n",
      "    accuracy                           0.69      1628\n",
      "   macro avg       0.42      0.44      0.42      1628\n",
      "weighted avg       0.71      0.69      0.70      1628\n",
      "\n",
      "cohen_kappa_score\n",
      "0.273772392566191\n",
      "balanced_accuracy_score\n",
      "0.6259868982312095\n",
      "accuracy_score\n",
      "0.5894656019656019\n",
      "f1_score_micro\n",
      "0.6934889434889435\n",
      "f1_score_macro\n",
      "0.4242553829715763\n",
      "f1_score_weighted\n",
      "0.6884200002936499\n",
      "          +-----+----+---+\n",
      "          |    1|   0|  2|\n",
      "          +-----+----+---+\n",
      "          | 1195| 410| 23|\n",
      "          | 1101| 527|   |\n",
      "          +-----+----+---+\n"
     ]
    }
   ],
   "source": [
    "X_test = final_pred_data_indvl_test.loc[:,nominal]\n",
    "X_train = nnom\n",
    "y_train = final_data.target\n",
    "y_test = baseline_prediction_data.target\n",
    "pipe,y_pred = run_pipe(X_train, y_train, X_test, y_test)\n",
    "__t = [pd.Series(y_pred).value_counts().to_dict(),y_test.value_counts().to_dict()]\n",
    "tablify(__t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c13926-e643-4148-8afa-7db683cf2068",
   "metadata": {},
   "outputs": [],
   "source": [
    "StackingClassifier([AdaBoostClassifier(random_state=10), RandomForestClassifier(random_state=10,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7af3f-d195-4169-bd14-65363e7cfbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target  y_pred\n",
       "0       1         297\n",
       "        0         229\n",
       "        2           1\n",
       "1       1         908\n",
       "        0         182\n",
       "        2          11\n",
       "Name: y_pred, dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_comp = pd.concat([y_test,pd.Series(y_pred,name='y_pred')],axis=1)\n",
    "\n",
    "perf_comp.groupby(['target'])['y_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35ef0e8-0671-476d-b58e-ee18e2c298fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autoskl)",
   "language": "python",
   "name": "autoskl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
