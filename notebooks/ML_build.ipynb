{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f0379-dcfe-4789-98e1-095a9661cea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "# patch_sklearn()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import (\n",
    "    Binarizer,\n",
    "    StandardScaler,\n",
    "    LabelBinarizer,\n",
    "    OneHotEncoder,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "# from xgboost import XGBRFClassifier, XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# import dtale\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "# sns.set()\n",
    "from sklearn import set_config\n",
    "from sklearn.ensemble import StackingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import (\n",
    "    Product,\n",
    "    RBF,\n",
    "    CompoundKernel,\n",
    "    Exponentiation,\n",
    "    Matern,\n",
    "    Sum,\n",
    ")\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from category_encoders import (\n",
    "    TargetEncoder,\n",
    "    BackwardDifferenceEncoder,\n",
    "    BaseNEncoder,\n",
    "    BinaryEncoder,\n",
    "    CatBoostEncoder,\n",
    "    CatBoostEncoder,\n",
    "    GLMMEncoder,\n",
    "    HelmertEncoder,\n",
    "    JamesSteinEncoder,\n",
    "    PolynomialEncoder,\n",
    "    QuantileEncoder,\n",
    "    SumEncoder,\n",
    "    SummaryEncoder,\n",
    "    WOEEncoder,\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    MinMaxScaler,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\n",
    "import warnings\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import plotly.express as px\n",
    "# from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_validate,\n",
    "    # StratifiedGroupKFold,\n",
    "    StratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    HalvingGridSearchCV,\n",
    "    HalvingRandomSearchCV\n",
    ")\n",
    "import sigopt\n",
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    CategoricalNB,\n",
    "    MultinomialNB,\n",
    "    ComplementNB,\n",
    "    GaussianNB,\n",
    ")\n",
    "from sklearnex.cluster import DBSCAN, KMeans     \n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.base import clone as model_clone\n",
    "from sklearnex.svm import NuSVC,SVR\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# nb_est = [CategoricalNB(), MultinomialNB(), ComplementNB(), GaussianNB()]\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.tree import export_graphviz, plot_tree\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_selection import (\n",
    "    mutual_info_classif,\n",
    "    SelectKBest,\n",
    "    f_classif,\n",
    "    chi2,\n",
    "    RFE,\n",
    "    SelectFdr,\n",
    "    SelectFpr,\n",
    "    SelectFwe,\n",
    "    SelectPercentile,\n",
    ")\n",
    "from tbb import Monkey\n",
    "# from xgboost import XGBClassifier\n",
    "import os\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from joblib import parallel_backend\n",
    "from category_encoders.wrapper import PolynomialWrapper, NestedCVWrapper\n",
    "# from autosklearn.automl import AutoMLClassifier\n",
    "import sklearn.metrics\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pd.options.display.max_columns = 50\n",
    "set_config(display=\"diagram\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# import sklearnex,daal4py\n",
    "\n",
    "cls_names = [sklearn.ensemble._weight_boosting.AdaBoostClassifier,\n",
    " sklearn.naive_bayes.BernoulliNB,\n",
    " # sklearn.naive_bayes.CategoricalNB,\n",
    " # sklearn.naive_bayes.ComplementNB,\n",
    " sklearn.tree._classes.DecisionTreeClassifier,\n",
    " sklearn.tree._classes.ExtraTreeClassifier,\n",
    " sklearn.ensemble._forest.ExtraTreesClassifier,\n",
    " sklearn.naive_bayes.GaussianNB,\n",
    " # sklearn.gaussian_process._gpc.GaussianProcessClassifier,\n",
    " sklearn.ensemble._gb.GradientBoostingClassifier,\n",
    " sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
    " sklearn.neighbors.KNeighborsClassifier,\n",
    " sklearn.svm._classes.LinearSVC,\n",
    " sklearn.linear_model.LogisticRegression,\n",
    " # sklearn.linear_model._logistic.LogisticRegressionCV,\n",
    " # sklearn.neural_network._multilayer_perceptron.MLPClassifier,\n",
    " sklearn.naive_bayes.MultinomialNB,\n",
    " # sklearn.neighbors._nearest_centroid.NearestCentroid,\n",
    " sklearn.svm.NuSVC,\n",
    " sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier,\n",
    " sklearn.linear_model._perceptron.Perceptron,\n",
    " # sklearn.neighbors._classification.RadiusNeighborsClassifier,\n",
    " sklearn.ensemble._forest.RandomForestClassifier,\n",
    " sklearn.linear_model._ridge.RidgeClassifier,\n",
    " # sklearn.linear_model._ridge.RidgeClassifierCV,\n",
    " sklearn.linear_model._stochastic_gradient.SGDClassifier,\n",
    " sklearn.svm.SVC,\n",
    "]\n",
    "classifiers = [f() for f in cls_names]\n",
    "dtype_info = {\n",
    "    \"v_1\": \"Binary\",\n",
    "    \"v_26\": \"Binary\",\n",
    "    \"v_11\": \"Binary\",\n",
    "    \"v_14\": \"Binary\",\n",
    "    \"v_30\": \"Binary\",\n",
    "    \"v_28\": \"Binary\",\n",
    "    \"v_9\": \"Binary\",\n",
    "    \"v_27\": \"Binary\",\n",
    "    \"v_32\": \"Nominal\",\n",
    "    \"v_4\": \"Nominal\",\n",
    "    \"v_3\": \"Nominal\",\n",
    "    \"v_20\": \"Nominal\",\n",
    "    \"v_21\": \"Nominal\",\n",
    "    \"v_18\": \"Nominal\",\n",
    "    \"v_25\": \"Nominal\",\n",
    "    \"v_12\": \"Nominal\",\n",
    "    \"v_31\": \"Ordinal\",\n",
    "    \"v_15\": \"Ordinal\",\n",
    "    \"v_19\": \"Ordinal\",\n",
    "    \"v_13\": \"Ordinal\",\n",
    "    \"v_33\": \"Ordinal\",\n",
    "    \"v_17\": \"Ordinal\",\n",
    "    \"v_29\": \"Ordinal\",\n",
    "    \"v_23\": \"Ordinal\",\n",
    "    \"v_6\": \"Ordinal\",\n",
    "    \"v_24\": \"Ordinal\",\n",
    "    \"v_10\": \"Ordinal\",\n",
    "    \"v_5\": \"Ordinal\",\n",
    "    \"v_22\": \"Ordinal\",\n",
    "    \"v_0\": \"Ordinal\",\n",
    "    \"v_16\": \"Ratio\",\n",
    "    \"v_2\": \"Ratio\",\n",
    "    \"v_8\": \"Ratio\",\n",
    "    \"v_7\": \"Ratio\",\n",
    "    \"v_39\": \"Ratio\",\n",
    "    \"v_37\": \"Ratio\",\n",
    "    \"v_38\": \"Ratio\",\n",
    "    \"v_34\": \"Ratio\",\n",
    "    \"v_40\": \"Ratio\",\n",
    "    \"v_36\": \"Ratio\",\n",
    "    \"v_35\": \"Ratio\",\n",
    "}\n",
    "# data = pd.read_csv(\n",
    "#     \"../data/train.csv\",\n",
    "#     index_col=0,\n",
    "# )\n",
    "data__ = pd.read_parquet(\"../data/data_with_ridit.hdfs\", engine=\"fastparquet\")\n",
    "prediction_data = pd.read_parquet(\"../data/test.parquet\", engine=\"fastparquet\")\n",
    "data = pd.read_parquet(\"../data/train.parquet\", engine=\"fastparquet\")\n",
    "ordinal = [i for i in dtype_info if dtype_info[i] == \"Ordinal\"]\n",
    "nominal = [i for i in dtype_info if dtype_info[i] == \"Nominal\"]\n",
    "binary = [i for i in dtype_info if dtype_info[i] == \"Binary\"]\n",
    "ratio = [i for i in dtype_info if dtype_info[i] == \"Ratio\"]\n",
    "final_data = pd.read_parquet(\"../data/final_data.parquet\", engine=\"fastparquet\")\n",
    "final_pred_data = pd.read_parquet(\n",
    "    \"../data/final_pred_data.parquet\", engine=\"fastparquet\"\n",
    ")\n",
    "baseline_prediction_data = pd.read_parquet('../data/baseline.parquet')\n",
    "\n",
    "\n",
    "def categorise_data(data):\n",
    "    ordinal_data = data.loc[:, ordinal]\n",
    "    nominal_data = data.loc[:, nominal]\n",
    "    binary_data = data.loc[:, binary]\n",
    "    ratio_data = data.loc[:, ratio]\n",
    "    return ordinal_data, nominal_data, binary_data, ratio_data\n",
    "\n",
    "\n",
    "ordinal_data, nominal_data, binary_data, ratio_data = categorise_data(final_data)\n",
    "baseline_prediction_data.rename(columns={'label':'target'}, inplace=True)\n",
    "final_data.rename(columns={'label':'target'}, inplace=True)\n",
    "data.rename(columns={'label':'target'}, inplace=True)\n",
    "# baseline_prediction_data\n",
    "\n",
    "bs = pd.concat([final_data,baseline_prediction_data], axis=0,ignore_index=True)\n",
    "\n",
    "def gen_balanced_trained_test(data, p):\n",
    "    Y = data.target\n",
    "    X_2 = Y_2 = Y[Y == 2].index\n",
    "    X_0 = Y_0 = Y[Y == 0].index\n",
    "    X_1 = Y_1 = Y[Y == 1].index\n",
    "    train_size = int(p * Y_2.shape[0])\n",
    "    test_size = int((1 - p) * Y_2.shape[0])\n",
    "\n",
    "    train_idx_2 = np.random.choice(Y_2, (train_size,))\n",
    "    train_idx_1 = np.random.choice(Y_1, (train_size,))\n",
    "    train_idx_0 = np.random.choice(Y_0, (train_size,))\n",
    "    train_idx = np.r_[train_idx_0, train_idx_1, train_idx_2]\n",
    "    # train_idx.shape\n",
    "\n",
    "    test_idx_2 = np.random.choice(np.setdiff1d(Y_2, train_idx_2), (test_size,))\n",
    "    test_idx_1 = np.random.choice(np.setdiff1d(Y_1, train_idx_1), (test_size,))\n",
    "    test_idx_0 = np.random.choice(np.setdiff1d(Y_0, train_idx_0), (test_size,))\n",
    "    test_idx = np.r_[test_idx_0, test_idx_1, test_idx_2]\n",
    "    # test_idx.shape\n",
    "    return train_idx, test_idx\n",
    "def gen_nominal_maps(bs: pd.DataFrame = bs) -> tuple[defaultdict, defaultdict]:\n",
    "    nominal_master_db = bs.loc[:, nominal]\n",
    "    nominal_cont_map=defaultdict(dict)\n",
    "    nominal_indvl_map=defaultdict(dict)\n",
    "    for c in nominal:\n",
    "        un = sorted(nominal_master_db[c].unique().tolist())\n",
    "        n = len(un)\n",
    "        new_id = list(range(n))\n",
    "        nominal_indvl_map[c] = dict(zip(un,new_id))\n",
    "    start = 0\n",
    "    for c in nominal:\n",
    "        un = sorted(nominal_master_db[c].unique().tolist())\n",
    "        n = len(un)\n",
    "        new_id = list(range(start,start+n))\n",
    "        nominal_cont_map[c] = dict(zip(un,new_id))\n",
    "        start += n\n",
    "    return nominal_indvl_map, nominal_cont_map\n",
    "\n",
    "nominal_indvl_map, nominal_cont_map = gen_nominal_maps() \n",
    "\n",
    "nominal_master_db_indvl = nominal_master_db.copy()\n",
    "nominal_master_db_cont = nominal_master_db.copy()\n",
    "\n",
    "\n",
    "# nominal_indvl_map\n",
    "def nm_indvl_data_trnsform(row):\n",
    "    for c in nominal:\n",
    "        curr = row[c]\n",
    "        row[c] = nominal_indvl_map[c][curr]\n",
    "    return row\n",
    "\n",
    "\n",
    "test1_nominal = nominal_master_db_indvl.apply(nm_indvl_data_trnsform,axis=1)\n",
    "\n",
    "\n",
    "def nm_cont_data_trnsform(row):\n",
    "    for c in nominal:\n",
    "        curr = row[c]\n",
    "        row[c] = nominal_cont_map[c][curr]\n",
    "    return row\n",
    "\n",
    "\n",
    "test2_nominal = nominal_master_db_cont.apply(nm_cont_data_trnsform,axis=1)\n",
    "\n",
    "\n",
    "def best_n_features(n, data=pd.DataFrame(), target=pd.DataFrame):\n",
    "    if (data.shape[0]>0) and (target.shape[0]>0):\n",
    "        mic = mutual_info_classif(data, target, discrete_features=True)\n",
    "    else:\n",
    "        default_data = pd.read_parquet('../data/final_data.parquet', engine='fastparquet')\n",
    "        data_X = default_data.drop(['target'], axis=1)\n",
    "        data_y = default_data.target\n",
    "    return ohe.get_feature_names_out()[mic.argsort()[-n:]]\n",
    "\n",
    "\n",
    "# prediction_data = pd.read_pickle(\"../data/pred_data.pkl\")\n",
    "# est_ = [(\"cnb\",CategoricalNB()),]\n",
    "def iv_woe(data, target, bins=10, show_woe=False):\n",
    "    \"\"\"\n",
    "    :params data: pandas.DataFrame\n",
    "    :params target: str\n",
    "    :params bins: int\n",
    "    :params show_woe: bool\n",
    "    :returns newDF: pandas.DataFrame, woeDF: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    # Empty Dataframe\n",
    "    newDF, woeDF = pd.DataFrame(), pd.DataFrame()\n",
    "    # Extract Column Names\n",
    "    cols = data.columns\n",
    "    # Run WOE and IV on all the independent variables\n",
    "    for ivars in cols[~cols.isin([target])]:\n",
    "        if (data[ivars].dtype.kind in \"bifc\") and (len(np.unique(data[ivars])) > 1000):\n",
    "            binned_x = pd.qcut(data[ivars], bins, duplicates=\"drop\")\n",
    "            d0 = pd.DataFrame({\"x\": binned_x, \"y\": data[target]})\n",
    "        else:\n",
    "            d0 = pd.DataFrame({\"x\": data[ivars], \"y\": data[target]})\n",
    "        d0 = d0.astype({\"x\": str})\n",
    "        d = d0.groupby(\"x\", as_index=False, dropna=False).agg({\"y\": [\"count\", \"sum\"]})\n",
    "        d.columns = [\"Cutoff\", \"N\", \"Events\"]\n",
    "        d[\"% of Events\"] = np.maximum(d[\"Events\"], 0.5) / d[\"Events\"].sum()\n",
    "        d[\"Non-Events\"] = d[\"N\"] - d[\"Events\"]\n",
    "        d[\"% of Non-Events\"] = np.maximum(d[\"Non-Events\"], 0.5) / d[\"Non-Events\"].sum()\n",
    "        d[\"WoE\"] = np.log(d[\"% of Non-Events\"] / d[\"% of Events\"])\n",
    "        d[\"IV\"] = d[\"WoE\"] * (d[\"% of Non-Events\"] - d[\"% of Events\"])\n",
    "        d.insert(loc=0, column=\"Variable\", value=ivars)\n",
    "        print(\"Information value of \" + ivars + \" is \" + str(round(d[\"IV\"].sum(), 6)))\n",
    "        temp = pd.DataFrame(\n",
    "            {\"Variable\": [ivars], \"IV\": [d[\"IV\"].sum()]}, columns=[\"Variable\", \"IV\"]\n",
    "        )\n",
    "        newDF = pd.concat([newDF, temp], axis=0)\n",
    "        woeDF = pd.concat([woeDF, d], axis=0)\n",
    "        # Show WOE Table\n",
    "        if show_woe == True:\n",
    "            print(d)\n",
    "    return newDF, woeDF\n",
    "\n",
    "def wf_create(cat_encoder=TargetEncoder, model=None):\n",
    "    \"\"\"\n",
    "    :param cat_encoder: category_encoders\n",
    "    :param model: scikit-learn Model\n",
    "    :return pipe: sklearn.pipeline.Pipline\n",
    "    Examples of model param:\n",
    "        \n",
    "    model = ComplementNB(norm=True,fit_prior=True,)\n",
    "    model = MultinomialNB()\n",
    "    model = LogisticRegression(n_jobs=-1, max_iter=10000,random_state=19)\n",
    "    \"\"\"\n",
    "    _steps = []\n",
    "    encoder__name = cat_encoder.__class__.__name__\n",
    "    _steps.append(('PW' + encoder__name, PolynomialWrapper(feature_encoder=cat_encoder)))\n",
    "    if model is None:\n",
    "        pass\n",
    "    else:\n",
    "        model__name = model.__class__.__name__\n",
    "        _steps.append((model__name, model))\n",
    "    pipe = Pipeline(steps= _steps)\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def plot_m_variance(d_coll: list[tuple[float, float, float, float] ]):\n",
    "    \"\"\"\n",
    "    :param d_coll: list[tuple[float, float, float, float]\n",
    "    list of data points in the format\n",
    "    (abcissa, mean, std, max)\n",
    "    \"\"\"\n",
    "    sns.set()\n",
    "    ddx= [x for x,y,u,r in d_coll]\n",
    "    ddc = [y for x,y,u,r in d_coll]\n",
    "    ddep = [y+u for x,y,u,r in d_coll]\n",
    "    dden = [y-u for x,y,u,r in d_coll]\n",
    "    ddem = [r for x,y,u,r in d_coll]\n",
    "    plt.plot(ddx,ddc,'b',label=\"\\u00b5\")\n",
    "    plt.plot(ddx,ddep,'r',label=\"\\u03c3\"+\"+\")\n",
    "    plt.plot(ddx,dden,'g',label=\"\\u03c3\"+\"-\")\n",
    "    plt.plot(ddx,ddem,'y',label=\"\\u03c3\"+\"max\")\n",
    "    fig=plt.fill_between(ddx,ddep,dden,alpha=0.5)\n",
    "    fig = plt.legend()\n",
    "# print(pipe)\n",
    "# wf_create(TargetEncoder(),RandomForestClassifier())\n",
    "final_data_indvl = final_data.copy()\n",
    "final_pred_data_indvl_test = final_pred_data.copy()\n",
    "final_data_indvl.loc[:,nominal] = test1_nominal.loc[final_data.index,:]\n",
    "final_pred_data_indvl_test.loc[:,nominal] = test1_nominal.loc[3796:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40214c99-a6f7-40c8-93dd-12183d1e4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(TargetEncoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b428290-d847-44c9-a9aa-7acfa29177aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "mdb = MongoClient()\n",
    "# mdb.list_database_names()\n",
    "class evaluate_model:\n",
    "    collector = mdb.ml_results.cv_results\n",
    "    \n",
    "    \n",
    "    def __init__(self, model, data=pd.DataFrame(), features=[], target = None):\n",
    "        self.model = model\n",
    "        if data.shape[0]>0:\n",
    "            self.data = data\n",
    "        else:\n",
    "            self.data = pd.read_parquet('../data/final_data.parquet', engine='fastparquet')\n",
    "        if len(features)>0:\n",
    "            self.features = features\n",
    "        else:\n",
    "            self.features = list(self.data.columns)\n",
    "        self.train_idx, self.test_idx = gen_balanced_trained_test(self.data, 1.0)\n",
    "        self.X_train = self.data.loc[self.train_idx, self.features]\n",
    "        self.y_train = self.data.target.loc[self.train_idx]\n",
    "        # label_encoder = \n",
    "        self.cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3)\n",
    "        self.cv_results = None\n",
    "        self.legacy=0\n",
    "        \n",
    "        \n",
    "    def run(self, custom_model=None):\n",
    "        if custom_model:\n",
    "            selected_model = custom_model\n",
    "        else:\n",
    "            selected_model = self.model\n",
    "        with parallel_backend('threading'):\n",
    "            cv_results = cross_validate(selected_model,\n",
    "                                        self.X_train,\n",
    "                                        self.y_train,\n",
    "                                        cv=self.cv,\n",
    "                                        n_jobs=-1,\n",
    "                                        scoring=['f1_micro',\n",
    "                                                 'f1_macro',\n",
    "                                                 'f1_weighted',\n",
    "                                                 'precision_micro',\n",
    "                                                 'precision_macro',\n",
    "                                                 'precision_weighted',\n",
    "                                                 'recall_micro',\n",
    "                                                 'recall_macro',\n",
    "                                                 'recall_weighted'],return_train_score=True,return_estimator=True)\n",
    "                                        # error_score='raise',return_estimator=True)\n",
    "                                    ## Save this results\n",
    "        self.cv_results = cv_results\n",
    "        # cv_db.insert_one(cv_results)\n",
    "        # cnst = {'estimator_params': selected_model.get_params()}\n",
    "        record =  {}\n",
    "        record['estimator_params'] = str(selected_model.get_params())\n",
    "        \n",
    "        record['model_name'] = selected_model.__class__.__name__\n",
    "        record['features'] = list(self.features)\n",
    "        # record['data_idx'] = {'train_idx': list(self.train_idx),'test_idx': list(self.test_idx)}\n",
    "        # cnst['fit_time'] = list(cv_results['fit_time'])\n",
    "        for k,v in cv_results.items():\n",
    "            if k in ['estimator','fit_time','score_time']:\n",
    "                continue\n",
    "            else:\n",
    "                record[k] = v.mean()\n",
    "        # for _ in cv_db.find():\n",
    "        #     print(_.keys())\n",
    "        # print(record)\n",
    "        record['is_legacy_run'] = self.legacy\n",
    "        record['legacy_report'] = None\n",
    "        if self.legacy ==1:\n",
    "            # print(self.legacy_run())\n",
    "            l_rpt = self.legacy_run()\n",
    "            for k, v in l_rpt.items():\n",
    "                print(k)\n",
    "                print('~'*20)\n",
    "                print(v)\n",
    "                print(\"#\"*90)\n",
    "            record['legacy_report'] = l_rpt\n",
    "        \n",
    "        collector.insert_one(record)\n",
    "        \n",
    "        return cv_results\n",
    "    \n",
    "    \n",
    "    def plot(self):\n",
    "        sns.set()\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(18, 15))        \n",
    "        # fig.layout='constrained'\n",
    "\n",
    "        metrics = ['f1_micro', 'f1_macro', 'f1_weighted', 'precision_micro', 'precision_macro', 'precision_weighted', 'recall_micro', 'recall_macro', 'recall_weighted']\n",
    "        # for m  in metrics:\n",
    "        #     n = len(self.cv_results['test_'+m])\n",
    "        idx=0\n",
    "        for i in range(3):\n",
    "            n = len(self.cv_results['test_'+ metrics[0]])\n",
    "            for j in range(3):\n",
    "                axes[i,j].plot(range(n),self.cv_results['test_'+metrics[idx]],label=['test_'+metrics[idx]])\n",
    "                # axes[i,j].plot(range(n),self.cv_results['train_'+metrics[idx]],label=['train_'+metrics[idx]])\n",
    "                # axes[i,j].fill_between(range(n),self.cv_results['test_'+metrics[idx]],self.cv_results['train_'+metrics[idx]])\n",
    "                axes[i,j].legend()\n",
    "                axes[i,j].set_title(metrics[idx])\n",
    "                # axes[i,j].autoscale(enable=False)\n",
    "                idx+=1\n",
    "        plt.legend()\n",
    "    \n",
    "    \n",
    "    def plot_metric(self, metric):\n",
    "        ydata = self.cv_results['test_'+metric]\n",
    "        n = ydata.shape[0]\n",
    "        ffig = plt.plot(np.arange(n),ydata,)\n",
    "        fig = sns.regplot(x=np.arange(n), y=ydata)\n",
    "        plt.title(metric.upper())\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def switch_to_main_data(self):\n",
    "        if self.legacy==1:\n",
    "            print(\"#\"*100)\n",
    "            print(\"ALREADY ON LEGACY MODE\")\n",
    "            print(\"#\"*100)\n",
    "            return\n",
    "        self.legacy = 1\n",
    "        self.X_train_old = self.X_train\n",
    "        self.y_train_old = self.y_train\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.data.loc[:,self.features], self.data.target)\n",
    "        print(\"#\"*100)\n",
    "        print(\"SWITCHING TO LEGACY MODE\")\n",
    "        print(\"#\"*100)\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        if self.legacy==0:\n",
    "            print(\"#\"*100)\n",
    "            print(\"NOT ON LEGACY MODE\")\n",
    "            print(\"#\"*100)\n",
    "            return\n",
    "        self.legacy = 0\n",
    "        self.X_train = self.X_train_old\n",
    "        self.y_train = self.y_train_old\n",
    "    \n",
    "    \n",
    "    def legacy_run(self):\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        self.y_pred = self.model.predict(self.X_test)\n",
    "        self.legacy_report = {}\n",
    "        self.legacy_report['classification_report'] = classification_report(self.y_test, self.y_pred)\n",
    "        self.legacy_report['balanced_accuracy'] = sklearn.metrics.balanced_accuracy_score(self.y_test, self.y_pred)\n",
    "        return self.legacy_report\n",
    "\n",
    "    \n",
    "    def print_last_run_stat(self):\n",
    "        N = collector.estimated_document_count()\n",
    "        idx = 0\n",
    "        for c in collector.find(skip=N-1):\n",
    "            for k,v in c.items():\n",
    "                if 'f1_' in k and 'test' in k:\n",
    "                    print(k)\n",
    "                    print('~'*5)\n",
    "                    print(v)\n",
    "                    print('-'*20)\n",
    "            print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765191c8-79d5-49cb-8a68-6f68cfcbf6dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_selected= {0:[],\n",
    "                   1:[],\n",
    "                   2:[]}\n",
    "selectors = {0:[],\n",
    "            1:[],\n",
    "            2:[]}\n",
    "feature_selection_df = {0:None,\n",
    "                       1:None,\n",
    "                       2:None}\n",
    "feature_performance = {0:None,\n",
    "                       1:None,\n",
    "                       2:None}\n",
    "improvements = {0:[],\n",
    "                   1:[],\n",
    "                   2:[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248ddd8-918e-4ce4-af1f-32212f2cf2fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "estimator =make_pipeline(VarianceThreshold(),LogisticRegression(solver='liblinear',n_jobs=-1,C=0.03,random_state=2, fit_intercept=False))\n",
    "\n",
    "try:\n",
    "    idx=0\n",
    "    for X, y in tr_nom_db:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(enc_nom_df,y,test_size=0.3,random_state=42)\n",
    "        vt = VarianceThreshold()\n",
    "        print(f'Shape of data {X_train.shape}')\n",
    "        __c = vt.fit_transform(X_train)\n",
    "        print(f'Dim Reduction {X_train.shape[1]-__c.shape[1]}')\n",
    "        \n",
    "        # Baseline\n",
    "        model = model_clone(estimator)\n",
    "        model.fit(X_train,y_train)\n",
    "        acc_init = sklearn.metrics.f1_score(model.predict(X_test),y_test,average='micro')\n",
    "        print(f'baseline f-1 score {acc_init}')\n",
    "        print(f'Running SFS')\n",
    "        with parallel_backend('loky'):\n",
    "            # selector = SequentialFeatureSelector(model_clone(estimator),tol=0.001,n_features_to_select='auto',cv=RepeatedStratifiedKFold(n_splits=3,n_repeats=2),n_jobs=-1,scoring='f1_micro',)\n",
    "            selector = RFEC(model_clone(estimator)[1],step=5,cv=RepeatedStratifiedKFold(n_splits=3,n_repeats=2),n_jobs=-1,scoring='f1_micro',)\n",
    "            selector.fit(enc_nom_df, y)\n",
    "        new_data = selector.transform(enc_nom_df)\n",
    "        print(f'SFS Dim Reduction {X_train.shape[1]-new_data.shape[1]}')\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(new_data,y,test_size=0.3,random_state=42)\n",
    "        model = model_clone(estimator)\n",
    "        model.fit(X_train,y_train)\n",
    "        acc_improved = sklearn.metrics.f1_score(model.predict(X_test),y_test,average='micro')\n",
    "        print(f\"Improved By {acc_improved - acc_init} on {idx} Label\")\n",
    "        improvements[idx].append(acc_improved-acc_init)\n",
    "        print(f'New F-1 Score {acc_improved}')\n",
    "        idx+=1\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped Checking\")\n",
    "    # print(acc_improved)\n",
    "    print(acc_init)\n",
    "        # y_pred = \n",
    "    # features_selected\n",
    "    # # selectors\n",
    "    # # Nominal Feature Performance\n",
    "\n",
    "# for i in range(3):\n",
    "#     feature_selection_df[i] = pd.DataFrame(features_selected[i], columns=nominal)\n",
    "#     feature_performance[i] = feature_selection_df[i].mean(axis=0).round(2).to_dict()\n",
    "# pd.DataFrame(feature_performance).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb66d705-a228-468c-8ccf-adc0c8793378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1767d40c-3a37-43ed-a1b2-1cf0770ec43f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "pd.options.plotting.backend='matplotlib'\n",
    "targets = pd.get_dummies(final_data.target)\n",
    "f_sets_kendall = {}\n",
    "f_sets_p = {}\n",
    "for i in range(3):\n",
    "    g1= final_data_indvl.drop(['target'],axis=1).astype(np.float64).corrwith(pd.get_dummies(final_data.target)[i], method='kendall').sort_values()\n",
    "    g2 = g1[(g1>0) | (g1<0)]\n",
    "    non_lin,lin = g1[g1<0].index, g1[g1>0].index\n",
    "    f_sets_kendall[i]=(non_lin,lin)\n",
    "    g1= final_data_indvl.drop(['target'],axis=1).astype(np.float64).corrwith(pd.get_dummies(final_data.target)[i], method='pearson').sort_values()\n",
    "    g2 = g1[(g1>0) | (g1<0)]\n",
    "    non_lin,lin = g1[g1<0].index, g1[g1>0].index\n",
    "    f_sets_p[i]=(non_lin,lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0247eed7-3d75-4aeb-a9f3-290ac7c0eda8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k,v in f_sets_kendall.items():\n",
    "    nl, l = f_sets_kendall[k]\n",
    "    print(len(nl),len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0989edb7-e07c-4b5f-8fd6-7c969234e83e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nd: pd.DataFrame = final_data.loc[:,nominal].astype('float')\n",
    "# nd.corrwith(final_data.loc[:,binary+ordinal+ratio].astype('float'))\n",
    "final_data_indvl.corr(method='kendall')\n",
    "\n",
    "fi_indvl_corr = final_data_indvl.corr(method='kendall')\n",
    "\n",
    "final_data_indvl = pd.get_dummies(final_data_indvl,columns=nominal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc73b71f-801a-4451-b33c-1d18661ab46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# final_data_indvl.info\n",
    "strg_msk= fi_indvl_corr[(fi_indvl_corr>0.3)|(fi_indvl_corr<-0.3)]\n",
    "strg_msk = strg_msk.fillna(0)\n",
    "# penalize corr with itself\n",
    "for c in strg_msk.columns:\n",
    "    strg_msk.loc[c,c]=0\n",
    "strg_msk=strg_msk.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23015fc3-7426-4507-b173-dcf3f0f32cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import unpatch_sklearn\n",
    "unpatch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fce894-a8ae-4290-9cd7-9fd8b29d6390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX 0.56\n",
    "\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from tqdm import trange\n",
    "\n",
    "# Import Data and One Hot Encode with Drop First\n",
    "final_data_indvl = pd.get_dummies(final_data_indvl,columns=nominal,drop_first=True,sparse=False,dtype=np.float64)\n",
    "mean_test_val_clf=defaultdict(list)\n",
    "# model = NuSVC(nu=0.5,kernel='poly',degree=2,random_state=0)\n",
    "mem = Memory('../data/tmp/')\n",
    "for clf in tqdm(classifiers):\n",
    "    nw_noms = list(set(final_data_indvl.columns) -set(binary)-set(ratio)-set(ordinal)-{'target'})\n",
    "    for _i in range(5,150,50):\n",
    "        feature_agglomeration = FeatureAgglomeration(n_clusters=_i,affinity='euclidean',memory=mem,compute_full_tree=True,linkage='ward',compute_distances=True)\n",
    "        check = feature_agglomeration.fit_transform(final_data_indvl.loc[:,nw_noms].astype(np.float64))\n",
    "        with parallel_backend('loky'):\n",
    "            # rpt = cross_validate(OneVsOneClassifier(NuSVC(nu=0.5,kernel='poly',degree=4,random_state=0)),check,final_data.target,cv=3,n_jobs=-1)\n",
    "            try:\n",
    "                rpt = cross_validate(OneVsOneClassifier(clf),check,final_data.target,cv=3,n_jobs=-1,scoring='f1_micro')\n",
    "            except:\n",
    "                continue\n",
    "            # rpt = cross_validate(OneVsOneClassifier(LogisticRegression(fit_intercept=False,n_jobs=-1,random_state=0)),check,final_data.target,cv=3,n_jobs=-1)\n",
    "        mean_test_val_clf[clf.__class__.__name__].append((_i,rpt['test_score'].mean(),rpt['test_score'].std()))\n",
    "\n",
    "\n",
    "# sns.set()\n",
    "# plot_agg_perf()\n",
    "mean_test_val_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f148eb-8510-4a96-8416-ef796e42699c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAX 0.56\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold,RFECV\n",
    "from sklearn.base import clone as model_clone\n",
    "from joblib.memory import Memory\n",
    "# Pipeline\n",
    "final_data_indvl = pd.get_dummies(final_data_indvl,columns=nominal,drop_first=True,sparse=False,dtype=np.float64)\n",
    "nw_noms = list(set(final_data_indvl.columns) -set(binary)-set(ratio)-set(ordinal)-{'target'})\n",
    "mem = Memory('../data/tmp3/')\n",
    "mean_test_val_clf=defaultdict(list)\n",
    "for clf in tqdm(classifiers):\n",
    "    pipe = Pipeline(steps=[('variancethreshold', VarianceThreshold()),\n",
    "                    ('rfecv',RFE(estimator=clf)),\n",
    "                    (clf.__class__.__name__,clf)])\n",
    "    with parallel_backend('loky'):\n",
    "        try:\n",
    "            rpt = cross_validate(OneVsOneClassifier(pipe,n_jobs=-1),final_data_indvl.loc[:,nw_noms],final_data.target,cv=3,n_jobs=-1,scoring='f1_micro')\n",
    "        except:\n",
    "            continue\n",
    "        # rpt = cross_validate(OneVsOneClassifier(LogisticRegression(fit_intercept=False,n_jobs=-1,random_state=0)),check,final_data.target,cv=3,n_jobs=-1)\n",
    "    mean_test_val_clf[clf.__class__.__name__].append((rpt['test_score'].mean(),rpt['test_score'].std()))\n",
    "mean_test_val_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296365ec-b733-4d0b-b3bb-bef5e710afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_agg_perf(mean_test_val):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    # for i in range(3):\n",
    "    c, mu , err = zip(*mean_test_val)\n",
    "    nerr =np.asarray(mu)+ -1*np.asarray(err)\n",
    "    perr =np.asarray(mu)+ np.asarray(err)\n",
    "    # perr = mu+err\n",
    "    fi =plt.plot(c,perr)\n",
    "    fi = plt.plot(c,mu,label=f'Mean Test Score in Label')\n",
    "    fi = plt.plot(c,nerr)\n",
    "    plt.fill_between(c,perr,nerr,'r',alpha=0.5,color='g',animated=True,cmap='inferno')\n",
    "    tg = plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f192f-e073-4dd6-9c27-4ebd6ae0750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in classifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f5c98-f7de-482e-b6f0-50696e48bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params={'base_score': None,\n",
    "            'booster': 'gbtree',\n",
    "            # 'colsample_bylevel': None,\n",
    "            # 'colsample_bynode': None,\n",
    "            # 'colsample_bytree': None,\n",
    "            'gamma': 0,\n",
    "            # 'gpu_id': None,\n",
    "            'importance_type': 'total_cover',\n",
    "            # 'interaction_constraints': None,\n",
    "            'learning_rate': 0.2,\n",
    "            'eval_metric':'aucpr',\n",
    "            'max_delta_step': None,\n",
    "            # 'max_depth': 0,\n",
    "            'min_child_weight': 1,\n",
    "            # 'missing': np.nan,\n",
    "            'monotone_constraints': None,\n",
    "            'n_estimators': 100,\n",
    "            # 'updater': 'grow_colmaker',\n",
    "            'n_jobs': -1,\n",
    "            'num_parallel_tree': None,\n",
    "            'random_state': 42,\n",
    "            'reg_alpha': None,\n",
    "            'reg_lambda': None,\n",
    "            'scale_pos_weight': None,\n",
    "            'subsample': 0.5,\n",
    "            'tree_method': 'approx',\n",
    "            'use_label_encoder': False,\n",
    "            'validate_parameters': None,\n",
    "            'verbosity': 1}\n",
    "h2 = XGBClassifier(n_jobs=-1)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "X,y = load_iris(return_X_y=True,as_frame=True)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# h1.fit(X,y)\n",
    "with parallel_backend('threading'):\n",
    "    cv_ = cross_validate(h1,X,y,cv=3,error_score='raise',n_jobs=-1)\n",
    "cv_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a10d01-6237-4ff4-979d-2cd3edf56e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c5e9f-a9e5-443e-a2f7-644ebcfbc08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# h1 = XGBClassifier(n_estimators=50,random_state=0,max_depth=30,n_jobs=-1,eval_metric='mlogloss')\n",
    "h1=XGBClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a7051-1875-48fb-8a9d-e4d338c32284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from featboost.feat_boost import FeatBoostClassifier\n",
    "fbc = FeatBoostClassifier([h1,LogisticRegression(random_state=43,tol=0.00000001,max_iter=10000,n_jobs=-1,fit_intercept=False,)],number_of_folds=3,max_number_of_features=50,metric='f1',verbose=2,siso_ranking_size=20)\n",
    "# fbc = FeatBoostClassifier(RandomForestClassifier(n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7803878-5aa4-4c60-a361-fd622980fc41",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with parallel_backend('loky'):\n",
    "    fbc.fit(x__,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c1b77-8618-48d6-b963-7e5678b40dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x__= final_data_indvl.loc[:,nw_noms].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b03d45-75f3-42c2-8093-46c3feb0566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = final_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408f8b0-8f1f-403f-9094-21b00a9ebf2f",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "x_153\n",
    "x_004\n",
    "x_390\n",
    "x_150\n",
    "x_323\n",
    "x_074\n",
    "x_403\n",
    "x_147\n",
    "x_364\n",
    "x_039\n",
    "\n",
    "\n",
    "x_146\n",
    "x_158\n",
    "x_237\n",
    "x_364\n",
    "x_331\n",
    "x_335\n",
    "x_478\n",
    "x_435\n",
    "x_370\n",
    "x_373\n",
    "x_128\n",
    "x_074\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1fd17-d198-4d3b-b5c6-a9619fcfc4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx__ = fbc.transform(x__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e04fd-23a6-4baa-ae1b-b3d2211651c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV \n",
    "lcv = LogisticRegressionCV(Cs=100,\n",
    "    fit_intercept=False,\n",
    "    cv=RepeatedStratifiedKFold(n_repeats=3,n_splits=3),\n",
    "    dual=False,\n",
    "    penalty='l2',\n",
    "    scoring='f1_macro',\n",
    "    solver='lbfgs',\n",
    "    tol=0.0000000001,\n",
    "    max_iter=10000,\n",
    "\n",
    "    # class_weight=None,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    refit=True,\n",
    "    # intercept_scaling=1.0,\n",
    "    multi_class='ovr',\n",
    "    random_state=42,\n",
    "    l1_ratios=np.linspace(0.0001,0.9999,num=10))\n",
    "lcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9aea98-94e6-4a19-929e-6d2e8901d330",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with parallel_backend('loky'):\n",
    "    lcv.fit(x__,final_data.target.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bab11f2-7654-4af6-9bf4-a595643ab507",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcv.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ac816-3d87-4904-8b0f-f37f4bb241fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcv.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca55cd-041a-4b5c-8562-449549b3ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcv.scores_[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2417df4-9028-4194-a2b2-c831562d0b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([ 1.25921561,  2.7317216 , 22.76970255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b68e8-224e-4459-a958-d44d7d1fe89c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autoskl)",
   "language": "python",
   "name": "autoskl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
