{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f0379-dcfe-4789-98e1-095a9661cea9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import (\n",
    "    Binarizer,\n",
    "    StandardScaler,\n",
    "    LabelBinarizer,\n",
    "    OneHotEncoder,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "# from xgboost import XGBRFClassifier, XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# import dtale\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "# sns.set()\n",
    "from sklearn import set_config\n",
    "from sklearn.ensemble import StackingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import (\n",
    "    Product,\n",
    "    RBF,\n",
    "    CompoundKernel,\n",
    "    Exponentiation,\n",
    "    Matern,\n",
    "    Sum,\n",
    ")\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from category_encoders import (\n",
    "    TargetEncoder,\n",
    "    BackwardDifferenceEncoder,\n",
    "    BaseNEncoder,\n",
    "    BinaryEncoder,\n",
    "    CatBoostEncoder,\n",
    "    CatBoostEncoder,\n",
    "    GLMMEncoder,\n",
    "    HelmertEncoder,\n",
    "    JamesSteinEncoder,\n",
    "    PolynomialEncoder,\n",
    "    QuantileEncoder,\n",
    "    SumEncoder,\n",
    "    SummaryEncoder,\n",
    "    WOEEncoder,\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    MinMaxScaler,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\n",
    "import warnings\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import plotly.express as px\n",
    "# from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_validate,\n",
    "    # StratifiedGroupKFold,\n",
    "    StratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    HalvingGridSearchCV,\n",
    "    HalvingRandomSearchCV\n",
    ")\n",
    "import sigopt\n",
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    CategoricalNB,\n",
    "    MultinomialNB,\n",
    "    ComplementNB,\n",
    "    GaussianNB,\n",
    ")\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# nb_est = [CategoricalNB(), MultinomialNB(), ComplementNB(), GaussianNB()]\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.tree import export_graphviz, plot_tree\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_selection import (\n",
    "    mutual_info_classif,\n",
    "    SelectKBest,\n",
    "    f_classif,\n",
    "    chi2,\n",
    "    RFE,\n",
    "    SelectFdr,\n",
    "    SelectFpr,\n",
    "    SelectFwe,\n",
    "    SelectPercentile,\n",
    ")\n",
    "from tbb import Monkey\n",
    "# from xgboost import XGBClassifier\n",
    "import os\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from joblib import parallel_backend\n",
    "from category_encoders.wrapper import PolynomialWrapper, NestedCVWrapper\n",
    "# from autosklearn.automl import AutoMLClassifier\n",
    "import sklearn.metrics\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pd.options.display.max_columns = 50\n",
    "set_config(display=\"diagram\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "dtype_info = {\n",
    "    \"v_1\": \"Binary\",\n",
    "    \"v_26\": \"Binary\",\n",
    "    \"v_11\": \"Binary\",\n",
    "    \"v_14\": \"Binary\",\n",
    "    \"v_30\": \"Binary\",\n",
    "    \"v_28\": \"Binary\",\n",
    "    \"v_9\": \"Binary\",\n",
    "    \"v_27\": \"Binary\",\n",
    "    \"v_32\": \"Nominal\",\n",
    "    \"v_4\": \"Nominal\",\n",
    "    \"v_3\": \"Nominal\",\n",
    "    \"v_20\": \"Nominal\",\n",
    "    \"v_21\": \"Nominal\",\n",
    "    \"v_18\": \"Nominal\",\n",
    "    \"v_25\": \"Nominal\",\n",
    "    \"v_12\": \"Nominal\",\n",
    "    \"v_31\": \"Ordinal\",\n",
    "    \"v_15\": \"Ordinal\",\n",
    "    \"v_19\": \"Ordinal\",\n",
    "    \"v_13\": \"Ordinal\",\n",
    "    \"v_33\": \"Ordinal\",\n",
    "    \"v_17\": \"Ordinal\",\n",
    "    \"v_29\": \"Ordinal\",\n",
    "    \"v_23\": \"Ordinal\",\n",
    "    \"v_6\": \"Ordinal\",\n",
    "    \"v_24\": \"Ordinal\",\n",
    "    \"v_10\": \"Ordinal\",\n",
    "    \"v_5\": \"Ordinal\",\n",
    "    \"v_22\": \"Ordinal\",\n",
    "    \"v_0\": \"Ordinal\",\n",
    "    \"v_16\": \"Ratio\",\n",
    "    \"v_2\": \"Ratio\",\n",
    "    \"v_8\": \"Ratio\",\n",
    "    \"v_7\": \"Ratio\",\n",
    "    \"v_39\": \"Ratio\",\n",
    "    \"v_37\": \"Ratio\",\n",
    "    \"v_38\": \"Ratio\",\n",
    "    \"v_34\": \"Ratio\",\n",
    "    \"v_40\": \"Ratio\",\n",
    "    \"v_36\": \"Ratio\",\n",
    "    \"v_35\": \"Ratio\",\n",
    "}\n",
    "# data = pd.read_csv(\n",
    "#     \"../data/train.csv\",\n",
    "#     index_col=0,\n",
    "# )\n",
    "data__ = pd.read_parquet(\"../data/data_with_ridit.hdfs\", engine=\"fastparquet\")\n",
    "prediction_data = pd.read_parquet(\"../data/test.parquet\", engine=\"fastparquet\")\n",
    "data = pd.read_parquet(\"../data/train.parquet\", engine=\"fastparquet\")\n",
    "ordinal = [i for i in dtype_info if dtype_info[i] == \"Ordinal\"]\n",
    "nominal = [i for i in dtype_info if dtype_info[i] == \"Nominal\"]\n",
    "binary = [i for i in dtype_info if dtype_info[i] == \"Binary\"]\n",
    "ratio = [i for i in dtype_info if dtype_info[i] == \"Ratio\"]\n",
    "final_data = pd.read_parquet(\"../data/final_data.parquet\", engine=\"fastparquet\")\n",
    "final_pred_data = pd.read_parquet(\n",
    "    \"../data/final_pred_data.parquet\", engine=\"fastparquet\"\n",
    ")\n",
    "baseline_prediction_data = pd.read_parquet('../data/baseline.parquet')\n",
    "\n",
    "\n",
    "def categorise_data(data):\n",
    "    ordinal_data = data.loc[:, ordinal]\n",
    "    nominal_data = data.loc[:, nominal]\n",
    "    binary_data = data.loc[:, binary]\n",
    "    ratio_data = data.loc[:, ratio]\n",
    "    return ordinal_data, nominal_data, binary_data, ratio_data\n",
    "\n",
    "\n",
    "ordinal_data, nominal_data, binary_data, ratio_data = categorise_data(final_data)\n",
    "baseline_prediction_data.rename(columns={'label':'target'}, inplace=True)\n",
    "final_data.rename(columns={'label':'target'}, inplace=True)\n",
    "data.rename(columns={'label':'target'}, inplace=True)\n",
    "# baseline_prediction_data\n",
    "\n",
    "bs = pd.concat([final_data,baseline_prediction_data], axis=0,ignore_index=True)\n",
    "nominal_master_db = bs.loc[:, nominal]\n",
    "\n",
    "def gen_train_test(data, p):\n",
    "    Y = data.target\n",
    "    X_2 = Y_2 = Y[Y == 2].index\n",
    "    X_0 = Y_0 = Y[Y == 0].index\n",
    "    X_1 = Y_1 = Y[Y == 1].index\n",
    "    train_size = int(p * Y_2.shape[0])\n",
    "    test_size = int((1 - p) * Y_2.shape[0])\n",
    "\n",
    "    train_idx_2 = np.random.choice(Y_2, (train_size,))\n",
    "    train_idx_1 = np.random.choice(Y_1, (train_size,))\n",
    "    train_idx_0 = np.random.choice(Y_0, (train_size,))\n",
    "    train_idx = np.r_[train_idx_0, train_idx_1, train_idx_2]\n",
    "    # train_idx.shape\n",
    "\n",
    "    test_idx_2 = np.random.choice(np.setdiff1d(Y_2, train_idx_2), (test_size,))\n",
    "    test_idx_1 = np.random.choice(np.setdiff1d(Y_1, train_idx_1), (test_size,))\n",
    "    test_idx_0 = np.random.choice(np.setdiff1d(Y_0, train_idx_0), (test_size,))\n",
    "    test_idx = np.r_[test_idx_0, test_idx_1, test_idx_2]\n",
    "    # test_idx.shape\n",
    "    return train_idx, test_idx\n",
    "\n",
    "nominal_cont_map=defaultdict(dict)\n",
    "nominal_indvl_map=defaultdict(dict)\n",
    "for c in nominal:\n",
    "    un = sorted(nominal_master_db[c].unique().tolist())\n",
    "    n = len(un)\n",
    "    new_id = list(range(n))\n",
    "    nominal_indvl_map[c] = dict(zip(un,new_id))\n",
    "start = 0\n",
    "for c in nominal:\n",
    "    un = sorted(nominal_master_db[c].unique().tolist())\n",
    "    n = len(un)\n",
    "    new_id = list(range(start,start+n))\n",
    "    nominal_cont_map[c] = dict(zip(un,new_id))\n",
    "    start += n\n",
    "\n",
    "\n",
    "nominal_master_db_indvl = nominal_master_db.copy()\n",
    "nominal_master_db_cont = nominal_master_db.copy()\n",
    "\n",
    "\n",
    "# nominal_indvl_map\n",
    "def nm_indvl_data_trnsform(row):\n",
    "    for c in nominal:\n",
    "        curr = row[c]\n",
    "        row[c] = nominal_indvl_map[c][curr]\n",
    "    return row\n",
    "\n",
    "\n",
    "test1_nominal = nominal_master_db_indvl.apply(nm_indvl_data_trnsform,axis=1)\n",
    "\n",
    "\n",
    "def nm_cont_data_trnsform(row):\n",
    "    for c in nominal:\n",
    "        curr = row[c]\n",
    "        row[c] = nominal_cont_map[c][curr]\n",
    "    return row\n",
    "\n",
    "\n",
    "test2_nominal = nominal_master_db_cont.apply(nm_cont_data_trnsform,axis=1)\n",
    "\n",
    "\n",
    "def best_n_features(n, X_train, y_train):\n",
    "    ohe = OneHotEncoder(\n",
    "        min_frequency=0.00001, handle_unknown=\"infrequent_if_exist\", sparse=False\n",
    "    )\n",
    "    X_train_t = ohe.fit_transform(X_train)\n",
    "    mic = mutual_info_classif(X_train_t, y_train, discrete_features=True)\n",
    "\n",
    "    return ohe.get_feature_names_out()[mic.argsort()[-n:]]\n",
    "\n",
    "\n",
    "# prediction_data = pd.read_pickle(\"../data/pred_data.pkl\")\n",
    "# est_ = [(\"cnb\",CategoricalNB()),]\n",
    "def iv_woe(data, target, bins=10, show_woe=False):\n",
    "\n",
    "    # Empty Dataframe\n",
    "    newDF, woeDF = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Extract Column Names\n",
    "    cols = data.columns\n",
    "\n",
    "    # Run WOE and IV on all the independent variables\n",
    "    for ivars in cols[~cols.isin([target])]:\n",
    "        if (data[ivars].dtype.kind in \"bifc\") and (len(np.unique(data[ivars])) > 1000):\n",
    "            binned_x = pd.qcut(data[ivars], bins, duplicates=\"drop\")\n",
    "            d0 = pd.DataFrame({\"x\": binned_x, \"y\": data[target]})\n",
    "        else:\n",
    "            d0 = pd.DataFrame({\"x\": data[ivars], \"y\": data[target]})\n",
    "        d0 = d0.astype({\"x\": str})\n",
    "        d = d0.groupby(\"x\", as_index=False, dropna=False).agg({\"y\": [\"count\", \"sum\"]})\n",
    "        d.columns = [\"Cutoff\", \"N\", \"Events\"]\n",
    "        d[\"% of Events\"] = np.maximum(d[\"Events\"], 0.5) / d[\"Events\"].sum()\n",
    "        d[\"Non-Events\"] = d[\"N\"] - d[\"Events\"]\n",
    "        d[\"% of Non-Events\"] = np.maximum(d[\"Non-Events\"], 0.5) / d[\"Non-Events\"].sum()\n",
    "        d[\"WoE\"] = np.log(d[\"% of Non-Events\"] / d[\"% of Events\"])\n",
    "        d[\"IV\"] = d[\"WoE\"] * (d[\"% of Non-Events\"] - d[\"% of Events\"])\n",
    "        d.insert(loc=0, column=\"Variable\", value=ivars)\n",
    "        print(\"Information value of \" + ivars + \" is \" + str(round(d[\"IV\"].sum(), 6)))\n",
    "        temp = pd.DataFrame(\n",
    "            {\"Variable\": [ivars], \"IV\": [d[\"IV\"].sum()]}, columns=[\"Variable\", \"IV\"]\n",
    "        )\n",
    "        newDF = pd.concat([newDF, temp], axis=0)\n",
    "        woeDF = pd.concat([woeDF, d], axis=0)\n",
    "\n",
    "        # Show WOE Table\n",
    "        if show_woe == True:\n",
    "            print(d)\n",
    "    return newDF, woeDF\n",
    "\n",
    "def wf_create(cat_encoder, model):\n",
    "    encoder__name = cat_encoder.__class__.__name__\n",
    "            # model = ComplementNB(norm=True,fit_prior=True,)\n",
    "    # model = MultinomialNB()\n",
    "    # model = LogisticRegression(n_jobs=-1, max_iter=10000,random_state=19)\n",
    "    model__name = model.__class__.__name__\n",
    "    pipe = Pipeline(steps=[('PW' + encoder__name, PolynomialWrapper(feature_encoder=cat_encoder)),\n",
    "                           (model__name,model )])\n",
    "    report = em.run(pipe) # Store A test log\n",
    "    print('PW' + encoder_name, model__name)\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def plot_m_variance(d_coll):\n",
    "    sns.set()\n",
    "    ddx= [x for x,y,u,r in d_coll]\n",
    "    ddc = [y for x,y,u,r in d_coll]\n",
    "    ddep = [y+u for x,y,u,r in d_coll]\n",
    "    dden = [y-u for x,y,u,r in d_coll]\n",
    "    ddem = [r for x,y,u,r in d_coll]\n",
    "    plt.plot(ddx,ddc,'b',label=\"\\u00b5\")\n",
    "    plt.plot(ddx,ddep,'r',label=\"\\u03c3\"+\"+\")\n",
    "    plt.plot(ddx,dden,'g',label=\"\\u03c3\"+\"-\")\n",
    "    plt.plot(ddx,ddem,'y',label=\"\\u03c3\"+\"max\")\n",
    "    fig=plt.fill_between(ddx,ddep,dden,alpha=0.5)\n",
    "    fig = plt.legend()\n",
    "# print(pipe)\n",
    "# wf_create(TargetEncoder(),RandomForestClassifier())\n",
    "final_data_indvl_test = final_data.copy()\n",
    "final_pred_data_indvl_test = final_pred_data.copy()\n",
    "final_data_indvl_test.loc[:,nominal] = test1_nominal.loc[final_data.index,:]\n",
    "final_pred_data_indvl_test.loc[:,nominal] = test1_nominal.loc[3796:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b428290-d847-44c9-a9aa-7acfa29177aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "mdb = MongoClient()\n",
    "# mdb.list_database_names()\n",
    "class evaluate_model:\n",
    "    collector = mdb.ml_results.cv_results\n",
    "    \n",
    "    \n",
    "    def __init__(self, model, data=pd.DataFrame(), features=[]):\n",
    "        self.model = model\n",
    "        if data.shape[0]>0:\n",
    "            self.data = data\n",
    "        else:\n",
    "            self.data = pd.read_parquet('../data/final_data.parquet', engine='fastparquet')\n",
    "        if len(features)>0:\n",
    "            self.features = features\n",
    "        else:\n",
    "            self.features = list(self.data.columns)\n",
    "        self.train_idx, self.test_idx = gen_train_test(self.data, 1.0)\n",
    "        self.X_train = self.data.loc[self.train_idx, self.features]\n",
    "        self.y_train = self.data.target.loc[self.train_idx]\n",
    "        self.cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=5)\n",
    "        self.cv_results = None\n",
    "        self.legacy=0\n",
    "        \n",
    "        \n",
    "    def run(self, custom_model=None):\n",
    "        if custom_model:\n",
    "            selected_model = custom_model\n",
    "        else:\n",
    "            selected_model = self.model\n",
    "        with parallel_backend('threading'):\n",
    "            cv_results = cross_validate(selected_model,\n",
    "                                        self.X_train,\n",
    "                                        self.y_train,\n",
    "                                        cv=self.cv,\n",
    "                                        n_jobs=-1,\n",
    "                                        scoring=['f1_micro',\n",
    "                                                 'f1_macro',\n",
    "                                                 'f1_weighted',\n",
    "                                                 'precision_micro',\n",
    "                                                 'precision_macro',\n",
    "                                                 'precision_weighted',\n",
    "                                                 'recall_micro',\n",
    "                                                 'recall_macro',\n",
    "                                                 'recall_weighted'],return_train_score=True,return_estimator=True)\n",
    "                                        # error_score='raise',return_estimator=True)\n",
    "                                    ## Save this results\n",
    "        self.cv_results = cv_results\n",
    "        # cv_db.insert_one(cv_results)\n",
    "        # cnst = {'estimator_params': selected_model.get_params()}\n",
    "        record =  {}\n",
    "        record['estimator_params'] = str(selected_model.get_params())\n",
    "        \n",
    "        record['model_name'] = selected_model.__class__.__name__\n",
    "        record['features'] = list(self.features)\n",
    "        # record['data_idx'] = {'train_idx': list(self.train_idx),'test_idx': list(self.test_idx)}\n",
    "        # cnst['fit_time'] = list(cv_results['fit_time'])\n",
    "        for k,v in cv_results.items():\n",
    "            if k in ['estimator','fit_time','score_time']:\n",
    "                continue\n",
    "            else:\n",
    "                record[k] = v.mean()\n",
    "        # for _ in cv_db.find():\n",
    "        #     print(_.keys())\n",
    "        # print(record)\n",
    "        record['is_legacy_run'] = self.legacy\n",
    "        record['legacy_report'] = None\n",
    "        if self.legacy ==1:\n",
    "            # print(self.legacy_run())\n",
    "            l_rpt = self.legacy_run()\n",
    "            for k, v in l_rpt.items():\n",
    "                print(k)\n",
    "                print('~'*20)\n",
    "                print(v)\n",
    "                print(\"#\"*90)\n",
    "            record['legacy_report'] = l_rpt\n",
    "        \n",
    "        collector.insert_one(record)\n",
    "        \n",
    "        return cv_results\n",
    "    \n",
    "    \n",
    "    def plot(self):\n",
    "        sns.set()\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(18, 15))        \n",
    "        # fig.layout='constrained'\n",
    "\n",
    "        metrics = ['f1_micro', 'f1_macro', 'f1_weighted', 'precision_micro', 'precision_macro', 'precision_weighted', 'recall_micro', 'recall_macro', 'recall_weighted']\n",
    "        # for m  in metrics:\n",
    "        #     n = len(self.cv_results['test_'+m])\n",
    "        idx=0\n",
    "        for i in range(3):\n",
    "            n = len(self.cv_results['test_'+ metrics[0]])\n",
    "            for j in range(3):\n",
    "                axes[i,j].plot(range(n),self.cv_results['test_'+metrics[idx]],label=['test_'+metrics[idx]])\n",
    "                # axes[i,j].plot(range(n),self.cv_results['train_'+metrics[idx]],label=['train_'+metrics[idx]])\n",
    "                # axes[i,j].fill_between(range(n),self.cv_results['test_'+metrics[idx]],self.cv_results['train_'+metrics[idx]])\n",
    "                axes[i,j].legend()\n",
    "                axes[i,j].set_title(metrics[idx])\n",
    "                # axes[i,j].autoscale(enable=False)\n",
    "                idx+=1\n",
    "        plt.legend()\n",
    "    \n",
    "    \n",
    "    def plot_metric(self, metric):\n",
    "        ydata = self.cv_results['test_'+metric]\n",
    "        n = ydata.shape[0]\n",
    "        ffig = plt.plot(np.arange(n),ydata,)\n",
    "        fig = sns.regplot(x=np.arange(n), y=ydata)\n",
    "        plt.title(metric.upper())\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def switch_to_main_data(self):\n",
    "        if self.legacy==1:\n",
    "            print(\"#\"*100)\n",
    "            print(\"ALREADY ON LEGACY MODE\")\n",
    "            print(\"#\"*100)\n",
    "            return\n",
    "        self.legacy = 1\n",
    "        self.X_train_old = self.X_train\n",
    "        self.y_train_old = self.y_train\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.data.loc[:,self.features], self.data.target)\n",
    "        print(\"#\"*100)\n",
    "        print(\"SWITCHING TO LEGACY MODE\")\n",
    "        print(\"#\"*100)\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        if self.legacy==0:\n",
    "            print(\"#\"*100)\n",
    "            print(\"NOT ON LEGACY MODE\")\n",
    "            print(\"#\"*100)\n",
    "            return\n",
    "        self.legacy = 0\n",
    "        self.X_train = self.X_train_old\n",
    "        self.y_train = self.y_train_old\n",
    "    \n",
    "    \n",
    "    def legacy_run(self):\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        self.y_pred = self.model.predict(self.X_test)\n",
    "        self.legacy_report = {}\n",
    "        self.legacy_report['classification_report'] = classification_report(self.y_test, self.y_pred)\n",
    "        self.legacy_report['balanced_accuracy'] = sklearn.metrics.balanced_accuracy_score(self.y_test, self.y_pred)\n",
    "        return self.legacy_report\n",
    "\n",
    "    \n",
    "    def print_last_run_stat(self):\n",
    "        N = collector.estimated_document_count()\n",
    "        idx = 0\n",
    "        for c in collector.find(skip=N-1):\n",
    "            for k,v in c.items():\n",
    "                if 'f1_' in k and 'test' in k:\n",
    "                    print(k)\n",
    "                    print('~'*5)\n",
    "                    print(v)\n",
    "                    print('-'*20)\n",
    "            print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765191c8-79d5-49cb-8a68-6f68cfcbf6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_selected= {0:[],\n",
    "                   1:[],\n",
    "                   2:[]}\n",
    "selectors = {0:[],\n",
    "            1:[],\n",
    "            2:[]}\n",
    "feature_selection_df = {0:None,\n",
    "                       1:None,\n",
    "                       2:None}\n",
    "feature_performance = {0:None,\n",
    "                       1:None,\n",
    "                       2:None}\n",
    "improvements = {0:[],\n",
    "                   1:[],\n",
    "                   2:[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee7968-881c-44d2-8706-f2504b31156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False,)\n",
    "encoded_nominal = ohe.fit_transform(bs.loc[:,nominal])\n",
    "# encoded_nominal[]\n",
    "enc_nom_df = pd.DataFrame(encoded_nominal,columns=ohe.get_feature_names_out())\n",
    "# final_data.shape\n",
    "enc_nom_df = enc_nom_df.loc[:3795, :]\n",
    "enc_nom_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248ddd8-918e-4ce4-af1f-32212f2cf2fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearnex.cluster import DBSCAN, KMeans     \n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.base import clone as model_clone\n",
    "from sklearnex.svm import NuSVC,SVR\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "SVR\n",
    "\n",
    "\n",
    "        # model = \n",
    "estimator =make_pipeline(VarianceThreshold(),LogisticRegression(solver='liblinear',n_jobs=-1,C=0.03,random_state=2, fit_intercept=False))\n",
    "# RFECV\n",
    "# features_selected = defaultdict(list)\n",
    "# selectors = []\n",
    "# for _ in range(5):\n",
    "try:\n",
    "    idx=0\n",
    "    for X, y in tr_nom_db:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(enc_nom_df,y,test_size=0.3,random_state=42)\n",
    "        vt = VarianceThreshold()\n",
    "        print(f'Shape of data {X_train.shape}')\n",
    "        __c = vt.fit_transform(X_train)\n",
    "        print(f'Dim Reduction {X_train.shape[1]-__c.shape[1]}')\n",
    "        \n",
    "        # Baseline\n",
    "        model = model_clone(estimator)\n",
    "        model.fit(X_train,y_train)\n",
    "        acc_init = sklearn.metrics.f1_score(model.predict(X_test),y_test,average='micro')\n",
    "        print(f'baseline f-1 score {acc_init}')\n",
    "        print(f'Running SFS')\n",
    "        with parallel_backend('loky'):\n",
    "            # selector = SequentialFeatureSelector(model_clone(estimator),tol=0.001,n_features_to_select='auto',cv=RepeatedStratifiedKFold(n_splits=3,n_repeats=2),n_jobs=-1,scoring='f1_micro',)\n",
    "            selector = RFECV(model_clone(estimator)[1],step=5,cv=RepeatedStratifiedKFold(n_splits=3,n_repeats=2),n_jobs=-1,scoring='f1_micro',)\n",
    "            selector.fit(enc_nom_df, y)\n",
    "        new_data = selector.transform(enc_nom_df)\n",
    "        print(f'SFS Dim Reduction {X_train.shape[1]-new_data.shape[1]}')\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(new_data,y,test_size=0.3,random_state=42)\n",
    "        model = model_clone(estimator)\n",
    "        model.fit(X_train,y_train)\n",
    "        acc_improved = sklearn.metrics.f1_score(model.predict(X_test),y_test,average='micro')\n",
    "        print(f\"Improved By {acc_improved - acc_init} on {idx} Label\")\n",
    "        improvements[idx].append(acc_improved-acc_init)\n",
    "        print(f'New F-1 Score {acc_improved}')\n",
    "        idx+=1\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped Checking\")\n",
    "    # print(acc_improved)\n",
    "    print(acc_init)\n",
    "        # y_pred = \n",
    "    # features_selected\n",
    "    # # selectors\n",
    "    # # Nominal Feature Performance\n",
    "\n",
    "# for i in range(3):\n",
    "#     feature_selection_df[i] = pd.DataFrame(features_selected[i], columns=nominal)\n",
    "#     feature_performance[i] = feature_selection_df[i].mean(axis=0).round(2).to_dict()\n",
    "# pd.DataFrame(feature_performance).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb66d705-a228-468c-8ccf-adc0c8793378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1767d40c-3a37-43ed-a1b2-1cf0770ec43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data_indvl_test\n",
    "# One Hot Encoding of All the Categories\n",
    "# Variance Threshold removal\n",
    "sns.set()\n",
    "# fig,ax = plt.subplots(3,1,squeeze=False,)\n",
    "# final_data.plot()\n",
    "# plt.Figure(fig_size=(30,20))\n",
    "pd.options.plotting.backend='matplotlib'\n",
    "# final_data_indvl_test = pd.get_dummies(final_data_indvl_test,columns=nominal)\n",
    "targets = pd.get_dummies(final_data.target)\n",
    "f_sets_kendall = {}\n",
    "f_sets_p = {}\n",
    "for i in range(3):\n",
    "    # plt.figure(figsize=(30,30))\n",
    "    \n",
    "    g1= final_data_indvl_test.drop(['target'],axis=1).astype(np.float64).corrwith(pd.get_dummies(final_data.target)[i], method='kendall').sort_values()\n",
    "    g2 = g1[(g1>0) | (g1<0)]\n",
    "    non_lin,lin = g1[g1<0].index, g1[g1>0].index\n",
    "    f_sets_kendall[i]=(non_lin,lin)\n",
    "    g1= final_data_indvl_test.drop(['target'],axis=1).astype(np.float64).corrwith(pd.get_dummies(final_data.target)[i], method='pearson').sort_values()\n",
    "    g2 = g1[(g1>0) | (g1<0)]\n",
    "    non_lin,lin = g1[g1<0].index, g1[g1>0].index\n",
    "    f_sets_p[i]=(non_lin,lin)\n",
    "    \n",
    "# g1= final_data_indvl_test[final_data_indvl_test.target == i ].drop(['target'],axis=1).astype(np.float64).var().sort_values()\n",
    "# g2 = baseline_prediction_data[baseline_prediction_data.target == i ].drop(['target'],axis=1).astype(np.float64).var().sort_values()\n",
    "# g1.loc[g2.index].plot(stacked=False,colormap='inferno',title=f'\\u03c3 of Training Data_Features {i}', fontsize=11,figsize=(30,20),kind='line',alpha=1.0,ax=ax[i][0])\n",
    "# ax[i][0].vl\n",
    "# plt.hlines\n",
    "# ax[i][0].hlines(0.0001,0.001,-0.001,color=\"red\")\n",
    "# g2.plot(stacked=True,colormap='flare',title=f'\\u03c3 of Testing Data_Features {i}', fontsize=11,figsize=(30,20),kind='bar',alpha=1.0,ax=ax[i][1])\n",
    "# plt.legend()\n",
    "\n",
    "\n",
    "# final_data_indvl_test.drop(['target'],axis=1).astype(np.float64).var().sort_values().plot(color='r',label='\\u03c3 of Training Data_Features', fontsize=11,figsize=(30,10),kind='bar',alpha=1.0)\n",
    "# baseline_prediction_data.drop(['target'],axis=1).astype(np.float64).var().sort_values().plot(color='g',label='\\u03c3 of Unknown Data_Features', fontsize=11,figsize=(30,10),kind='bar',alpha=0.3)\n",
    "# plt.autoscale(enable=True)\n",
    "# plt.xlabel(final_data_indvl_test.drop(['target'],axis=1).columns)\n",
    "# plt.xticks(list(final_data_indvl_test.drop(['target'],axis=1).columns))\n",
    "# plt.xcorr(final_data_indvl_test.drop(['target'],axis=1).astype(np.float64).var(),baseline_prediction_data.drop(['target'],axis=1).astype(np.float64).var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0247eed7-3d75-4aeb-a9f3-290ac7c0eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in f_sets_kendall.items():\n",
    "    nl, l = f_sets_kendall[k]\n",
    "    print(len(nl),len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab5411f-c535-4f2d-8ae2-35a8f45e0a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nd: pd.DataFrame = final_data.loc[:,nominal].astype('float')\n",
    "# nd.corrwith(final_data.loc[:,binary+ordinal+ratio].astype('float'))\n",
    "final_data_indvl_test.corr(method='kendall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b5a68-1adb-4359-9281-243886194372",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_indvl = final_data_indvl_test.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9861f5ba-2bcc-4bb9-948b-366082e718db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_indvl_corr = final_data_indvl.corr(method='kendall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b41f5-c2de-4dc5-8797-4b41ae15d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_indvl = pd.get_dummies(final_data_indvl,columns=nominal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc73b71f-801a-4451-b33c-1d18661ab46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data_indvl.info\n",
    "strg_msk= fi_indvl_corr[(fi_indvl_corr>0.3)|(fi_indvl_corr<-0.3)]\n",
    "strg_msk = strg_msk.fillna(0)\n",
    "# penalize corr with itself\n",
    "for c in strg_msk.columns:\n",
    "    strg_msk.loc[c,c]=0\n",
    "strg_msk=strg_msk.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fce894-a8ae-4290-9cd7-9fd8b29d6390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from joblib.memory import Memory\n",
    "from tqdm import trange\n",
    "\n",
    "mean_test_val=[]\n",
    "nw_noms = set(final_data_indvl.columns) -set(binary)-set(ratio)-set(ordinal)\n",
    "nw_noms = nw_noms -{'target'}\n",
    "nw_noms = list(nw_noms)\n",
    "mem = Memory('../data/tmp/')\n",
    "for _i in trange(3,300,15):\n",
    "    f_agg = FeatureAgglomeration(n_clusters=_i,affinity='euclidean',memory=mem,compute_full_tree=True,linkage='ward',compute_distances=True)\n",
    "    check = f_agg.fit_transform(final_data_indvl.loc[:,nw_noms].astype(np.float64))\n",
    "#     for i in range(3):\n",
    "    with parallel_backend('loky'):\n",
    "#         rpt = cross_validate(OneVsOneClassifier(NuSVC(nu=0.5,kernel='poly',degree=4,random_state=0)),check,final_data.target,cv=3,n_jobs=-1)\n",
    "        rpt = cross_validate(OneVsOneClassifier(LogisticRegression(fit_intercept=False,n_jobs=-1,random_state=0)),check,final_data.target,cv=3,n_jobs=-1)\n",
    "    mean_test_val.append((_i,rpt['test_score'].mean(),rpt['test_score'].std()))\n",
    "\n",
    "def plot_agg_perf(mean_test_val=mean_test_val):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    # for i in range(3):\n",
    "    c, mu , err = zip(*mean_test_val)\n",
    "    nerr =np.asarray(mu)+ -1*np.asarray(err)\n",
    "    perr =np.asarray(mu)+ np.asarray(err)\n",
    "    # perr = mu+err\n",
    "    fi =plt.plot(c,perr)\n",
    "    fi = plt.plot(c,mu,label=f'Mean Test Score in Label')\n",
    "    fi = plt.plot(c,nerr)\n",
    "    plt.fill_between(c,perr,nerr,)\n",
    "    tg = plt.legend(loc='lower right')\n",
    "plot_agg_perf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f5c98-f7de-482e-b6f0-50696e48bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from featboost.feat_boost import FeatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "xgb_params={'base_score': None,\n",
    "            'booster': 'gbtree',\n",
    "            # 'colsample_bylevel': None,\n",
    "            # 'colsample_bynode': None,\n",
    "            # 'colsample_bytree': None,\n",
    "            'gamma': 0,\n",
    "            # 'gpu_id': None,\n",
    "            'importance_type': 'total_cover',\n",
    "            # 'interaction_constraints': None,\n",
    "            'learning_rate': 0.2,\n",
    "            'eval_metric':'aucpr',\n",
    "            'max_delta_step': None,\n",
    "            # 'max_depth': 0,\n",
    "            'min_child_weight': 1,\n",
    "            # 'missing': np.nan,\n",
    "            'monotone_constraints': None,\n",
    "            'n_estimators': 100,\n",
    "            # 'updater': 'grow_colmaker',\n",
    "            'n_jobs': -1,\n",
    "            'num_parallel_tree': None,\n",
    "            'random_state': 42,\n",
    "            'reg_alpha': None,\n",
    "            'reg_lambda': None,\n",
    "            'scale_pos_weight': None,\n",
    "            'subsample': 0.5,\n",
    "            'tree_method': 'approx',\n",
    "            'use_label_encoder': False,\n",
    "            'validate_parameters': None,\n",
    "            'verbosity': 1}\n",
    "h1 = XGBClassifier(max_depth=20,n_jobs=-1)\n",
    "h2 = XGBClassifier(n_jobs=-1)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "X,y = load_iris(return_X_y=True,as_frame=True)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# h1.fit(X,y)\n",
    "with parallel_backend('threading'):\n",
    "    cv_ = cross_validate(h1,X,y,cv=3,error_score='raise',n_jobs=-1)\n",
    "cv_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a10d01-6237-4ff4-979d-2cd3edf56e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c5e9f-a9e5-443e-a2f7-644ebcfbc08f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a63a7051-1875-48fb-8a9d-e4d338c32284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-21 {color: black;background-color: white;}#sk-container-id-21 pre{padding: 0;}#sk-container-id-21 div.sk-toggleable {background-color: white;}#sk-container-id-21 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-21 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-21 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-21 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-21 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-21 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-21 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-21 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-21 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-21 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-21 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-21 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-21 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-21 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-21 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-21 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-21 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-21 div.sk-item {position: relative;z-index: 1;}#sk-container-id-21 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-21 div.sk-item::before, #sk-container-id-21 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-21 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-21 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-21 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-21 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-21 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-21 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-21 div.sk-label-container {text-align: center;}#sk-container-id-21 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-21 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-21\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>FeatBoostClassifier(estimator=[XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;,\n",
       "                                             colsample_bylevel=1,\n",
       "                                             colsample_bynode=1,\n",
       "                                             colsample_bytree=1,\n",
       "                                             eval_metric=&#x27;aucpr&#x27;, gamma=0,\n",
       "                                             gpu_id=-1,\n",
       "                                             importance_type=&#x27;total_cover&#x27;,\n",
       "                                             interaction_constraints=&#x27;&#x27;,\n",
       "                                             learning_rate=0.2,\n",
       "                                             max_delta_step=0, max_depth=6,\n",
       "                                             min_child_weight=1, missing=nan,\n",
       "                                             monotone_constraints=&#x27;()&#x27;,\n",
       "                                             n_estimators=100...\n",
       "                                             min_child_weight=1, missing=nan,\n",
       "                                             monotone_constraints=&#x27;()&#x27;,\n",
       "                                             n_estimators=100, n_jobs=-1,\n",
       "                                             num_parallel_tree=1,\n",
       "                                             objective=&#x27;multi:softprob&#x27;,\n",
       "                                             random_state=42, reg_alpha=0,\n",
       "                                             reg_lambda=1,\n",
       "                                             scale_pos_weight=None,\n",
       "                                             subsample=0.5,\n",
       "                                             tree_method=&#x27;approx&#x27;,\n",
       "                                             use_label_encoder=False,\n",
       "                                             validate_parameters=1,\n",
       "                                             verbosity=1)],\n",
       "                    max_number_of_features=250, metric=&#x27;f1&#x27;, number_of_folds=3,\n",
       "                    siso_ranking_size=50, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" checked><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FeatBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>FeatBoostClassifier(estimator=[XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;,\n",
       "                                             colsample_bylevel=1,\n",
       "                                             colsample_bynode=1,\n",
       "                                             colsample_bytree=1,\n",
       "                                             eval_metric=&#x27;aucpr&#x27;, gamma=0,\n",
       "                                             gpu_id=-1,\n",
       "                                             importance_type=&#x27;total_cover&#x27;,\n",
       "                                             interaction_constraints=&#x27;&#x27;,\n",
       "                                             learning_rate=0.2,\n",
       "                                             max_delta_step=0, max_depth=6,\n",
       "                                             min_child_weight=1, missing=nan,\n",
       "                                             monotone_constraints=&#x27;()&#x27;,\n",
       "                                             n_estimators=100...\n",
       "                                             min_child_weight=1, missing=nan,\n",
       "                                             monotone_constraints=&#x27;()&#x27;,\n",
       "                                             n_estimators=100, n_jobs=-1,\n",
       "                                             num_parallel_tree=1,\n",
       "                                             objective=&#x27;multi:softprob&#x27;,\n",
       "                                             random_state=42, reg_alpha=0,\n",
       "                                             reg_lambda=1,\n",
       "                                             scale_pos_weight=None,\n",
       "                                             subsample=0.5,\n",
       "                                             tree_method=&#x27;approx&#x27;,\n",
       "                                             use_label_encoder=False,\n",
       "                                             validate_parameters=1,\n",
       "                                             verbosity=1)],\n",
       "                    max_number_of_features=250, metric=&#x27;f1&#x27;, number_of_folds=3,\n",
       "                    siso_ranking_size=50, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "FeatBoostClassifier(estimator=[XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                             colsample_bylevel=1,\n",
       "                                             colsample_bynode=1,\n",
       "                                             colsample_bytree=1,\n",
       "                                             eval_metric='aucpr', gamma=0,\n",
       "                                             gpu_id=-1,\n",
       "                                             importance_type='total_cover',\n",
       "                                             interaction_constraints='',\n",
       "                                             learning_rate=0.2,\n",
       "                                             max_delta_step=0, max_depth=6,\n",
       "                                             min_child_weight=1, missing=nan,\n",
       "                                             monotone_constraints='()',\n",
       "                                             n_estimators=100...\n",
       "                                             min_child_weight=1, missing=nan,\n",
       "                                             monotone_constraints='()',\n",
       "                                             n_estimators=100, n_jobs=-1,\n",
       "                                             num_parallel_tree=1,\n",
       "                                             objective='multi:softprob',\n",
       "                                             random_state=42, reg_alpha=0,\n",
       "                                             reg_lambda=1,\n",
       "                                             scale_pos_weight=None,\n",
       "                                             subsample=0.5,\n",
       "                                             tree_method='approx',\n",
       "                                             use_label_encoder=False,\n",
       "                                             validate_parameters=1,\n",
       "                                             verbosity=1)],\n",
       "                    max_number_of_features=250, metric='f1', number_of_folds=3,\n",
       "                    siso_ranking_size=50, verbose=1)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbc = FeatBoostClassifier([h1,h2],number_of_folds=3,max_number_of_features=250,metric='f1',verbose=1,siso_ranking_size=50)\n",
    "fbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7803878-5aa4-4c60-a361-fd622980fc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ranking features iteration 01\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "selected variable thus far:\n",
      "x_153\n",
      "Ranking features iteration 02\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "selected variable thus far:\n",
      "x_153\n",
      "x_004\n",
      "Ranking features iteration 03\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "selected variable thus far:\n",
      "x_153\n",
      "x_004\n",
      "x_390\n",
      "Ranking features iteration 04\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "selected variable thus far:\n",
      "x_153\n",
      "x_004\n",
      "x_390\n",
      "x_150\n",
      "Ranking features iteration 05\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "selected variable thus far:\n",
      "x_153\n",
      "x_004\n",
      "x_390\n",
      "x_150\n",
      "x_323\n",
      "Ranking features iteration 06\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "selected variable thus far:\n",
      "x_153\n",
      "x_004\n",
      "x_390\n",
      "x_150\n",
      "x_323\n",
      "x_074\n",
      "Ranking features iteration 07\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "selected variable thus far:\n",
      "x_153\n",
      "x_004\n",
      "x_390\n",
      "x_150\n",
      "x_323\n",
      "x_074\n",
      "x_403\n",
      "Ranking features iteration 08\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "selected variable thus far:\n",
      "x_153\n",
      "x_004\n",
      "x_390\n",
      "x_150\n",
      "x_323\n",
      "x_074\n",
      "x_403\n",
      "x_147\n",
      "Ranking features iteration 09\n",
      "\n",
      "\n",
      "ATTENTION: Reset occured because of tolerance reached!\n",
      "Infinite loop: No more resets this time!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "selected variable thus far:\n",
      "x_153\n",
      "x_004\n",
      "x_390\n",
      "x_150\n",
      "x_323\n",
      "x_074\n",
      "x_403\n",
      "x_147\n",
      "Ranking features iteration 09\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "selected variable thus far:\n",
      "x_153\n",
      "x_004\n",
      "x_390\n",
      "x_150\n",
      "x_323\n",
      "x_074\n",
      "x_403\n",
      "x_147\n",
      "x_364\n",
      "Ranking features iteration 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "selected variable thus far:\n",
      "x_153\n",
      "x_004\n",
      "x_390\n",
      "x_150\n",
      "x_323\n",
      "x_074\n",
      "x_403\n",
      "x_147\n",
      "x_364\n",
      "x_039\n",
      "Ranking features iteration 11\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('loky'):\n",
    "    fbc.fit(x__,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c1b77-8618-48d6-b963-7e5678b40dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x__= final_data_indvl.loc[:,nw_noms].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b03d45-75f3-42c2-8093-46c3feb0566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = final_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c546bf5-a877-45d6-8173-66c6cd8fab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_153\n",
    "x_004\n",
    "x_390\n",
    "x_150\n",
    "x_323\n",
    "x_074\n",
    "x_403\n",
    "x_147\n",
    "x_364\n",
    "x_039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1fd17-d198-4d3b-b5c6-a9619fcfc4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autoskl)",
   "language": "python",
   "name": "autoskl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
