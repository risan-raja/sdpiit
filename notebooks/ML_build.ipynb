{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a90f0379-dcfe-4789-98e1-095a9661cea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import (\n",
    "    Binarizer,\n",
    "    StandardScaler,\n",
    "    LabelBinarizer,\n",
    "    OneHotEncoder,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "# from xgboost import XGBRFClassifier, XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# import dtale\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "# sns.set()\n",
    "from sklearn import set_config\n",
    "from sklearn.ensemble import StackingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import (\n",
    "    Product,\n",
    "    RBF,\n",
    "    CompoundKernel,\n",
    "    Exponentiation,\n",
    "    Matern,\n",
    "    Sum,\n",
    ")\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from category_encoders import (\n",
    "    TargetEncoder,\n",
    "    BackwardDifferenceEncoder,\n",
    "    BaseNEncoder,\n",
    "    BinaryEncoder,\n",
    "    CatBoostEncoder,\n",
    "    CatBoostEncoder,\n",
    "    GLMMEncoder,\n",
    "    HelmertEncoder,\n",
    "    JamesSteinEncoder,\n",
    "    PolynomialEncoder,\n",
    "    QuantileEncoder,\n",
    "    SumEncoder,\n",
    "    SummaryEncoder,\n",
    "    WOEEncoder,\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    MinMaxScaler,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\n",
    "import warnings\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import plotly.express as px\n",
    "# from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_validate,\n",
    "    # StratifiedGroupKFold,\n",
    "    StratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    HalvingGridSearchCV,\n",
    "    HalvingRandomSearchCV\n",
    ")\n",
    "import sigopt\n",
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    CategoricalNB,\n",
    "    MultinomialNB,\n",
    "    ComplementNB,\n",
    "    GaussianNB,\n",
    ")\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# nb_est = [CategoricalNB(), MultinomialNB(), ComplementNB(), GaussianNB()]\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.tree import export_graphviz, plot_tree\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_selection import (\n",
    "    mutual_info_classif,\n",
    "    SelectKBest,\n",
    "    f_classif,\n",
    "    chi2,\n",
    "    RFE,\n",
    "    SelectFdr,\n",
    "    SelectFpr,\n",
    "    SelectFwe,\n",
    "    SelectPercentile,\n",
    ")\n",
    "from tbb import Monkey\n",
    "# from xgboost import XGBClassifier\n",
    "import os\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from joblib import parallel_backend\n",
    "from category_encoders.wrapper import PolynomialWrapper, NestedCVWrapper\n",
    "# from autosklearn.automl import AutoMLClassifier\n",
    "import sklearn.metrics\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pd.options.display.max_columns = 50\n",
    "set_config(display=\"diagram\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "dtype_info = {\n",
    "    \"v_1\": \"Binary\",\n",
    "    \"v_26\": \"Binary\",\n",
    "    \"v_11\": \"Binary\",\n",
    "    \"v_14\": \"Binary\",\n",
    "    \"v_30\": \"Binary\",\n",
    "    \"v_28\": \"Binary\",\n",
    "    \"v_9\": \"Binary\",\n",
    "    \"v_27\": \"Binary\",\n",
    "    \"v_32\": \"Nominal\",\n",
    "    \"v_4\": \"Nominal\",\n",
    "    \"v_3\": \"Nominal\",\n",
    "    \"v_20\": \"Nominal\",\n",
    "    \"v_21\": \"Nominal\",\n",
    "    \"v_18\": \"Nominal\",\n",
    "    \"v_25\": \"Nominal\",\n",
    "    \"v_12\": \"Nominal\",\n",
    "    \"v_31\": \"Ordinal\",\n",
    "    \"v_15\": \"Ordinal\",\n",
    "    \"v_19\": \"Ordinal\",\n",
    "    \"v_13\": \"Ordinal\",\n",
    "    \"v_33\": \"Ordinal\",\n",
    "    \"v_17\": \"Ordinal\",\n",
    "    \"v_29\": \"Ordinal\",\n",
    "    \"v_23\": \"Ordinal\",\n",
    "    \"v_6\": \"Ordinal\",\n",
    "    \"v_24\": \"Ordinal\",\n",
    "    \"v_10\": \"Ordinal\",\n",
    "    \"v_5\": \"Ordinal\",\n",
    "    \"v_22\": \"Ordinal\",\n",
    "    \"v_0\": \"Ordinal\",\n",
    "    \"v_16\": \"Ratio\",\n",
    "    \"v_2\": \"Ratio\",\n",
    "    \"v_8\": \"Ratio\",\n",
    "    \"v_7\": \"Ratio\",\n",
    "    \"v_39\": \"Ratio\",\n",
    "    \"v_37\": \"Ratio\",\n",
    "    \"v_38\": \"Ratio\",\n",
    "    \"v_34\": \"Ratio\",\n",
    "    \"v_40\": \"Ratio\",\n",
    "    \"v_36\": \"Ratio\",\n",
    "    \"v_35\": \"Ratio\",\n",
    "}\n",
    "# data = pd.read_csv(\n",
    "#     \"../data/train.csv\",\n",
    "#     index_col=0,\n",
    "# )\n",
    "data__ = pd.read_parquet(\"../data/data_with_ridit.hdfs\", engine=\"fastparquet\")\n",
    "prediction_data = pd.read_parquet(\"../data/test.parquet\", engine=\"fastparquet\")\n",
    "data = pd.read_parquet(\"../data/train.parquet\", engine=\"fastparquet\")\n",
    "ordinal = [i for i in dtype_info if dtype_info[i] == \"Ordinal\"]\n",
    "nominal = [i for i in dtype_info if dtype_info[i] == \"Nominal\"]\n",
    "binary = [i for i in dtype_info if dtype_info[i] == \"Binary\"]\n",
    "ratio = [i for i in dtype_info if dtype_info[i] == \"Ratio\"]\n",
    "final_data = pd.read_parquet(\"../data/final_data.parquet\", engine=\"fastparquet\")\n",
    "final_pred_data = pd.read_parquet(\n",
    "    \"../data/final_pred_data.parquet\", engine=\"fastparquet\"\n",
    ")\n",
    "baseline_prediction_data = pd.read_parquet('../data/baseline.parquet')\n",
    "\n",
    "\n",
    "def categorise_data(data):\n",
    "    ordinal_data = data.loc[:, ordinal]\n",
    "    nominal_data = data.loc[:, nominal]\n",
    "    binary_data = data.loc[:, binary]\n",
    "    ratio_data = data.loc[:, ratio]\n",
    "    return ordinal_data, nominal_data, binary_data, ratio_data\n",
    "\n",
    "\n",
    "ordinal_data, nominal_data, binary_data, ratio_data = categorise_data(final_data)\n",
    "baseline_prediction_data.rename(columns={'label':'target'}, inplace=True)\n",
    "final_data.rename(columns={'label':'target'}, inplace=True)\n",
    "data.rename(columns={'label':'target'}, inplace=True)\n",
    "# baseline_prediction_data\n",
    "\n",
    "bs = pd.concat([final_data,baseline_prediction_data], axis=0,ignore_index=True)\n",
    "nominal_master_db = bs.loc[:, nominal]\n",
    "\n",
    "def gen_train_test(data, p):\n",
    "    Y = data.target\n",
    "    X_2 = Y_2 = Y[Y == 2].index\n",
    "    X_0 = Y_0 = Y[Y == 0].index\n",
    "    X_1 = Y_1 = Y[Y == 1].index\n",
    "    train_size = int(p * Y_2.shape[0])\n",
    "    test_size = int((1 - p) * Y_2.shape[0])\n",
    "\n",
    "    train_idx_2 = np.random.choice(Y_2, (train_size,))\n",
    "    train_idx_1 = np.random.choice(Y_1, (train_size,))\n",
    "    train_idx_0 = np.random.choice(Y_0, (train_size,))\n",
    "    train_idx = np.r_[train_idx_0, train_idx_1, train_idx_2]\n",
    "    # train_idx.shape\n",
    "\n",
    "    test_idx_2 = np.random.choice(np.setdiff1d(Y_2, train_idx_2), (test_size,))\n",
    "    test_idx_1 = np.random.choice(np.setdiff1d(Y_1, train_idx_1), (test_size,))\n",
    "    test_idx_0 = np.random.choice(np.setdiff1d(Y_0, train_idx_0), (test_size,))\n",
    "    test_idx = np.r_[test_idx_0, test_idx_1, test_idx_2]\n",
    "    # test_idx.shape\n",
    "    return train_idx, test_idx\n",
    "\n",
    "nominal_cont_map=defaultdict(dict)\n",
    "nominal_indvl_map=defaultdict(dict)\n",
    "for c in nominal:\n",
    "    un = sorted(nominal_master_db[c].unique().tolist())\n",
    "    n = len(un)\n",
    "    new_id = list(range(n))\n",
    "    nominal_indvl_map[c] = dict(zip(un,new_id))\n",
    "start = 0\n",
    "for c in nominal:\n",
    "    un = sorted(nominal_master_db[c].unique().tolist())\n",
    "    n = len(un)\n",
    "    new_id = list(range(start,start+n))\n",
    "    nominal_cont_map[c] = dict(zip(un,new_id))\n",
    "    start += n\n",
    "\n",
    "\n",
    "nominal_master_db_indvl = nominal_master_db.copy()\n",
    "nominal_master_db_cont = nominal_master_db.copy()\n",
    "\n",
    "\n",
    "# nominal_indvl_map\n",
    "def nm_indvl_data_trnsform(row):\n",
    "    for c in nominal:\n",
    "        curr = row[c]\n",
    "        row[c] = nominal_indvl_map[c][curr]\n",
    "    return row\n",
    "\n",
    "\n",
    "test1_nominal = nominal_master_db_indvl.apply(nm_indvl_data_trnsform,axis=1)\n",
    "\n",
    "\n",
    "def nm_cont_data_trnsform(row):\n",
    "    for c in nominal:\n",
    "        curr = row[c]\n",
    "        row[c] = nominal_cont_map[c][curr]\n",
    "    return row\n",
    "\n",
    "\n",
    "test2_nominal = nominal_master_db_cont.apply(nm_cont_data_trnsform,axis=1)\n",
    "\n",
    "\n",
    "def best_n_features(n, X_train, y_train):\n",
    "    ohe = OneHotEncoder(\n",
    "        min_frequency=0.00001, handle_unknown=\"infrequent_if_exist\", sparse=False\n",
    "    )\n",
    "    X_train_t = ohe.fit_transform(X_train)\n",
    "    mic = mutual_info_classif(X_train_t, y_train, discrete_features=True)\n",
    "\n",
    "    return ohe.get_feature_names_out()[mic.argsort()[-n:]]\n",
    "\n",
    "\n",
    "# prediction_data = pd.read_pickle(\"../data/pred_data.pkl\")\n",
    "# est_ = [(\"cnb\",CategoricalNB()),]\n",
    "def iv_woe(data, target, bins=10, show_woe=False):\n",
    "\n",
    "    # Empty Dataframe\n",
    "    newDF, woeDF = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Extract Column Names\n",
    "    cols = data.columns\n",
    "\n",
    "    # Run WOE and IV on all the independent variables\n",
    "    for ivars in cols[~cols.isin([target])]:\n",
    "        if (data[ivars].dtype.kind in \"bifc\") and (len(np.unique(data[ivars])) > 1000):\n",
    "            binned_x = pd.qcut(data[ivars], bins, duplicates=\"drop\")\n",
    "            d0 = pd.DataFrame({\"x\": binned_x, \"y\": data[target]})\n",
    "        else:\n",
    "            d0 = pd.DataFrame({\"x\": data[ivars], \"y\": data[target]})\n",
    "        d0 = d0.astype({\"x\": str})\n",
    "        d = d0.groupby(\"x\", as_index=False, dropna=False).agg({\"y\": [\"count\", \"sum\"]})\n",
    "        d.columns = [\"Cutoff\", \"N\", \"Events\"]\n",
    "        d[\"% of Events\"] = np.maximum(d[\"Events\"], 0.5) / d[\"Events\"].sum()\n",
    "        d[\"Non-Events\"] = d[\"N\"] - d[\"Events\"]\n",
    "        d[\"% of Non-Events\"] = np.maximum(d[\"Non-Events\"], 0.5) / d[\"Non-Events\"].sum()\n",
    "        d[\"WoE\"] = np.log(d[\"% of Non-Events\"] / d[\"% of Events\"])\n",
    "        d[\"IV\"] = d[\"WoE\"] * (d[\"% of Non-Events\"] - d[\"% of Events\"])\n",
    "        d.insert(loc=0, column=\"Variable\", value=ivars)\n",
    "        print(\"Information value of \" + ivars + \" is \" + str(round(d[\"IV\"].sum(), 6)))\n",
    "        temp = pd.DataFrame(\n",
    "            {\"Variable\": [ivars], \"IV\": [d[\"IV\"].sum()]}, columns=[\"Variable\", \"IV\"]\n",
    "        )\n",
    "        newDF = pd.concat([newDF, temp], axis=0)\n",
    "        woeDF = pd.concat([woeDF, d], axis=0)\n",
    "\n",
    "        # Show WOE Table\n",
    "        if show_woe == True:\n",
    "            print(d)\n",
    "    return newDF, woeDF\n",
    "\n",
    "def wf_create(cat_encoder, model):\n",
    "    encoder__name = cat_encoder.__class__.__name__\n",
    "            # model = ComplementNB(norm=True,fit_prior=True,)\n",
    "    # model = MultinomialNB()\n",
    "    # model = LogisticRegression(n_jobs=-1, max_iter=10000,random_state=19)\n",
    "    model__name = model.__class__.__name__\n",
    "    pipe = Pipeline(steps=[('PW' + encoder__name, PolynomialWrapper(feature_encoder=cat_encoder)),\n",
    "                           (model__name,model )])\n",
    "    report = em.run(pipe) # Store A test log\n",
    "    print('PW' + encoder_name, model__name)\n",
    "    return pipe\n",
    "\n",
    "\n",
    "\n",
    "# print(pipe)\n",
    "# wf_create(TargetEncoder(),RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b428290-d847-44c9-a9aa-7acfa29177aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "mdb = MongoClient()\n",
    "# mdb.list_database_names()\n",
    "class evaluate_model:\n",
    "    collector = mdb.ml_results.cv_results\n",
    "    \n",
    "    \n",
    "    def __init__(self, model, data=pd.DataFrame(), features=[]):\n",
    "        self.model = model\n",
    "        if data.shape[0]>0:\n",
    "            self.data = data\n",
    "        else:\n",
    "            self.data = pd.read_parquet('../data/final_data.parquet', engine='fastparquet')\n",
    "        if len(features)>0:\n",
    "            self.features = features\n",
    "        else:\n",
    "            self.features = list(self.data.columns)\n",
    "        self.train_idx, self.test_idx = gen_train_test(self.data, 1.0)\n",
    "        self.X_train = self.data.loc[self.train_idx, self.features]\n",
    "        self.y_train = self.data.target.loc[self.train_idx]\n",
    "        self.cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=5)\n",
    "        self.cv_results = None\n",
    "        self.legacy=0\n",
    "        \n",
    "        \n",
    "    def run(self, custom_model=None):\n",
    "        if custom_model:\n",
    "            selected_model = custom_model\n",
    "        else:\n",
    "            selected_model = self.model\n",
    "        with parallel_backend('threading'):\n",
    "            cv_results = cross_validate(selected_model,\n",
    "                                        self.X_train,\n",
    "                                        self.y_train,\n",
    "                                        cv=self.cv,\n",
    "                                        n_jobs=-1,\n",
    "                                        scoring=['f1_micro',\n",
    "                                                 'f1_macro',\n",
    "                                                 'f1_weighted',\n",
    "                                                 'precision_micro',\n",
    "                                                 'precision_macro',\n",
    "                                                 'precision_weighted',\n",
    "                                                 'recall_micro',\n",
    "                                                 'recall_macro',\n",
    "                                                 'recall_weighted'],return_train_score=True,return_estimator=True)\n",
    "                                        # error_score='raise',return_estimator=True)\n",
    "                                    ## Save this results\n",
    "        self.cv_results = cv_results\n",
    "        # cv_db.insert_one(cv_results)\n",
    "        # cnst = {'estimator_params': selected_model.get_params()}\n",
    "        record =  {}\n",
    "        record['estimator_params'] = str(selected_model.get_params())\n",
    "        \n",
    "        record['model_name'] = selected_model.__class__.__name__\n",
    "        record['features'] = list(self.features)\n",
    "        # record['data_idx'] = {'train_idx': list(self.train_idx),'test_idx': list(self.test_idx)}\n",
    "        # cnst['fit_time'] = list(cv_results['fit_time'])\n",
    "        for k,v in cv_results.items():\n",
    "            if k in ['estimator','fit_time','score_time']:\n",
    "                continue\n",
    "            else:\n",
    "                record[k] = v.mean()\n",
    "        # for _ in cv_db.find():\n",
    "        #     print(_.keys())\n",
    "        # print(record)\n",
    "        record['is_legacy_run'] = self.legacy\n",
    "        record['legacy_report'] = None\n",
    "        if self.legacy ==1:\n",
    "            # print(self.legacy_run())\n",
    "            l_rpt = self.legacy_run()\n",
    "            for k, v in l_rpt.items():\n",
    "                print(k)\n",
    "                print('~'*20)\n",
    "                print(v)\n",
    "                print(\"#\"*90)\n",
    "            record['legacy_report'] = l_rpt\n",
    "        \n",
    "        collector.insert_one(record)\n",
    "        \n",
    "        return cv_results\n",
    "    \n",
    "    \n",
    "    def plot(self):\n",
    "        sns.set()\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(18, 15))        \n",
    "        # fig.layout='constrained'\n",
    "\n",
    "        metrics = ['f1_micro', 'f1_macro', 'f1_weighted', 'precision_micro', 'precision_macro', 'precision_weighted', 'recall_micro', 'recall_macro', 'recall_weighted']\n",
    "        # for m  in metrics:\n",
    "        #     n = len(self.cv_results['test_'+m])\n",
    "        idx=0\n",
    "        for i in range(3):\n",
    "            n = len(self.cv_results['test_'+ metrics[0]])\n",
    "            for j in range(3):\n",
    "                axes[i,j].plot(range(n),self.cv_results['test_'+metrics[idx]],label=['test_'+metrics[idx]])\n",
    "                # axes[i,j].plot(range(n),self.cv_results['train_'+metrics[idx]],label=['train_'+metrics[idx]])\n",
    "                # axes[i,j].fill_between(range(n),self.cv_results['test_'+metrics[idx]],self.cv_results['train_'+metrics[idx]])\n",
    "                axes[i,j].legend()\n",
    "                axes[i,j].set_title(metrics[idx])\n",
    "                # axes[i,j].autoscale(enable=False)\n",
    "                idx+=1\n",
    "        plt.legend()\n",
    "    \n",
    "    \n",
    "    def plot_metric(self, metric):\n",
    "        ydata = self.cv_results['test_'+metric]\n",
    "        n = ydata.shape[0]\n",
    "        ffig = plt.plot(np.arange(n),ydata,)\n",
    "        fig = sns.regplot(x=np.arange(n), y=ydata)\n",
    "        plt.title(metric.upper())\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def switch_to_main_data(self):\n",
    "        if self.legacy==1:\n",
    "            print(\"#\"*100)\n",
    "            print(\"ALREADY ON LEGACY MODE\")\n",
    "            print(\"#\"*100)\n",
    "            return\n",
    "        self.legacy = 1\n",
    "        self.X_train_old = self.X_train\n",
    "        self.y_train_old = self.y_train\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.data.loc[:,self.features], self.data.target)\n",
    "        print(\"#\"*100)\n",
    "        print(\"SWITCHING TO LEGACY MODE\")\n",
    "        print(\"#\"*100)\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        if self.legacy==0:\n",
    "            print(\"#\"*100)\n",
    "            print(\"NOT ON LEGACY MODE\")\n",
    "            print(\"#\"*100)\n",
    "            return\n",
    "        self.legacy = 0\n",
    "        self.X_train = self.X_train_old\n",
    "        self.y_train = self.y_train_old\n",
    "    \n",
    "    \n",
    "    def legacy_run(self):\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        self.y_pred = self.model.predict(self.X_test)\n",
    "        self.legacy_report = {}\n",
    "        self.legacy_report['classification_report'] = classification_report(self.y_test, self.y_pred)\n",
    "        self.legacy_report['balanced_accuracy'] = sklearn.metrics.balanced_accuracy_score(self.y_test, self.y_pred)\n",
    "        return self.legacy_report\n",
    "\n",
    "    \n",
    "    def print_last_run_stat(self):\n",
    "        N = collector.estimated_document_count()\n",
    "        idx = 0\n",
    "        for c in collector.find(skip=N-1):\n",
    "            for k,v in c.items():\n",
    "                if 'f1_' in k and 'test' in k:\n",
    "                    print(k)\n",
    "                    print('~'*5)\n",
    "                    print(v)\n",
    "                    print('-'*20)\n",
    "            print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90017aa8-8188-474b-8762-3d51e2a2c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.get_dummies(bs.target)\n",
    "target_classes = list(labels.columns)\n",
    "# transformed_nominal = pd.DataFrame()\n",
    "tr_nom_db = []\n",
    "for t_c in target_classes:\n",
    "    te = TargetEncoder(cols=nominal)\n",
    "    tmp = te.fit_transform(test1_nominal,labels[t_c])\n",
    "    tr_nom_db.append((transformed_nominal,labels[t_c]))\n",
    "# RFECV()\n",
    "\n",
    "# transformed_nominal.columns\n",
    "# from helpers.wrappers import PolynomialWrapper as pw \n",
    "# pw_te = pw(te)\n",
    "# del te_nominal_data\n",
    "# pw_te.\n",
    "# test1_nominal\n",
    "\n",
    "# pw_te.fit(test1_nominal, bs.target)\n",
    "# te_nom_mstr = pw_te.fit_transform(test1_nominal, pd.get_dummies(bs.target))\n",
    "# te_nominal_data = pw_te.fit_transform(test1_nominal.loc[:,nominal],bs.target)\n",
    "# pwt_data['target'] = final_data.target\n",
    "# pwt_data\n",
    "# te_nom_mstr.columns\n",
    "# te_nominal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c248ddd8-918e-4ce4-af1f-32212f2cf2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex.cluster import DBSCAN, KMeans     \n",
    "from sklearn.feature_selection import RFECV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2ba0ff-edfb-440f-ae82-c85bb46471ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import MEstimateEncoder\n",
    "\n",
    "\n",
    "ovr_pipe = Pipeline(steps=[(encoder_name,cat_encoder), (model__name, model)])\n",
    "                           \n",
    "# ovr_pipe\n",
    "ovr = OneVsRestClassifier(ovr_pipe)\n",
    "# ovr\n",
    "# pipe\n",
    "# print(pipe)\n",
    "d_coll = []\n",
    "try:\n",
    "    for m_ in tqdm(np.linspace(0.1,1.5,num=20)):\n",
    "        cat_encoder = TargetEncoder(drop_invariant=False,cols=['v_32','v_4','v_3','v_20','v_21','v_18','v_25','v_12'],smoothing=m_)\n",
    "        encoder_name = cat_encoder.__class__.__name__\n",
    "        # model = ComplementNB(norm=True,fit_prior=True,)\n",
    "        model = MultinomialNB()\n",
    "        model = LogisticRegression(n_jobs=-1, max_iter=10000,random_state=19)\n",
    "        model__name = model.__class__.__name__\n",
    "        pipe = Pipeline(steps=[('PW' + encoder_name, PolynomialWrapper(feature_encoder=cat_encoder)),\n",
    "                               (model__name,model )])\n",
    "\n",
    "        report = em.run(pipe)\n",
    "        print(report['test_f1_macro'].mean())\n",
    "        d_coll.append((m_,report['test_f1_macro'].mean(), report['test_f1_macro'].std(), report['test_f1_macro'].max()))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted, Plotting calculated values only\")\n",
    "# pipe = make\n",
    "def plot_m_variance(d_coll):\n",
    "    sns.set()\n",
    "    ddx= [x for x,y,u,r in d_coll]\n",
    "    ddc = [y for x,y,u,r in d_coll]\n",
    "    ddep = [y+u for x,y,u,r in d_coll]\n",
    "    dden = [y-u for x,y,u,r in d_coll]\n",
    "    ddem = [r for x,y,u,r in d_coll]\n",
    "    plt.plot(ddx,ddc,'b',label=\"\\u00b5\")\n",
    "    plt.plot(ddx,ddep,'r',label=\"\\u03c3\"+\"+\")\n",
    "    plt.plot(ddx,dden,'g',label=\"\\u03c3\"+\"-\")\n",
    "    plt.plot(ddx,ddem,'y',label=\"\\u03c3\"+\"max\")\n",
    "    fig=plt.fill_between(ddx,ddep,dden,alpha=0.5)\n",
    "    fig = plt.legend()\n",
    "plot_m_variance(d_coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1532766a-4cb7-4149-95f7-2446c263b36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       2\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "5419    0\n",
       "5420    1\n",
       "5421    0\n",
       "5422    1\n",
       "5423    1\n",
       "Name: target, Length: 5424, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autoskl)",
   "language": "python",
   "name": "autoskl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
